{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# preprocess_moleculenet.py\n\nimport os\nimport tensorflow as tf\nimport numpy as np\nfrom rdkit import Chem\nimport deepchem as dc\nfrom tqdm import tqdm\n\n# Suppress non-critical RDKit warnings\nfrom rdkit import rdBase\nrdBase.DisableLog('rdApp.warning')\nrdBase.DisableLog('rdApp.error')\n\n# --- Global Constants (MUST match your pre-training and fine-tuning notebooks) ---\nMAX_SMILES_LEN = 256\nMAX_NODES = 419 # This should be the MAX_NODES used in your pre-training\nNUM_ATOM_FEATURES = 5 # As defined by your atom_to_feature_vector\n\nOUTPUT_TFRECORD_DIR = 'moleculenet_tfrecords' # Directory to save preprocessed MoleculeNet data\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_TFRECORD_DIR, exist_ok=True)\n\nprint(f\"Output TFRecords will be saved to: {OUTPUT_TFRECORD_DIR}\")\nprint(f\"Configured MAX_NODES: {MAX_NODES}, NUM_ATOM_FEATURES: {NUM_ATOM_FEATURES}, MAX_SMILES_LEN: {MAX_SMILES_LEN}\")\n\n\n# --- Helper Functions (Copied from your notebooks for consistency) ---\n\n# --- SMILES Tokenization ---\ndef build_smiles_vocab(smiles_list, max_vocab_size=None):\n    all_chars = set()\n    for smiles in smiles_list:\n        for char in smiles:\n            all_chars.add(char)\n    vocab = sorted(list(all_chars))\n    vocab = ['<pad>', '<unk>', '<cls>', '<eos>'] + vocab\n    if max_vocab_size:\n        vocab = vocab[:max_vocab_size]\n    char_to_idx = {char: i for i, char in enumerate(vocab)}\n    idx_to_char = {i: char for i, char in enumerate(vocab)}\n    print(f\"Built vocabulary of size: {len(vocab)}\")\n    return vocab, char_to_idx, idx_to_char\n\ndef tokenize_smiles(smiles, char_to_idx, max_len):\n    tokens = list(smiles)\n    indexed_tokens = [char_to_idx.get(char, char_to_idx['<unk>']) for char in tokens]\n    if len(indexed_tokens) < max_len:\n        padded_tokens = indexed_tokens + [char_to_idx['<pad>']] * (max_len - len(indexed_tokens))\n    else:\n        padded_tokens = indexed_tokens[:max_len]\n    return np.array(padded_tokens, dtype=np.int32)\n\ndef create_smiles_mask(token_ids, pad_token_id):\n    return tf.cast(token_ids == pad_token_id, tf.bool)\n\n\n# --- SMILES to TensorFlow Graph Conversion ---\ndef atom_to_feature_vector(atom):\n    features = []\n    features.append(atom.GetAtomicNum())\n    features.append(atom.GetDegree())\n    features.append(int(atom.GetHybridization()))\n    features.append(int(atom.GetIsAromatic()))\n    features.append(atom.GetFormalCharge())\n    return np.array(features, dtype=np.float32)\n\ndef smiles_to_tf_graph(smiles_string):\n    mol = Chem.MolFromSmiles(smiles_string)\n    if mol is None:\n        return (np.zeros((0, NUM_ATOM_FEATURES), dtype=np.float32),\n                np.zeros((0, 2), dtype=np.int32),\n                0,\n                0)\n\n    node_features = [atom_to_feature_vector(atom) for atom in mol.GetAtoms()]\n    if not node_features:\n        return (np.zeros((0, NUM_ATOM_FEATURES), dtype=np.float32),\n                np.zeros((0, 2), dtype=np.int32),\n                0,\n                0)\n    node_features = np.array(node_features, dtype=np.float32)\n    num_nodes = len(node_features)\n\n    edge_indices = []\n    for bond in mol.GetBonds():\n        i = bond.GetBeginAtomIdx()\n        j = bond.GetEndAtomIdx()\n        edge_indices.append([i, j])\n        edge_indices.append([j, i])\n\n    num_edges = len(edge_indices)\n    if num_edges == 0:\n        if num_nodes > 0:\n            edge_indices_final = np.empty((0, 2), dtype=np.int32)\n            num_edges_final = 0\n        else:\n            return (np.zeros((0, NUM_ATOM_FEATURES), dtype=np.float32),\n                    np.zeros((0, 2), dtype=np.int32),\n                    0,\n                    0)\n    else:\n        edge_indices_final = np.array(edge_indices, dtype=np.int32)\n        num_edges_final = len(edge_indices_final)\n\n    return node_features, edge_indices_final, num_nodes, num_edges_final\n\n\n# Define the featurization function for a single sample (returns flat tuple of NumPy arrays)\ndef featurize_smiles_and_graph_with_label(smiles_string, label):\n    token_ids = tokenize_smiles(smiles_string, char_to_idx, MAX_SMILES_LEN)\n    mask = create_smiles_mask(token_ids, char_to_idx['<pad>'])\n\n    node_features, edge_indices, num_nodes, num_edges = smiles_to_tf_graph(smiles_string)\n\n    if num_nodes == 0:\n        dummy_node_features = np.zeros((MAX_NODES, NUM_ATOM_FEATURES), dtype=np.float32)\n        dummy_edge_indices = np.zeros((0, 2), dtype=np.int32)\n        dummy_num_nodes = 0\n        dummy_num_edges = 0\n        return (dummy_node_features, dummy_edge_indices, dummy_num_nodes, dummy_num_edges, token_ids, mask, label)\n    \n    padded_node_features = np.pad(node_features, [[0, MAX_NODES - num_nodes], [0, 0]])\n    \n    return (padded_node_features, edge_indices, num_nodes, num_edges, token_ids, mask, label)\n\n\n# --- TFRecord Serialization Functions ---\ndef _bytes_feature(value):\n    if isinstance(value, tf.Tensor): # Ensure it's a NumPy array before .tobytes()\n        value = value.numpy()\n    if isinstance(value, np.ndarray):\n        value = value.tobytes()\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef serialize_moleculenet_example(node_feat_padded, edge_idx, num_nodes, num_edges, token_ids, smiles_mask, label):\n    feature = {\n        'node_feat_padded': _bytes_feature(node_feat_padded),\n        'edge_idx': _bytes_feature(edge_idx),\n        'num_nodes': _int64_feature(num_nodes),\n        'num_edges': _int64_feature(num_edges),\n        'token_ids': _bytes_feature(token_ids),\n        'smiles_mask': _bytes_feature(smiles_mask),\n        'label': _bytes_feature(label) # Labels can be multi-dimensional (e.g., Tox21)\n    }\n    return tf.train.Example(features=tf.train.Features(feature=feature))\n\n\n# --- Main Preprocessing Loop for MoleculeNet Datasets ---\nif __name__ == \"__main__\":\n    print(\"Starting MoleculeNet preprocessing script...\")\n\n    # Build vocabulary from a dummy set of characters (or load from pre-training)\n    # This ensures char_to_idx is defined for tokenization.\n    dummy_smiles_for_vocab_build = [\"C\", \"N\", \"O\", \"F\", \"P\", \"S\", \"Cl\", \"Br\", \"I\", \"c\", \"n\", \"=\", \"#\", \"(\", \")\", \"[\", \"]\", \"@\", \"+\", \"-\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"H\", \"B\", \"b\", \"K\", \"k\", \"L\", \"l\", \"M\", \"m\", \"R\", \"r\", \"X\", \"x\", \"Y\", \"y\", \"Z\", \"z\"] \n    global char_to_idx, VOCAB_SIZE # Ensure these are global for the script\n    _, char_to_idx, _ = build_smiles_vocab(dummy_smiles_for_vocab_build)\n    VOCAB_SIZE = len(char_to_idx) \n    print(f\"Vocabulary built with {VOCAB_SIZE} tokens for MoleculeNet processing.\")\n\n    # --- Process and Save BBBP ---\n    print(\"\\nProcessing BBBP dataset...\")\n    bbbp_tasks, bbbp_datasets, _ = dc.molnet.load_bbbp(featurizer='Raw', splitter='scaffold')\n    \n    # Determine label dtype and shape for BBBP (binary classification, single label)\n    bbbp_labels_dtype = tf.float32 if bbbp_datasets[0].y.dtype == np.bool_ or bbbp_datasets[0].y.dtype == np.int_ else bbbp_datasets[0].y.dtype\n    bbbp_label_shape = bbbp_datasets[0].y.shape[1:] if bbbp_datasets[0].y.ndim > 1 else ()\n\n    for dataset_split, name in zip(bbbp_datasets, ['train', 'valid', 'test']):\n        tfrecord_path = os.path.join(OUTPUT_TFRECORD_DIR, f'bbbp_{name}.tfrecord')\n        num_samples_processed = 0\n        with tf.io.TFRecordWriter(tfrecord_path) as writer:\n            for i in tqdm(range(len(dataset_split.X)), desc=f\"Featurizing BBBP {name}\"):\n                smiles_str = dataset_split.X[i]\n                label = dataset_split.y[i]\n                \n                # Call featurization, it returns a flat tuple of NumPy arrays\n                processed_data_flat = featurize_smiles_and_graph_with_label(smiles_str, label)\n                \n                if processed_data_flat[2] > 0: # num_nodes > 0\n                    # Ensure label is correctly cast to the determined labels_dtype for serialization\n                    label_tensor = tf.constant(processed_data_flat[6], dtype=bbbp_labels_dtype)\n                    \n                    example = serialize_moleculenet_example(*processed_data_flat[:6], label=label_tensor)\n                    writer.write(example.SerializeToString())\n                    num_samples_processed += 1\n        print(f\"Saved {num_samples_processed} samples to {tfrecord_path}\")\n\n    # --- Process and Save Tox21 ---\n    print(\"\\nProcessing Tox21 dataset...\")\n    tox21_tasks, tox21_datasets, _ = dc.molnet.load_tox21(featurizer='Raw', splitter='scaffold')\n\n    # Determine label dtype and shape for Tox21 (multi-label classification)\n    tox21_labels_dtype = tf.float32 if tox21_datasets[0].y.dtype == np.bool_ or tox21_datasets[0].y.dtype == np.int_ else tox21_datasets[0].y.dtype\n    tox21_label_shape = tox21_datasets[0].y.shape[1:] if tox21_datasets[0].y.ndim > 1 else ()\n\n    for dataset_split, name in zip(tox21_datasets, ['train', 'valid', 'test']):\n        tfrecord_path = os.path.join(OUTPUT_TFRECORD_DIR, f'tox21_{name}.tfrecord')\n        num_samples_processed = 0\n        with tf.io.TFRecordWriter(tfrecord_path) as writer:\n            for i in tqdm(range(len(dataset_split.X)), desc=f\"Featurizing Tox21 {name}\"):\n                smiles_str = dataset_split.X[i]\n                label = dataset_split.y[i]\n                \n                processed_data_flat = featurize_smiles_and_graph_with_label(smiles_str, label)\n                \n                if processed_data_flat[2] > 0: # num_nodes > 0\n                    label_tensor = tf.constant(processed_data_flat[6], dtype=tox21_labels_dtype)\n                    example = serialize_moleculenet_example(*processed_data_flat[:6], label=label_tensor)\n                    writer.write(example.SerializeToString())\n                    num_samples_processed += 1\n        print(f\"Saved {num_samples_processed} samples to {tfrecord_path}\")\n\n    # --- Process and Save ESOL ---\n    print(\"\\nProcessing ESOL dataset...\")\n    esol_tasks, esol_datasets, _ = dc.molnet.load_esol(featurizer='Raw', splitter='scaffold')\n\n    # Determine label dtype and shape for ESOL (regression, single label)\n    esol_labels_dtype = tf.float32 if esol_datasets[0].y.dtype == np.bool_ or esol_datasets[0].y.dtype == np.int_ else esol_datasets[0].y.dtype\n    esol_label_shape = esol_datasets[0].y.shape[1:] if esol_datasets[0].y.ndim > 1 else ()\n\n    for dataset_split, name in zip(esol_datasets, ['train', 'valid', 'test']):\n        tfrecord_path = os.path.join(OUTPUT_TFRECORD_DIR, f'esol_{name}.tfrecord')\n        num_samples_processed = 0\n        with tf.io.TFRecordWriter(tfrecord_path) as writer:\n            for i in tqdm(range(len(dataset_split.X)), desc=f\"Featurizing ESOL {name}\"):\n                smiles_str = dataset_split.X[i]\n                label = dataset_split.y[i]\n                \n                processed_data_flat = featurize_smiles_and_graph_with_label(smiles_str, label)\n                \n                if processed_data_flat[2] > 0: # num_nodes > 0\n                    label_tensor = tf.constant(processed_data_flat[6], dtype=esol_labels_dtype)\n                    example = serialize_moleculenet_example(*processed_data_flat[:6], label=label_tensor)\n                    writer.write(example.SerializeToString())\n                    num_samples_processed += 1\n        print(f\"Saved {num_samples_processed} samples to {tfrecord_path}\")\n\n    print(\"\\nMoleculeNet preprocessing script finished.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}