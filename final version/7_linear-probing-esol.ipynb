{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12364870,"sourceType":"datasetVersion","datasetId":7796043},{"sourceId":12366455,"sourceType":"datasetVersion","datasetId":7797008}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport glob\nimport os\n\n#  CONFIGURATION FOR ESOL \nDATASET_NAME = 'esol'\nMOLECULENET_PATH = '/kaggle/input/moleculenet-tfrecords-final/moleculenet_tfrecords_final/'\nGRASP_CHECKPOINT_PATH = '/kaggle/input/pretraining-checkpoints/pretraining_checkpoints/'\nBATCH_SIZE = 64\nEPOCHS = 50\nLEARNING_RATE = 0.001\nMAX_NODES = 419\nNUM_ATOM_FEATURES = 5\n\n#  DATA PIPELINE \nDUMMY_SMILES_FOR_VOCAB = [\"C\", \"N\", \"O\", \"F\", \"P\", \"S\", \"Cl\", \"Br\", \"I\", \"c\", \"n\", \"=\", \"#\", \"(\", \")\", \"[\", \"]\", \"@\", \"+\", \"-\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"H\", \"B\", \"b\", \"K\", \"k\", \"L\", \"l\", \"M\", \"m\", \"R\", \"r\", \"X\", \"x\", \"Y\", \"y\", \"Z\", \"z\"] \nVOCAB = ['<pad>', '<unk>', '<cls>', '<eos>'] + sorted(list(set(\"\".join(DUMMY_SMILES_FOR_VOCAB))))\nCHAR_TO_IDX = {char: i for i, char in enumerate(VOCAB)}\n\ndef parse_fn(example):\n    feature_description = {\n        'atom_features': tf.io.FixedLenFeature([], tf.string), 'edge_index': tf.io.FixedLenFeature([], tf.string),\n        'num_nodes': tf.io.FixedLenFeature([], tf.string), 'token_ids': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n    atom_features = tf.io.parse_tensor(example['atom_features'], out_type=tf.float32)\n    edge_index = tf.io.parse_tensor(example['edge_index'], out_type=tf.int32)\n    num_nodes = tf.io.parse_tensor(example['num_nodes'], out_type=tf.int32)\n    token_ids = tf.io.parse_tensor(example['token_ids'], out_type=tf.int32)\n    label = tf.io.parse_tensor(example['label'], out_type=tf.float32)\n    # For ESOL, the label shape is (1,)\n    label = tf.reshape(label, [1])\n    return (atom_features, edge_index, num_nodes, token_ids), label\n\n@tf.function\ndef prepare_batch_for_model(features, label):\n    atom_features, edge_index, num_nodes, token_ids = features\n    atom_features_flat = tf.reshape(atom_features, (BATCH_SIZE * MAX_NODES, NUM_ATOM_FEATURES))\n    num_nodes_squeezed = tf.squeeze(num_nodes, axis=-1)\n    node_offsets = tf.cumsum(num_nodes_squeezed, exclusive=True)\n    is_real_edge_mask = edge_index[:, :, 0] >= 0\n    edge_batch_ids = tf.where(is_real_edge_mask)[:, 0]\n    edge_batch_ids = tf.cast(edge_batch_ids, dtype=tf.int32)\n    edge_offsets = tf.gather(node_offsets, edge_batch_ids)\n    real_edges = tf.boolean_mask(edge_index, is_real_edge_mask)\n    global_edge_index = real_edges + tf.expand_dims(edge_offsets, axis=-1)\n    padding_mask = (token_ids != CHAR_TO_IDX['<pad>'])\n    model_inputs = {\n        'atom_features_input': atom_features_flat, 'edge_index_input': global_edge_index,\n        'num_nodes_input': num_nodes_squeezed, 'token_ids_input': token_ids,\n        'padding_mask_input': padding_mask\n    }\n    return model_inputs, label\n\ndef create_dataset(file_pattern, should_shuffle=False):\n    files = glob.glob(file_pattern)\n    if not files: return None\n    dataset = tf.data.TFRecordDataset(files, num_parallel_reads=tf.data.AUTOTUNE).map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n    if should_shuffle: dataset = dataset.shuffle(buffer_size=1024)\n    # The label shape for padded_batch is now hardcoded to 1 for ESOL\n    dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=((tf.TensorShape([MAX_NODES, NUM_ATOM_FEATURES]), tf.TensorShape([None, 2]), tf.TensorShape([1]), tf.TensorShape([256])), tf.TensorShape([1])), drop_remainder=True)\n    dataset = dataset.map(prepare_batch_for_model, num_parallel_calls=tf.data.AUTOTUNE)\n    return dataset.prefetch(tf.data.AUTOTUNE)\n\n\n#  Subclassed keras.Model \nclass GraspLinearProbeModel(keras.Model):\n    def __init__(self, checkpoint_path, **kwargs):\n        super().__init__(**kwargs)\n        self.gin_model = tf.saved_model.load(os.path.join(checkpoint_path, 'gin_encoder_best'))\n        self.transformer_model = tf.saved_model.load(os.path.join(checkpoint_path, 'transformer_encoder_best'))\n        self.gin_function = self.gin_model.signatures['serving_default']\n        self.transformer_function = self.transformer_model.signatures['serving_default']\n        # The head now has 1 output unit and a linear activation for regression\n        self.head = layers.Dense(1, activation='linear')\n        self.concat = layers.Concatenate()\n        \n    def call(self, data, training=False):\n        atom_feats, edge_index, num_nodes, token_ids, padding_mask = (\n            data['atom_features_input'], data['edge_index_input'], data['num_nodes_input'],\n            data['token_ids_input'], data['padding_mask_input']\n        )\n        edge_index_float = tf.cast(edge_index, tf.float32)\n        num_nodes_float = tf.cast(num_nodes, tf.float32)\n        token_ids_float = tf.cast(token_ids, tf.float32)\n        padding_mask_float = tf.cast(padding_mask, tf.float32)\n        \n        graph_embedding = self.gin_function(inputs=atom_feats, inputs_1=edge_index_float, inputs_2=num_nodes_float)\n        smiles_embedding = self.transformer_function(inputs=token_ids_float, inputs_1=padding_mask_float)\n        \n        concatenated_embeddings = self.concat([graph_embedding['output_0'], smiles_embedding['output_0']])\n        return self.head(concatenated_embeddings)\n\n    def train_step(self, data):\n        inputs, y_true = data\n        with tf.GradientTape() as tape:\n            y_pred = self(inputs, training=True)\n            loss = self.compiled_loss(y_true, y_pred, regularization_losses=self.losses)\n            \n        trainable_vars = self.head.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        \n        self.compiled_metrics.update_state(y_true, y_pred)\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        inputs, y_true = data\n        y_pred = self(inputs, training=False)\n        self.compiled_loss(y_true, y_pred, regularization_losses=self.losses)\n        self.compiled_metrics.update_state(y_true, y_pred)\n        return {m.name: m.result() for m in self.metrics}\n\n#  Main Execution \ndef run_linear_probing():\n    print(f\" Starting Linear Probing for Dataset: {DATASET_NAME} \")\n    \n    model = GraspLinearProbeModel(GRASP_CHECKPOINT_PATH)\n    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    \n    # compiling the model with regression loss and metrics\n    model.compile(optimizer=optimizer, \n                  loss='mean_squared_error', \n                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), \n                           tf.keras.metrics.MeanAbsoluteError(name='mae')])\n    \n    train_ds = create_dataset(os.path.join(MOLECULENET_PATH, f'{DATASET_NAME.lower()}_train.tfrecord'), should_shuffle=True)\n    valid_ds = create_dataset(os.path.join(MOLECULENET_PATH, f'{DATASET_NAME.lower()}_valid.tfrecord'))\n    test_ds = create_dataset(os.path.join(MOLECULENET_PATH, f'{DATASET_NAME.lower()}_test.tfrecord'))\n    if not train_ds: raise ValueError(f\"Training TFRecord not found for {MOLECULENET_PATH}\")\n\n    print(\"\\n Starting Training of the Linear Head \")\n    model.fit(train_ds, epochs=EPOCHS, validation_data=valid_ds, verbose=1)\n    \n    print(\"\\n Training Finished \")\n    print(\"\\n Final Performance on Unseen Test Data \")\n    \n    model.evaluate(test_ds, verbose=0)\n\n    for metric in model.metrics:\n        result = metric.result()\n        # Check if the result is a dictionary (which happens for some metrics)\n        if isinstance(result, dict):\n            for key, value in result.items():\n                print(f\"  Final Test {key}: {value.numpy():.4f}\")\n        else:\n            print(f\"  Final Test {metric.name}: {result.numpy():.4f}\")\n\nrun_linear_probing()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T06:10:03.614494Z","iopub.execute_input":"2025-07-04T06:10:03.614812Z","iopub.status.idle":"2025-07-04T06:39:10.349820Z","shell.execute_reply.started":"2025-07-04T06:10:03.614791Z","shell.execute_reply":"2025-07-04T06:39:10.348917Z"}},"outputs":[{"name":"stdout","text":" Starting Linear Probing for Dataset: esol \n\n Starting Training of the Linear Head \nEpoch 1/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - mae: 783.9236 - rmse: 6409.7808 - loss: -352.4199 - val_loss: 387.2663\nEpoch 2/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - mae: 776.0024 - rmse: 7425.4927 - loss: 157.4381 - val_loss: -843.6974\nEpoch 3/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 1604.3159 - rmse: 26026.7344 - loss: -1633.7240 - val_loss: 121.7905\nEpoch 4/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 1114.4108 - rmse: 10660.2549 - loss: -55.7649 - val_loss: 613.6386\nEpoch 5/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 737.4864 - rmse: 6891.5420 - loss: 555.0538 - val_loss: 645.5946\nEpoch 6/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - mae: 676.8101 - rmse: 6999.9136 - loss: 455.9750 - val_loss: 407.0562\nEpoch 7/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 437.2980 - rmse: 3558.3555 - loss: 215.8057 - val_loss: 63.5081\nEpoch 8/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 343.0542 - rmse: 2471.1860 - loss: -158.9319 - val_loss: -29.0267\nEpoch 9/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - mae: 275.1209 - rmse: 1515.6744 - loss: -16.9016 - val_loss: -57.0764\nEpoch 10/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 233.9207 - rmse: 1383.4386 - loss: -61.1369 - val_loss: -95.6243\nEpoch 11/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 309.4677 - rmse: 2307.9939 - loss: 30.8316 - val_loss: -136.4119\nEpoch 12/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - mae: 458.6295 - rmse: 6345.9653 - loss: 79.4089 - val_loss: -961.3973\nEpoch 13/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 514.5820 - rmse: 3290.1870 - loss: -453.5226 - val_loss: -1266.0341\nEpoch 14/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 552.7969 - rmse: 3572.0439 - loss: -521.0621 - val_loss: -1125.8931\nEpoch 15/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 1422.8396 - rmse: 13292.6016 - loss: -1382.2163 - val_loss: -317.1551\nEpoch 16/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 278.4368 - rmse: 1628.3420 - loss: -22.5044 - val_loss: -189.8180\nEpoch 17/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - mae: 280.6785 - rmse: 1632.8240 - loss: 40.6840 - val_loss: -192.2118\nEpoch 18/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 211.9339 - rmse: 1039.2483 - loss: -101.2584 - val_loss: -234.5767\nEpoch 19/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 241.0208 - rmse: 1133.7369 - loss: -109.8340 - val_loss: -266.9701\nEpoch 20/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 235.9315 - rmse: 1800.0704 - loss: -57.7696 - val_loss: -466.9827\nEpoch 21/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 654.4521 - rmse: 10434.7773 - loss: 96.1840 - val_loss: -749.9081\nEpoch 22/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 425.3844 - rmse: 2790.0239 - loss: -405.6790 - val_loss: -856.2197\nEpoch 23/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - mae: 427.4639 - rmse: 2750.4661 - loss: -406.3339 - val_loss: -595.0934\nEpoch 24/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 415.2637 - rmse: 4241.7471 - loss: -150.4903 - val_loss: -685.5924\nEpoch 25/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 540.2358 - rmse: 3695.1982 - loss: -509.5523 - val_loss: -870.8192\nEpoch 26/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 383.2570 - rmse: 2566.7668 - loss: -354.8443 - val_loss: -787.6630\nEpoch 27/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 517.3256 - rmse: 3288.0166 - loss: -495.0591 - val_loss: -511.8828\nEpoch 28/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - mae: 337.9387 - rmse: 3236.4189 - loss: -300.6505 - val_loss: -100.9461\nEpoch 29/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 535.7067 - rmse: 6601.6768 - loss: 260.9109 - val_loss: 387.3781\nEpoch 30/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - mae: 394.8282 - rmse: 2366.7046 - loss: 113.9674 - val_loss: -289.6647\nEpoch 31/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 209.2428 - rmse: 789.4392 - loss: -157.7060 - val_loss: -461.2389\nEpoch 32/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - mae: 710.4000 - rmse: 5904.8516 - loss: -720.6981 - val_loss: -253.7674\nEpoch 33/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 904.5374 - rmse: 10253.5264 - loss: -621.6385 - val_loss: 770.0157\nEpoch 34/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 1091.8499 - rmse: 8337.7188 - loss: 804.6705 - val_loss: 460.2376\nEpoch 35/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 441.8607 - rmse: 3446.0637 - loss: 206.3509 - val_loss: 267.2219\nEpoch 36/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 431.1153 - rmse: 5209.2197 - loss: 237.4668 - val_loss: 48.6149\nEpoch 37/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - mae: 344.6153 - rmse: 2742.4238 - loss: 38.5582 - val_loss: -39.8172\nEpoch 38/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 210.6672 - rmse: 1608.4315 - loss: -190.1555 - val_loss: 55.8925\nEpoch 39/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 1076.9988 - rmse: 13884.2305 - loss: 868.8659 - val_loss: -547.3047\nEpoch 40/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 587.8987 - rmse: 5649.6221 - loss: 102.8882 - val_loss: -1449.5332\nEpoch 41/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 730.5960 - rmse: 5499.7964 - loss: -704.2120 - val_loss: -1377.6982\nEpoch 42/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 592.2454 - rmse: 4397.9585 - loss: -365.5926 - val_loss: -765.6262\nEpoch 43/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 234.7780 - rmse: 1112.0551 - loss: -154.6521 - val_loss: -492.9589\nEpoch 44/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 297.0349 - rmse: 1533.6351 - loss: -282.0966 - val_loss: -334.5697\nEpoch 45/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - mae: 422.2220 - rmse: 3849.0356 - loss: 204.7398 - val_loss: -396.8958\nEpoch 46/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - mae: 364.5803 - rmse: 3476.6182 - loss: 17.7112 - val_loss: -740.3087\nEpoch 47/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 599.6828 - rmse: 5762.6553 - loss: -612.7806 - val_loss: -414.8058\nEpoch 48/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 221.7247 - rmse: 1603.6288 - loss: -44.7373 - val_loss: -223.6879\nEpoch 49/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - mae: 324.3288 - rmse: 2614.4238 - loss: 53.1061 - val_loss: -385.0618\nEpoch 50/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - mae: 249.2064 - rmse: 1930.6444 - loss: -236.4368 - val_loss: -342.4970\n\n Training Finished \n\n Final Performance on Unseen Test Data \n  Final Test loss: -55.1849\n  Final Test rmse: 87.1586\n  Final Test mae: 55.1139\n","output_type":"stream"}],"execution_count":5}]}