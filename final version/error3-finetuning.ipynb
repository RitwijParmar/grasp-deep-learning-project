{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12355703,"sourceType":"datasetVersion","datasetId":7789724},{"sourceId":12355720,"sourceType":"datasetVersion","datasetId":7789733}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install deepchem --quiet\n!pip install rdkit --quiet\n!pip install tqdm --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-02T22:13:23.611466Z","iopub.execute_input":"2025-07-02T22:13:23.611748Z","iopub.status.idle":"2025-07-02T22:13:35.915597Z","shell.execute_reply.started":"2025-07-02T22:13:23.611729Z","shell.execute_reply":"2025-07-02T22:13:35.914128Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nfrom rdkit import Chem\nimport deepchem as dc \nfrom tqdm import tqdm # For progress bars\n\n# Suppress non-critical RDKit warnings\nfrom rdkit import rdBase\nrdBase.DisableLog('rdApp.warning')\nrdBase.DisableLog('rdApp.error')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T22:13:35.917843Z","iopub.execute_input":"2025-07-02T22:13:35.918263Z","iopub.status.idle":"2025-07-02T22:13:35.925259Z","shell.execute_reply.started":"2025-07-02T22:13:35.918225Z","shell.execute_reply":"2025-07-02T22:13:35.924379Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# ## 2. GPU Initialization\n# Sets up the TensorFlow strategy for Kaggle GPU.\n\n# %%\nprint(\"Checking for GPU support...\")\ntry:\n    # Try to detect GPU (NVIDIA T4/P100 on Kaggle)\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        print(f\"Found {len(gpus)} GPU(s) available.\")\n        # Use the default strategy, which will pick up the single GPU\n        strategy = tf.distribute.get_strategy() \n        print(f\"Using GPU strategy on: {gpus[0].name}\")\n    else:\n        print(\"No GPU device found, defaulting to CPU.\")\n        strategy = tf.distribute.get_strategy() # Default to CPU strategy\nexcept Exception as e:\n    print(f\"Error during GPU detection/setup: {e}\")\n    print(\"Defaulting to CPU.\")\n    strategy = tf.distribute.get_strategy()\n\nprint(f\"Number of accelerators: {strategy.num_replicas_in_sync}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T22:13:35.926279Z","iopub.execute_input":"2025-07-02T22:13:35.926543Z","iopub.status.idle":"2025-07-02T22:13:35.954881Z","shell.execute_reply.started":"2025-07-02T22:13:35.926524Z","shell.execute_reply":"2025-07-02T22:13:35.953870Z"}},"outputs":[{"name":"stdout","text":"Checking for GPU support...\nNo GPU device found, defaulting to CPU.\nNumber of accelerators: 1\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# ## 3. Global Configuration\n# Define global hyperparameters. These must be consistent with your pre-training notebook.\n\n# %%\n# --- Global Configuration (MUST match your pre-training notebook) ---\nMAX_SMILES_LEN = 256 \nMAX_NODES = 419 # This should be the MAX_NODES used during pre-training\nNUM_ATOM_FEATURES = 5 # As defined by your atom_to_feature_vector\nPROJECTION_DIM = 128\nHIDDEN_DIM_GIN = 256 \nEMBED_DIM_TRANSFORMER = 256 \n\nBATCH_SIZE_PER_REPLICA = 64 # Batch size per GPU. Adjust based on T4/P100 memory.\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync # For 1 device, this is BATCH_SIZE_PER_REPLICA\n\n# --- Paths for Pre-trained Encoders ---\n# IMPORTANT: Update these paths to your new Kaggle dataset containing the saved encoders.\n# Example: '/kaggle/input/your-pretrained-encoders-dataset-name/gin_encoder_best'\nPRETRAINED_GIN_ENCODER_PATH = '/kaggle/input/grasp-saved-model/pretraining_checkpoints/gin_encoder_best' \nPRETRAINED_TRANSFORMER_ENCODER_PATH = '/kaggle/input/grasp-saved-model/pretraining_checkpoints/gin_transformer_best' \n\n# --- Path to Preprocessed MoleculeNet TFRecords ---\n# If you ran preprocess_moleculenet.py on Kaggle and saved outputs, update this.\n# If you haven't preprocessed MoleculeNet to TFRecords yet, this path won't be used,\n# and the eager featurization will run.\nMOLECULE_NET_TFRECORDS_DIR = '/kaggle/input/moleculenet-tfrecords/moleculenet_tfrecords' \n# Set to None if you want to use eager featurization from DeepChem directly:\n# MOLECULE_NET_TFRECORDS_DIR = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T22:13:35.957366Z","iopub.execute_input":"2025-07-02T22:13:35.957665Z","iopub.status.idle":"2025-07-02T22:13:35.979640Z","shell.execute_reply.started":"2025-07-02T22:13:35.957644Z","shell.execute_reply":"2025-07-02T22:13:35.978449Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# ## 4. Helper Functions (Data Processing)\n# These functions are crucial for featurizing SMILES to graph and tokenized forms.\n\n# %%\n# --- SMILES Tokenization ---\ndef build_smiles_vocab(smiles_list, max_vocab_size=None):\n    all_chars = set()\n    for smiles in smiles_list:\n        for char in smiles:\n            all_chars.add(char)\n    vocab = sorted(list(all_chars))\n    vocab = ['<pad>', '<unk>', '<cls>', '<eos>'] + vocab\n    if max_vocab_size:\n        vocab = vocab[:max_vocab_size]\n    char_to_idx = {char: i for i, char in enumerate(vocab)}\n    idx_to_char = {i: char for i, char in enumerate(vocab)}\n    print(f\"Built vocabulary of size: {len(vocab)}\")\n    return vocab, char_to_idx, idx_to_char\n\ndef tokenize_smiles(smiles, char_to_idx, max_len):\n    tokens = list(smiles)\n    indexed_tokens = [char_to_idx.get(char, char_to_idx['<unk>']) for char in tokens]\n    if len(indexed_tokens) < max_len:\n        padded_tokens = indexed_tokens + [char_to_idx['<pad>']] * (max_len - len(indexed_tokens))\n    else:\n        padded_tokens = indexed_tokens[:max_len]\n    return np.array(padded_tokens, dtype=np.int32)\n\ndef create_smiles_mask(token_ids, pad_token_id):\n    return tf.cast(token_ids == pad_token_id, tf.bool)\n\n\n# --- SMILES to TensorFlow Graph Conversion ---\ndef atom_to_feature_vector(atom):\n    features = []\n    features.append(atom.GetAtomicNum())\n    features.append(atom.GetDegree())\n    features.append(int(atom.GetHybridization()))\n    features.append(int(atom.GetIsAromatic()))\n    features.append(atom.GetFormalCharge())\n    return np.array(features, dtype=np.float32)\n\ndef smiles_to_tf_graph(smiles_string):\n    mol = Chem.MolFromSmiles(smiles_string)\n    if mol is None:\n        return (np.zeros((0, NUM_ATOM_FEATURES), dtype=np.float32),\n                np.zeros((0, 2), dtype=np.int32),\n                0,\n                0)\n\n    node_features = [atom_to_feature_vector(atom) for atom in mol.GetAtoms()]\n    if not node_features:\n        return (np.zeros((0, NUM_ATOM_FEATURES), dtype=np.float32),\n                np.zeros((0, 2), dtype=np.int32),\n                0,\n                0)\n    node_features = np.array(node_features, dtype=np.float32)\n    num_nodes = len(node_features)\n\n    edge_indices = []\n    for bond in mol.GetBonds():\n        i = bond.GetBeginAtomIdx()\n        j = bond.GetEndAtomIdx()\n        edge_indices.append([i, j])\n        edge_indices.append([j, i])\n\n    num_edges = len(edge_indices)\n    if num_edges == 0:\n        if num_nodes > 0:\n            edge_indices_final = np.empty((0, 2), dtype=np.int32)\n            num_edges_final = 0\n        else:\n            return (np.zeros((0, NUM_ATOM_FEATURES), dtype=np.float32),\n                    np.zeros((0, 2), dtype=np.int32),\n                    0,\n                    0)\n    else:\n        edge_indices_final = np.array(edge_indices, dtype=np.int32)\n        num_edges_final = len(edge_indices_final)\n\n    return node_features, edge_indices_final, num_nodes, num_edges_final\n\ndef featurize_smiles_and_graph_with_label(smiles_input, label): \n    # --- CRITICAL FIX: Ensure smiles_input is a string, converting from Mol if necessary ---\n    # This ensures that `smiles_string` is always a Python string for `tokenize_smiles` and `smiles_to_tf_graph`.\n    smiles_string = None # Initialize to None\n\n    if isinstance(smiles_input, str):\n        smiles_string = smiles_input\n    elif isinstance(smiles_input, Chem.Mol): \n        try:\n            smiles_string = Chem.MolToSmiles(smiles_input, canonical=True)\n        except Exception:\n            pass \n    # If smiles_input is None or other unexpected type, smiles_string remains None\n\n    if smiles_string is None or len(smiles_string) == 0:\n        # If the SMILES string is invalid or unconvertible, return dummy data\n        dummy_node_features = np.zeros((MAX_NODES, NUM_ATOM_FEATURES), dtype=np.float32)\n        dummy_edge_indices = np.zeros((0, 2), dtype=np.int32)\n        dummy_num_nodes = 0\n        dummy_num_edges = 0\n        dummy_token_ids = np.zeros((MAX_SMILES_LEN,), dtype=np.int32) \n        dummy_mask = np.zeros((MAX_SMILES_LEN,), dtype=np.bool_) \n        return (dummy_node_features, dummy_edge_indices, dummy_num_nodes, dummy_num_edges, dummy_token_ids, dummy_mask, label)\n    \n    # Now, smiles_string is guaranteed to be a valid string\n    token_ids = tokenize_smiles(smiles_string, char_to_idx, MAX_SMILES_LEN)\n    mask = create_smiles_mask(token_ids, char_to_idx['<pad>'])\n\n    node_features, edge_indices, num_nodes, num_edges = smiles_to_tf_graph(smiles_string)\n\n    if num_nodes == 0: \n        dummy_node_features = np.zeros((MAX_NODES, NUM_ATOM_FEATURES), dtype=np.float32)\n        dummy_edge_indices = np.zeros((0, 2), dtype=np.int32)\n        dummy_num_nodes = 0\n        dummy_num_edges = 0\n        dummy_token_ids = np.zeros((MAX_SMILES_LEN,), dtype=np.int32) \n        dummy_mask = np.zeros((MAX_SMILES_LEN,), dtype=np.bool_) \n        return (dummy_node_features, dummy_edge_indices, dummy_num_nodes, dummy_num_edges, dummy_token_ids, dummy_mask, label)\n    \n    padded_node_features = np.pad(node_features, [[0, MAX_NODES - num_nodes], [0, 0]])\n    \n    return (padded_node_features, edge_indices, num_nodes, num_edges, token_ids, mask, label)\n\n# --- Global Vocabulary Building (Needed for tokenization) ---\nprint(\"Building vocabulary for MoleculeNet data...\")\ndummy_smiles_for_vocab_build = [\"C\", \"N\", \"O\", \"F\", \"P\", \"S\", \"Cl\", \"Br\", \"I\", \"c\", \"n\", \"=\", \"#\", \"(\", \")\", \"[\", \"]\", \"@\", \"+\", \"-\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"H\", \"B\", \"b\", \"K\", \"k\", \"L\", \"l\", \"M\", \"m\", \"R\", \"r\", \"X\", \"x\", \"Y\", \"y\", \"Z\", \"z\"] \n_, char_to_idx, _ = build_smiles_vocab(dummy_smiles_for_vocab_build)\nVOCAB_SIZE = len(char_to_idx) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T22:29:28.630942Z","iopub.execute_input":"2025-07-02T22:29:28.631279Z","iopub.status.idle":"2025-07-02T22:29:28.654764Z","shell.execute_reply.started":"2025-07-02T22:29:28.631251Z","shell.execute_reply":"2025-07-02T22:29:28.653694Z"}},"outputs":[{"name":"stdout","text":"Building vocabulary for MoleculeNet data...\nBuilt vocabulary of size: 49\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"# ## 6. Model Architecture (Re-define for Loading)\n# These classes must be IDENTICAL to how they were defined in your 2_GRASP_Pretraining.ipynb.\n\n# %%\n# GIN Layer\nclass GINLayer(layers.Layer):\n    def __init__(self, output_dim, activation=None, **kwargs):\n        super(GINLayer, self).__init__(**kwargs)\n        self.output_dim = output_dim\n        self.mlp = keras.Sequential([\n            layers.Dense(output_dim, activation='relu'),\n            layers.Dense(output_dim)\n        ])\n        self.epsilon = self.add_weight(name='epsilon', shape=(),\n                                       initializer=keras.initializers.Constant(0.0),\n                                       trainable=True)\n        self.activation = keras.activations.get(activation)\n\n    def call(self, inputs):\n        node_features, edge_indices_batch, num_nodes_batch = inputs \n        edge_values = tf.ones(tf.shape(edge_indices_batch)[0], dtype=tf.float32)\n        total_nodes_in_batch = tf.shape(node_features)[0]\n        adj_shape = tf.cast([total_nodes_in_batch, total_nodes_in_batch], dtype=tf.int64)\n        adj_sparse = tf.sparse.SparseTensor(indices=tf.cast(edge_indices_batch, tf.int64),\n                                            values=edge_values,\n                                            dense_shape=adj_shape)\n        neighbor_sum = tf.sparse.sparse_dense_matmul(adj_sparse, node_features)\n        combined_features = (1 + self.epsilon) * node_features + neighbor_sum\n        output = self.mlp(combined_features)\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0][0], self.output_dim\n\n# GIN Encoder\nclass GINEncoder(keras.Model):\n    def __init__(self, num_layers, hidden_dim, num_node_features, **kwargs): \n        super(GINEncoder, self).__init__(**kwargs)\n        self.hidden_dim = hidden_dim \n        self.initial_mlp = keras.Sequential([\n            layers.Dense(hidden_dim, activation='relu'),\n            layers.Dense(hidden_dim) \n        ])\n        self.gin_layers = []\n        self.bns = []\n        for i in range(num_layers): \n            self.gin_layers.append(GINLayer(hidden_dim, activation='relu' if i < num_layers - 1 else None))\n            self.bns.append(layers.BatchNormalization())\n    \n    def call(self, inputs):\n        node_features, edge_indices, num_nodes = inputs\n        x = self.initial_mlp(node_features)\n        for i in range(len(self.gin_layers)): \n            x = self.gin_layers[i]((x, edge_indices, num_nodes))\n            x = self.bns[i](x)\n        batch_size = tf.shape(node_features)[0] // MAX_NODES\n        x_reshaped = tf.reshape(x, (batch_size, MAX_NODES, self.hidden_dim)) \n        sequence_mask = tf.sequence_mask(num_nodes, maxlen=MAX_NODES, dtype=tf.float32) \n        sequence_mask = tf.expand_dims(sequence_mask, axis=-1) \n        masked_x = x_reshaped * sequence_mask\n        graph_embedding = tf.reduce_sum(masked_x, axis=1)\n        return graph_embedding\n\n# Transformer Encoder\nclass TransformerEncoder(keras.Model):\n    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, max_seq_len, dropout_rate=0.1, **kwargs):\n        super(TransformerEncoder, self).__init__(**kwargs)\n        self.token_embedding = layers.Embedding(vocab_size, embed_dim)\n        self.positional_embedding = self.add_weight(\n            name=\"pos_embed\",\n            shape=(1, max_seq_len, embed_dim),\n            initializer=\"random_normal\",\n            trainable=True\n        )\n        self.encoder_layers = []\n        for _ in range(num_layers):\n            self.encoder_layers.append([\n                layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=dropout_rate),\n                layers.LayerNormalization(epsilon=1e-6),\n                layers.Dense(embed_dim * 4, activation=\"relu\"),\n                layers.Dense(embed_dim),\n                layers.LayerNormalization(epsilon=1e-6),\n            ])\n        self.final_norm = layers.LayerNormalization(epsilon=1e-6)\n\n    def call(self, inputs, training=False, mask=None):\n        token_ids, padding_mask_bool = inputs \n        x = self.token_embedding(token_ids)\n        x = x + self.positional_embedding[:, :tf.shape(x)[1], :]\n        attention_mask_additive = tf.cast(padding_mask_bool, dtype=tf.float32) * -1e9\n        attention_mask_additive = tf.expand_dims(attention_mask_additive, axis=1) \n        \n        for i, (attention, norm1, ff_dense1, ff_dense2, norm2) in enumerate(self.encoder_layers):\n            attn_output = attention(x, x, attention_mask=attention_mask_additive, training=training)\n            x = norm1(x + attn_output)\n            ff_output = ff_dense2(ff_dense1(x))\n            x = norm2(x + ff_output)\n        \n        expanded_padding_mask = tf.cast(tf.expand_dims(padding_mask_bool, axis=-1), dtype=x.dtype)\n        non_padded_mask = 1.0 - expanded_padding_mask\n        x_masked = x * non_padded_mask\n        sum_embeddings = tf.reduce_sum(x_masked, axis=1)\n        non_padded_len = tf.reduce_sum(non_padded_mask, axis=1)\n        smiles_embedding = sum_embeddings / (non_padded_len + 1e-9)\n        \n        return self.final_norm(smiles_embedding)\n\n# Projection Head\nclass ProjectionHead(keras.Model):\n    def __init__(self, input_dim, output_dim, hidden_dim=256, **kwargs):\n        super(ProjectionHead, self).__init__(**kwargs)\n        self.net = keras.Sequential([\n            layers.Dense(hidden_dim, activation='relu'),\n            layers.Dense(output_dim)\n        ])\n\n    def call(self, x):\n        return self.net(x)\n\n# Downstream Model\nclass DownstreamModel(keras.Model):\n    def __init__(self, pre_trained_gin_encoder, pre_trained_transformer_encoder, output_dim, task_type='classification', **kwargs):\n        super(DownstreamModel, self).__init__(**kwargs)\n        self.gin_encoder = pre_trained_gin_encoder\n        self.transformer_encoder = pre_trained_transformer_encoder\n        \n        # Freeze the pre-trained encoders initially to use them as feature extractors.\n        self.gin_encoder.trainable = False\n        self.transformer_encoder.trainable = False\n\n        input_to_head_dim = HIDDEN_DIM_GIN + EMBED_DIM_TRANSFORMER \n\n        self.classifier_head = keras.Sequential([\n            layers.Dense(input_to_head_dim // 2, activation='relu'), \n            layers.Dropout(0.3), \n            layers.Dense(output_dim, activation='sigmoid' if task_type == 'classification' else 'linear')\n        ])\n        self.task_type = task_type\n\n    def call(self, inputs, training=False):\n        node_features_padded, edge_indices_padded, num_nodes, num_edges, token_ids, smiles_mask = inputs\n\n        node_features_flat = tf.reshape(node_features_padded, (-1, tf.shape(node_features_padded)[2]))\n        \n        batch_size = tf.shape(node_features_padded)[0] \n        edge_mask = tf.sequence_mask(num_edges, maxlen=tf.shape(edge_indices_padded)[1], dtype=tf.bool)\n        valid_edge_indices = tf.cast(tf.boolean_mask(edge_indices_padded, edge_mask), dtype=tf.int32) \n        batch_ids_for_edges = tf.cast(tf.where(edge_mask)[:, 0], dtype=tf.int32)\n        node_offsets_for_edges = tf.range(batch_size) * MAX_NODES\n        offsets = tf.gather(node_offsets_for_edges, batch_ids_for_edges)\n        offsets = tf.expand_dims(offsets, axis=-1)\n        global_edge_indices_filtered = valid_edge_indices + offsets\n        \n        graph_embeddings = self.gin_encoder((node_features_flat, global_edge_indices_filtered, num_nodes), training=training)\n        smiles_embeddings = self.transformer_encoder((token_ids, smiles_mask), training=training)\n\n        combined_embeddings = tf.concat([graph_embeddings, smiles_embeddings], axis=-1)\n\n        output = self.classifier_head(combined_embeddings, training=training)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T22:29:30.450971Z","iopub.execute_input":"2025-07-02T22:29:30.451281Z","iopub.status.idle":"2025-07-02T22:29:30.477685Z","shell.execute_reply.started":"2025-07-02T22:29:30.451260Z","shell.execute_reply":"2025-07-02T22:29:30.476824Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# ## 6. Load Pre-trained Encoders\n# Loads the saved encoders from the pre-training phase.\n\n# %%\nwith strategy.scope(): \n    pre_trained_gin_encoder = tf.saved_model.load(PRETRAINED_GIN_ENCODER_PATH)\n    pre_trained_transformer_encoder = tf.saved_model.load(PRETRAINED_TRANSFORMER_ENCODER_PATH)\n\nprint(\"Pre-trained GIN Encoder and Transformer Encoder loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T22:29:32.713505Z","iopub.execute_input":"2025-07-02T22:29:32.713900Z","iopub.status.idle":"2025-07-02T22:29:34.672601Z","shell.execute_reply.started":"2025-07-02T22:29:32.713876Z","shell.execute_reply":"2025-07-02T22:29:34.670947Z"}},"outputs":[{"name":"stdout","text":"Pre-trained GIN Encoder and Transformer Encoder loaded successfully.\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"# ## 7. Data Preparation for Downstream Tasks (MoleculeNet)\n# This section handles loading and eager featurization of MoleculeNet datasets.\n\n# %%\n# Define the featurization function for a single sample (returns flat tuple of NumPy arrays)\ndef featurize_smiles_and_graph_with_label(smiles_string, label):\n    # This function will be called in eager mode (Python loop)\n    token_ids = tokenize_smiles(smiles_string, char_to_idx, MAX_SMILES_LEN)\n    mask = create_smiles_mask(token_ids, char_to_idx['<pad>'])\n\n    node_features, edge_indices, num_nodes, num_edges = smiles_to_tf_graph(smiles_string)\n\n    if num_nodes == 0:\n        # Return dummy NumPy arrays with correct shapes and dtypes for filtering\n        dummy_node_features = np.zeros((MAX_NODES, NUM_ATOM_FEATURES), dtype=np.float32)\n        dummy_edge_indices = np.zeros((0, 2), dtype=np.int32)\n        dummy_num_nodes = 0\n        dummy_num_edges = 0\n        dummy_token_ids = np.zeros((MAX_SMILES_LEN,), dtype=np.int32)\n        dummy_mask = np.zeros((MAX_SMILES_LEN,), dtype=np.bool_)\n        return (dummy_node_features, dummy_edge_indices, dummy_num_nodes, dummy_num_edges, dummy_token_ids, dummy_mask, label) # Include label in flat return\n    \n    padded_node_features = np.pad(node_features, [[0, MAX_NODES - num_nodes], [0, 0]])\n    \n    # Return a flat tuple of NumPy arrays (will be converted to tf.Tensor by from_tensor_slices)\n    return (padded_node_features, edge_indices, num_nodes, num_edges, token_ids, mask, label)\n\n\n# Function to convert DeepChem dataset to TensorFlow dataset\ndef dc_dataset_to_tf_dataset_for_downstream(dc_dataset, batch_size, shuffle_buffer_size=1000):\n    smiles_list = dc_dataset.X.tolist() \n    labels = dc_dataset.y \n\n    # Handle multi-task labels if necessary (e.g., Tox21 has multiple labels per sample)\n    # Ensure labels are float32 for BCELoss\n    labels_dtype = tf.float32 if labels.dtype == np.bool_ or labels.dtype == np.int_ else labels.dtype\n    label_shape = labels.shape[1:] if labels.ndim > 1 else ()\n\n    print(f\"Pre-featurizing {len(smiles_list)} samples for DeepChem dataset...\")\n    \n    all_processed_inputs = []\n    all_processed_labels = []\n    \n    for i in tqdm(range(len(smiles_list)), desc=\"Eager Featurization\"):\n        smiles_str = smiles_list[i]\n        label = labels[i]\n        \n        # Call the featurization function directly\n        # featurize_smiles_and_graph_with_label returns a flat tuple of 7 NumPy arrays\n        processed_data_flat = featurize_smiles_and_graph_with_label(smiles_str, label)\n            \n        # Filter out invalid samples immediately (num_nodes is at index 2 in the flat tuple)\n        if processed_data_flat[2] > 0: \n            # Append the flat tuple of NumPy arrays\n            all_processed_inputs.append(processed_data_flat[:6]) # Append the 6 input components\n            all_processed_labels.append(processed_data_flat[6])  # Append the label component\n    \n    print(f\"Finished eager featurization. {len(all_processed_inputs)} valid samples processed.\")\n    \n    # Create tf.data.Dataset from these already processed NumPy arrays\n    # from_tensor_slices is suitable here as all_processed_inputs is a list of tuples of NumPy arrays.\n    dataset = tf.data.Dataset.from_tensor_slices((all_processed_inputs, all_processed_labels))\n    \n    dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n\n    # Define padded shapes for the loaded data\n    padded_shapes_inputs_tf = (\n        tf.TensorShape([MAX_NODES, NUM_ATOM_FEATURES]), # node_features\n        tf.TensorShape([None, 2]),                     # edge_indices\n        tf.TensorShape([]),                           # num_nodes\n        tf.TensorShape([]),                           # num_edges\n        tf.TensorSpec(shape=[MAX_SMILES_LEN], dtype=tf.int32).shape, # token_ids, use .shape\n        tf.TensorSpec(shape=[MAX_SMILES_LEN], dtype=tf.bool).shape # smiles_mask, use .shape\n    )\n    padding_values_inputs_tf = (\n        tf.constant(0.0, dtype=tf.float32),\n        tf.constant(0, dtype=tf.int32),\n        tf.constant(0, dtype=tf.int32),\n        tf.constant(0, dtype=tf.int32),\n        tf.constant(char_to_idx['<pad>'], dtype=tf.int32),\n        tf.constant(False, dtype=tf.bool) \n    )\n\n    # Combine input and label shapes for padded_batch\n    padded_shapes_full = (padded_shapes_inputs_tf, label_shape)\n    padding_values_full = (padding_values_inputs_tf, tf.constant(0, dtype=labels_dtype))\n    \n    dataset = dataset.padded_batch(batch_size, padded_shapes=padded_shapes_full, padding_values=padding_values_full, drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T22:29:34.674300Z","iopub.execute_input":"2025-07-02T22:29:34.674612Z","iopub.status.idle":"2025-07-02T22:29:34.690024Z","shell.execute_reply.started":"2025-07-02T22:29:34.674588Z","shell.execute_reply":"2025-07-02T22:29:34.688859Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# ## 8. Benchmarking Loop for BBBP, Tox21, ESOL\n# Executes fine-tuning and evaluation for each MoleculeNet task.\n\n# %%\n# Define the global `char_to_idx` and `VOCAB_SIZE` first.\n# You need to ensure this matches the vocab built during pre-training.\n# For a full setup, you would save the vocab from pre-training and load it here.\n# For this example, we'll build it from a small set, assuming it's representative enough\n# for the characters present in MoleculeNet, but this is a potential source of errors\n# if MoleculeNet has chars not in this small set.\n# Ideally, load your pre-training vocab.\nprint(\"Building vocabulary for MoleculeNet data...\")\ndummy_smiles_for_vocab_build = [\"C\", \"N\", \"O\", \"F\", \"P\", \"S\", \"Cl\", \"Br\", \"I\", \"c\", \"n\", \"=\", \"#\", \"(\", \")\", \"[\", \"]\", \"@\", \"+\", \"-\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"H\", \"B\", \"b\", \"K\", \"k\", \"L\", \"l\", \"M\", \"m\", \"R\", \"r\", \"X\", \"x\", \"Y\", \"y\", \"Z\", \"z\"] \n_, char_to_idx, _ = build_smiles_vocab(dummy_smiles_for_vocab_build)\nVOCAB_SIZE = len(char_to_idx) \n\n\n# --- BBBP (Blood-Brain Barrier Permeability) ---\nprint(\"\\n--- Benchmarking BBBP ---\")\nbbbp_tasks, bbbp_datasets, bbbp_transformers = dc.molnet.load_bbbp(featurizer='Raw', splitter='scaffold')\ntrain_bbbp, valid_bbbp, test_bbbp = bbbp_datasets\nbbbp_output_dim = len(bbbp_tasks) \n\ntrain_bbbp_tf = dc_dataset_to_tf_dataset_for_downstream(train_bbbp, GLOBAL_BATCH_SIZE)\nvalid_bbbp_tf = dc_dataset_to_tf_dataset_for_downstream(valid_bbbp, GLOBAL_BATCH_SIZE)\ntest_bbbp_tf = dc_dataset_to_tf_dataset_for_downstream(test_bbbp, GLOBAL_BATCH_SIZE)\n\nwith strategy.scope():\n    bbbp_model = DownstreamModel(pre_trained_gin_encoder, pre_trained_transformer_encoder, \n                                 output_dim=bbbp_output_dim, task_type='classification')\n    bbbp_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n                       loss=keras.losses.BinaryCrossentropy(from_logits=False),\n                       metrics=[keras.metrics.AUC(name='auc'), keras.metrics.BinaryAccuracy(name='accuracy')])\n    \n    # Build the model with a dummy input shape for the first call\n    dummy_input_for_build = (\n        tf.zeros((GLOBAL_BATCH_SIZE, MAX_NODES, NUM_ATOM_FEATURES), dtype=tf.float32), \n        tf.zeros((GLOBAL_BATCH_SIZE, 10, 2), dtype=tf.int32), \n        tf.constant([50]*GLOBAL_BATCH_SIZE, dtype=tf.int32),  \n        tf.constant([10]*GLOBAL_BATCH_SIZE, dtype=tf.int32),  \n        tf.zeros((GLOBAL_BATCH_SIZE, MAX_SMILES_LEN), dtype=tf.int32),\n        tf.zeros((GLOBAL_BATCH_SIZE, MAX_SMILES_LEN), dtype=tf.bool)\n    )\n    _ = bbbp_model(dummy_input_for_build, training=False)\n    print(\"BBBP DownstreamModel built successfully.\")\n\nprint(\"Fine-tuning BBBP model...\")\nFINE_TUNE_EPOCHS = 10 \nbbbp_model.fit(train_bbbp_tf, epochs=FINE_TUNE_EPOCHS, validation_data=valid_bbbp_tf)\n\nprint(\"\\nEvaluating BBBP model on test set...\")\nbbbp_results = bbbp_model.evaluate(test_bbbp_tf)\nprint(f\"BBBP Test Loss: {bbbp_results[0]:.4f}, Test AUC: {bbbp_results[1]:.4f}, Test Accuracy: {bbbp_results[2]:.4f}\")\n\n\n# --- Tox21 ---\nprint(\"\\n--- Benchmarking Tox21 ---\")\ntox21_tasks, tox21_datasets, tox21_transformers = dc.molnet.load_tox21(featurizer='Raw', splitter='scaffold')\ntrain_tox21, valid_tox21, test_tox21 = tox21_datasets\ntox21_output_dim = len(tox21_tasks) \n\ntrain_tox21_tf = dc_dataset_to_tf_dataset_for_downstream(tox21_train, GLOBAL_BATCH_SIZE)\nvalid_tox21_tf = dc_dataset_to_tf_dataset_for_downstream(tox21_valid, GLOBAL_BATCH_SIZE)\ntest_tox21_tf = dc_dataset_to_tf_dataset_for_downstream(tox21_test, GLOBAL_BATCH_SIZE)\n\nwith strategy.scope():\n    tox21_model = DownstreamModel(pre_trained_gin_encoder, pre_trained_transformer_encoder, \n                                  output_dim=tox21_output_dim, task_type='classification')\n    tox21_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n                        loss=keras.losses.BinaryCrossentropy(from_logits=False),\n                        metrics=[keras.metrics.AUC(name='auc', multi_label=True), keras.metrics.BinaryAccuracy(name='accuracy')])\n    \n    _ = tox21_model(dummy_input_for_build, training=False)\n    print(\"Tox21 DownstreamModel built successfully.\")\n\nprint(\"Fine-tuning Tox21 model...\")\ntox21_model.fit(train_tox21_tf, epochs=FINE_TUNE_EPOCHS, validation_data=valid_tox21_tf)\n\nprint(\"\\nEvaluating Tox21 model on test set...\")\ntox21_results = tox21_model.evaluate(test_tox21_tf)\nprint(f\"Tox21 Test Loss: {tox21_results[0]:.4f}, Test AUC: {tox21_results[1]:.4f}, Test Accuracy: {tox21_results[2]:.4f}\")\n\n\n# --- ESOL (Solubility Estimation) ---\nprint(\"\\n--- Benchmarking ESOL ---\")\nesol_tasks, esol_datasets, esol_transformers = dc.molnet.load_esol(featurizer='Raw', splitter='scaffold')\ntrain_esol, valid_esol, test_esol = esol_datasets\nesol_output_dim = len(esol_tasks) \n\ntrain_esol_tf = dc_dataset_to_tf_dataset_for_downstream(esol_train, GLOBAL_BATCH_SIZE)\nvalid_esol_tf = dc_dataset_to_tf_dataset_for_downstream(esol_valid, GLOBAL_BATCH_SIZE)\ntest_esol_tf = dc_dataset_to_tf_dataset_for_downstream(esol_test, GLOBAL_BATCH_SIZE)\n\nwith strategy.scope():\n    esol_model = DownstreamModel(pre_trained_gin_encoder, pre_trained_transformer_encoder, \n                                 output_dim=esol_output_dim, task_type='regression')\n    esol_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n                       loss=keras.losses.MeanSquaredError(),\n                       metrics=[keras.metrics.MeanAbsoluteError(name='mae')])\n    \n    _ = esol_model(dummy_input_for_build, training=False)\n    print(\"ESOL DownstreamModel built successfully.\")\n\nprint(\"Fine-tuning ESOL model...\")\nesol_model.fit(train_esol_tf, epochs=FINE_TUNE_EPOCHS, validation_data=valid_esol_tf)\n\nprint(\"\\nEvaluating ESOL model on test set...\")\nesol_results = esol_model.evaluate(test_esol_tf)\nprint(f\"ESOL Test Loss: {esol_results[0]:.4f}, Test MAE: {esol_results[1]:.4f}\")\n\n\nprint(\"\\nAll MoleculeNet benchmarking complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T22:29:34.690896Z","iopub.execute_input":"2025-07-02T22:29:34.691124Z","iopub.status.idle":"2025-07-02T22:29:34.867644Z","shell.execute_reply.started":"2025-07-02T22:29:34.691108Z","shell.execute_reply":"2025-07-02T22:29:34.866188Z"}},"outputs":[{"name":"stdout","text":"Building vocabulary for MoleculeNet data...\nBuilt vocabulary of size: 49\n\n--- Benchmarking BBBP ---\nPre-featurizing 1631 samples for DeepChem dataset...\n","output_type":"stream"},{"name":"stderr","text":"Eager Featurization:   0%|          | 0/1631 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3541040436.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mbbbp_output_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbbp_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrain_bbbp_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc_dataset_to_tf_dataset_for_downstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_bbbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGLOBAL_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mvalid_bbbp_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc_dataset_to_tf_dataset_for_downstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_bbbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGLOBAL_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtest_bbbp_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc_dataset_to_tf_dataset_for_downstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_bbbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGLOBAL_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1906248238.py\u001b[0m in \u001b[0;36mdc_dataset_to_tf_dataset_for_downstream\u001b[0;34m(dc_dataset, batch_size, shuffle_buffer_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Call the featurization function directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# featurize_smiles_and_graph_with_label returns a flat tuple of 7 NumPy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mprocessed_data_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturize_smiles_and_graph_with_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Filter out invalid samples immediately (num_nodes is at index 2 in the flat tuple)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1906248238.py\u001b[0m in \u001b[0;36mfeaturize_smiles_and_graph_with_label\u001b[0;34m(smiles_string, label)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeaturize_smiles_and_graph_with_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# This function will be called in eager mode (Python loop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SMILES_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_smiles_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1516195068.py\u001b[0m in \u001b[0;36mtokenize_smiles\u001b[0;34m(smiles, char_to_idx, max_len)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mindexed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'Mol' object is not iterable"],"ename":"TypeError","evalue":"'Mol' object is not iterable","output_type":"error"}],"execution_count":64},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}