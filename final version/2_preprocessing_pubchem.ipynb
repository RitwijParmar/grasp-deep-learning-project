{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing SMILES from: pubchem_smiles_for_pretraining.txt\n",
      "Targeting 1000000 samples.\n",
      "Output TFRecords will be saved to: pubchem_tfrecords_1M in 100 shards.\n",
      "Configured MAX_NODES: 419, NUM_ATOM_FEATURES: 5, MAX_SMILES_LEN: 256\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress non-critical RDKit warnings\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.warning')\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "\n",
    "# --- Global Constants (MUST match your pre-training notebook) ---\n",
    "# Update this path to where your pubchem_smiles_for_pretraining.txt is located\n",
    "SMILES_FILE_PATH = 'pubchem_smiles_for_pretraining.txt' \n",
    "\n",
    "# MAX_NODES: This should be the maximum number of nodes found in your ENTIRE PubChem dataset\n",
    "# or a very large representative sample (e.g., first 1-5M samples).\n",
    "# 419 was from a 100k sample. For 1M, it might be slightly higher, but 419 is a reasonable\n",
    "# starting point. You can run a quick max_nodes check on 1M samples if you want to be precise.\n",
    "MAX_NODES = 419 \n",
    "\n",
    "NUM_ATOM_FEATURES = 5 # As defined by your atom_to_feature_vector\n",
    "MAX_SMILES_LEN = 256 # Max sequence length for Transformer.\n",
    "\n",
    "# --- Configuration for this preprocessing run ---\n",
    "SAMPLES_TO_PROCESS_IN_THIS_RUN = 1_000_000 # Target: Process 1 million samples\n",
    "OUTPUT_TFRECORD_DIR = 'pubchem_tfrecords_1M' # Output directory for 1 million samples\n",
    "NUM_SHARDS = 100 # Number of TFRecord files to split the data into (e.g., 100 files for 1M samples, ~100MB each)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_TFRECORD_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Preprocessing SMILES from: {SMILES_FILE_PATH}\")\n",
    "print(f\"Targeting {SAMPLES_TO_PROCESS_IN_THIS_RUN} samples.\")\n",
    "print(f\"Output TFRecords will be saved to: {OUTPUT_TFRECORD_DIR} in {NUM_SHARDS} shards.\")\n",
    "print(f\"Configured MAX_NODES: {MAX_NODES}, NUM_ATOM_FEATURES: {NUM_ATOM_FEATURES}, MAX_SMILES_LEN: {MAX_SMILES_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions (Copied from your pre-training notebook) ---\n",
    "\n",
    "# --- SMILES Tokenization ---\n",
    "def build_smiles_vocab(smiles_list, max_vocab_size=None):\n",
    "    all_chars = set()\n",
    "    for smiles in smiles_list:\n",
    "        for char in smiles:\n",
    "            all_chars.add(char)\n",
    "    vocab = sorted(list(all_chars))\n",
    "    vocab = ['<pad>', '<unk>', '<cls>', '<eos>'] + vocab\n",
    "    if max_vocab_size:\n",
    "        vocab = vocab[:max_vocab_size]\n",
    "    char_to_idx = {char: i for i, char in enumerate(vocab)}\n",
    "    idx_to_char = {i: char for i, char in enumerate(vocab)}\n",
    "    print(f\"Built vocabulary of size: {len(vocab)}\")\n",
    "    return vocab, char_to_idx, idx_to_char\n",
    "\n",
    "def tokenize_smiles(smiles, char_to_idx, max_len):\n",
    "    tokens = list(smiles)\n",
    "    indexed_tokens = [char_to_idx.get(char, char_to_idx['<unk>']) for char in tokens]\n",
    "    if len(indexed_tokens) < max_len:\n",
    "        padded_tokens = indexed_tokens + [char_to_idx['<pad>']] * (max_len - len(indexed_tokens))\n",
    "    else:\n",
    "        padded_tokens = indexed_tokens[:max_len]\n",
    "    return np.array(padded_tokens, dtype=np.int32)\n",
    "\n",
    "def create_smiles_mask(token_ids, pad_token_id):\n",
    "    return tf.cast(token_ids == pad_token_id, tf.bool)\n",
    "\n",
    "\n",
    "# --- SMILES to TensorFlow Graph Conversion ---\n",
    "def atom_to_feature_vector(atom):\n",
    "    features = []\n",
    "    features.append(atom.GetAtomicNum())\n",
    "    features.append(atom.GetDegree())\n",
    "    features.append(int(atom.GetHybridization()))\n",
    "    features.append(int(atom.GetIsAromatic()))\n",
    "    features.append(atom.GetFormalCharge())\n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "def smiles_to_tf_graph(smiles_string):\n",
    "    mol = Chem.MolFromSmiles(smiles_string)\n",
    "    if mol is None:\n",
    "        return (np.zeros((0, NUM_ATOM_FEATURES), dtype=np.float32),\n",
    "                np.zeros((0, 2), dtype=np.int32),\n",
    "                0,\n",
    "                0)\n",
    "\n",
    "    node_features = [atom_to_feature_vector(atom) for atom in mol.GetAtoms()]\n",
    "    if not node_features:\n",
    "        return (np.zeros((0, NUM_ATOM_FEATURES), dtype=np.float32),\n",
    "                np.zeros((0, 2), dtype=np.int32),\n",
    "                0,\n",
    "                0)\n",
    "    node_features = np.array(node_features, dtype=np.float32)\n",
    "    num_nodes = len(node_features)\n",
    "\n",
    "    edge_indices = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices.append([i, j])\n",
    "        edge_indices.append([j, i])\n",
    "\n",
    "    num_edges = len(edge_indices)\n",
    "    if num_edges == 0:\n",
    "        if num_nodes > 0:\n",
    "            edge_indices_final = np.empty((0, 2), dtype=np.int32)\n",
    "            num_edges_final = 0\n",
    "        else:\n",
    "            return (np.zeros((0, NUM_ATOM_FEATURES), dtype=np.float32),\n",
    "                    np.zeros((0, 2), dtype=np.int32),\n",
    "                    0,\n",
    "                    0)\n",
    "    else:\n",
    "        edge_indices_final = np.array(edge_indices, dtype=np.int32)\n",
    "        num_edges_final = len(edge_indices_final)\n",
    "\n",
    "    return node_features, edge_indices_final, num_nodes, num_edges_final\n",
    "\n",
    "\n",
    "# Define the featurization function that returns a flat tuple of tensors\n",
    "def featurize_smiles_and_graph(smiles_string):\n",
    "    token_ids = tokenize_smiles(smiles_string, char_to_idx, MAX_SMILES_LEN)\n",
    "    mask = create_smiles_mask(token_ids, char_to_idx['<pad>'])\n",
    "\n",
    "    node_features, edge_indices, num_nodes, num_edges = smiles_to_tf_graph(smiles_string)\n",
    "\n",
    "    if num_nodes == 0:\n",
    "        dummy_node_features = np.zeros((MAX_NODES, NUM_ATOM_FEATURES), dtype=np.float32)\n",
    "        dummy_edge_indices = np.zeros((0, 2), dtype=np.int32)\n",
    "        dummy_num_nodes = 0\n",
    "        dummy_num_edges = 0\n",
    "        return (dummy_node_features, dummy_edge_indices, dummy_num_nodes, dummy_num_edges, token_ids, mask)\n",
    "    \n",
    "    padded_node_features = np.pad(node_features, [[0, MAX_NODES - num_nodes], [0, 0]])\n",
    "    \n",
    "    return (padded_node_features, edge_indices, num_nodes, num_edges, token_ids, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TFRecord Serialization Functions ---\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, tf.Tensor):\n",
    "        value = value.numpy()\n",
    "    if isinstance(value, np.ndarray):\n",
    "        value = value.tobytes()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_pretraining_example(node_feat_padded, edge_idx, num_nodes, num_edges, token_ids, smiles_mask):\n",
    "    feature = {\n",
    "        'node_feat_padded': _bytes_feature(node_feat_padded),\n",
    "        'edge_idx': _bytes_feature(edge_idx),\n",
    "        'num_nodes': _int64_feature(num_nodes),\n",
    "        'num_edges': _int64_feature(num_edges),\n",
    "        'token_ids': _bytes_feature(token_ids),\n",
    "        'smiles_mask': _bytes_feature(smiles_mask),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PubChem preprocessing script...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading SMILES: 1000000it [00:00, 3626185.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000000 SMILES strings from file for processing.\n",
      "Built vocabulary of size: 71\n",
      "Vocabulary built with 71 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing & Saving to TFRecords:   0%|          | 0/1000000 [00:00<?, ?it/s]2025-07-01 04:20:35.641112: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-07-01 04:20:35.641148: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-07-01 04:20:35.641159: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-07-01 04:20:35.641199: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-07-01 04:20:35.641210: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Featurizing & Saving to TFRecords: 100%|██████████| 1000000/1000000 [13:40<00:00, 1218.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished featurizing and saving 1000000 valid samples to 100 TFRecord shards in pubchem_tfrecords_1M.\n",
      "PubChem preprocessing script finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Main Preprocessing Loop and TFRecord Saving ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting PubChem preprocessing script...\")\n",
    "\n",
    "    # Load SMILES data for the target number of samples\n",
    "    # This reads only the required number of lines from the file.\n",
    "    all_smiles_list = []\n",
    "    with open(SMILES_FILE_PATH, 'r') as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=\"Loading SMILES\")):\n",
    "            if i >= SAMPLES_TO_PROCESS_IN_THIS_RUN:\n",
    "                break\n",
    "            all_smiles_list.append(line.strip())\n",
    "    print(f\"Loaded {len(all_smiles_list)} SMILES strings from file for processing.\")\n",
    "\n",
    "    # Build vocabulary from the loaded SMILES list\n",
    "    global vocab, char_to_idx, idx_to_char, VOCAB_SIZE\n",
    "    vocab, char_to_idx, idx_to_char = build_smiles_vocab(all_smiles_list)\n",
    "    VOCAB_SIZE = len(vocab)\n",
    "    print(f\"Vocabulary built with {VOCAB_SIZE} tokens.\")\n",
    "\n",
    "    # Initialize TFRecord Writers\n",
    "    writers = []\n",
    "    for i in range(NUM_SHARDS):\n",
    "        writers.append(tf.io.TFRecordWriter(os.path.join(OUTPUT_TFRECORD_DIR, f'pubchem_shard_{i:03d}.tfrecord')))\n",
    "    \n",
    "    num_processed = 0\n",
    "    # Iterate through the selected subset of SMILES for featurization and saving\n",
    "    for i, smiles_str in enumerate(tqdm(all_smiles_list, desc=\"Featurizing & Saving to TFRecords\")):\n",
    "        processed_data = featurize_smiles_and_graph(smiles_str)\n",
    "        \n",
    "        # Filter out invalid samples (where num_nodes was 0)\n",
    "        if processed_data[2] > 0: # num_nodes is the 3rd element (index 2) in the returned flat tuple\n",
    "            example = serialize_pretraining_example(*processed_data)\n",
    "            writers[num_processed % NUM_SHARDS].write(example.SerializeToString())\n",
    "            num_processed += 1\n",
    "        \n",
    "    for writer in writers:\n",
    "        writer.close()\n",
    "            \n",
    "    print(f\"Finished featurizing and saving {num_processed} valid samples to {NUM_SHARDS} TFRecord shards in {OUTPUT_TFRECORD_DIR}.\")\n",
    "    print(\"PubChem preprocessing script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7768225,
     "sourceId": 12323934,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (tf-metal)",
   "language": "python",
   "name": "tf-metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
