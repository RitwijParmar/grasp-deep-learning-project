{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport deepchem as dc\nimport numpy as np\nfrom rdkit import Chem\nfrom rdkit import rdBase\nimport os\nfrom tqdm.auto import tqdm\n\n# Suppress non-critical RDKit messages to keep the output clean\nrdBase.DisableLog('rdApp.warning')\nrdBase.DisableLog('rdApp.error')\n\n## --- CONFIGURATION ---\n# List of MoleculeNet datasets you want to process\nDATASET_NAMES = ['Tox21', 'BBBP', 'ESOL']\n\n# Parameters that MUST match your pre-training setup\nMAX_SMILES_LEN = 256\nMAX_NODES = 419\n# Match the number of features your pre-training GIN model expects\nNUM_ATOM_FEATURES = 5\n\n# Output directory for the new TFRecord files. Using a new name is recommended.\nOUTPUT_DIR = 'moleculenet_tfrecords_v2'\nos.makedirs(OUTPUT_DIR, exist_ok=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- TOKENIZER (Must be identical to your pre-training notebook) ---\n# We use the same hardcoded vocab to ensure consistency.\nVOCAB = ['<pad>', '<unk>', '<cls>', '<eos>'] + sorted(list(\"CN=OP#SFClBI()[]@+1234567890HBrKkLlMmRrXxYyZzcbn\"))\nCHAR_TO_IDX = {char: i for i, char in enumerate(VOCAB)}\nPAD_TOKEN_ID = CHAR_TO_IDX['<pad>']\nprint(f\"Using a consistent vocabulary of size: {len(VOCAB)}\")\n\n\ndef tokenize_smiles(smiles, max_len):\n    \"\"\"Converts a SMILES string to a padded sequence of token IDs.\"\"\"\n    tokens = list(smiles)\n    indexed_tokens = [CHAR_TO_IDX.get(char, CHAR_TO_IDX['<unk>']) for char in tokens]\n    # Truncate if longer than max_len\n    if len(indexed_tokens) > max_len:\n        indexed_tokens = indexed_tokens[:max_len]\n    # Pad if shorter\n    padded_tokens = indexed_tokens + [PAD_TOKEN_ID] * (max_len - len(indexed_tokens))\n    return np.array(padded_tokens, dtype=np.int32)\n\n\n# --- GRAPH FEATURIZER (From your pre-training notebook) ---\ndef atom_to_feature_vector(atom):\n    \"\"\"Generates a feature vector for a single atom.\"\"\"\n    return np.array([\n        atom.GetAtomicNum(),\n        atom.GetDegree(),\n        int(atom.GetHybridization()),\n        int(atom.GetIsAromatic()),\n        atom.GetFormalCharge()\n    ], dtype=np.float32)\n\ndef smiles_to_graph_and_tokens(smiles_string, max_nodes, max_len):\n    \"\"\"\n    Converts a SMILES string to all necessary features for the GRASP model.\n    Returns: A tuple containing padded atom features, edge index list, number of nodes, and token IDs.\n    \"\"\"\n    mol = Chem.MolFromSmiles(smiles_string)\n    if not mol or mol.GetNumAtoms() > max_nodes:\n        return None # Skip molecules that are invalid or too large\n\n    # --- Graph Features ---\n    # 1. Atom Features\n    atom_features = np.array([atom_to_feature_vector(atom) for atom in mol.GetAtoms()])\n    num_nodes = len(atom_features)\n    # Pad atom features to max_nodes\n    padded_atom_features = np.zeros((max_nodes, NUM_ATOM_FEATURES), dtype=np.float32)\n    padded_atom_features[:num_nodes] = atom_features\n\n    # 2. Edge Index List (Correct format for your model)\n    edge_indices = []\n    for bond in mol.GetBonds():\n        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n        edge_indices.extend([[i, j], [j, i]]) # Add edges in both directions\n    \n    edge_index_array = np.array(edge_indices, dtype=np.int32) if edge_indices else np.zeros((0, 2), dtype=np.int32)\n\n    # --- SMILES Features ---\n    token_ids = tokenize_smiles(smiles_string, max_len)\n    \n    return padded_atom_features, edge_index_array, np.array([num_nodes], dtype=np.int32), token_ids\n\n\n# --- TFRECORD SERIALIZATION ---\n# Helper functions to create TensorFlow features\ndef _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))): # if value is a tensor\n        value = value.numpy() # get its numpy value\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef create_tf_example(atom_features, edge_index, num_nodes, token_ids, label):\n    \"\"\"Creates a tf.train.Example proto from a single molecule's data.\"\"\"\n    feature = {\n        'atom_features': _bytes_feature(tf.io.serialize_tensor(atom_features)),\n        'edge_index': _bytes_feature(tf.io.serialize_tensor(edge_index)),\n        'num_nodes': _bytes_feature(tf.io.serialize_tensor(num_nodes)),\n        'token_ids': _bytes_feature(tf.io.serialize_tensor(token_ids)),\n        'label': _bytes_feature(tf.io.serialize_tensor(label)),\n    }\n    return tf.train.Example(features=tf.train.Features(feature=feature))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# --- MAIN PROCESSING LOOP ---\ndef process_and_save_datasets():\n    \"\"\"Main function to load, process, and save all specified datasets.\"\"\"\n    for name in DATASET_NAMES:\n        print(f\"\\n--- Processing dataset: {name} ---\")\n        \n        # 1. Load data using DeepChem's MolNet\n        if name == 'Tox21':\n            tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='Raw', splitter='scaffold')\n        elif name == 'BBBP':\n            tasks, datasets, transformers = dc.molnet.load_bbbp(featurizer='Raw', splitter='scaffold')\n        elif name == 'ESOL':\n            tasks, datasets, transformers = dc.molnet.load_esol(featurizer='Raw', splitter='random')\n        else:\n            continue\n            \n        train_dataset, valid_dataset, test_dataset = datasets\n        \n        # 2. Process each split (train, valid, test)\n        for split_name, dataset in [('train', train_dataset), ('valid', valid_dataset), ('test', test_dataset)]:\n            output_filename = os.path.join(OUTPUT_DIR, f'{name.lower()}_{split_name}.tfrecord')\n            \n            with tf.io.TFRecordWriter(output_filename) as writer:\n                processed_count = 0\n                # Use tqdm for a nice progress bar\n                for smiles, label in tqdm(zip(dataset.ids, dataset.y), total=len(dataset), desc=f\"  Writing {split_name}\"):\n                    \n                    # Featurize the SMILES into graph and token representations\n                    featurized_data = smiles_to_graph_and_tokens(smiles, MAX_NODES, MAX_SMILES_LEN)\n                    if featurized_data is None:\n                        continue # Skip if molecule is invalid or too large\n                    \n                    atom_f, edge_idx, num_n, token_ids = featurized_data\n                    \n                    # Ensure label is in the correct format (especially for multi-task like Tox21)\n                    label_np = np.array(label, dtype=np.float32)\n                    \n                    # Create and write the TFRecord example\n                    tf_example = create_tf_example(atom_f, edge_idx, num_n, token_ids, label_np)\n                    writer.write(tf_example.SerializeToString())\n                    processed_count += 1\n\n            print(f\"  âœ… Saved {processed_count} molecules to {output_filename}\")\n\n    print(f\"\\n--- All datasets processed successfully and saved in '{OUTPUT_DIR}'! ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the entire preprocessing pipeline\nprocess_and_save_datasets()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}