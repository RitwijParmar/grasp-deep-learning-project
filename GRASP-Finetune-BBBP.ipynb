{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a02080",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import deepchem as dc # For MoleculeNet datasets\n",
    "\n",
    "# --- 0. TPU/GPU/CPU Initialization ---\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print('Not running on TPU, defaulting to GPU/CPU.')\n",
    "    tpu = None\n",
    "    strategy = tf.distribute.get_strategy() # Default to GPU or CPU strategy\n",
    "\n",
    "print(f\"Number of accelerators: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "\n",
    "# --- 1. Global Configuration (Must match pre-training notebook) ---\n",
    "# These values MUST be consistent with your 2_GRASP_Pretraining.ipynb notebook.\n",
    "# If they are different, the loaded models/data processing will be incompatible.\n",
    "\n",
    "MAX_SMILES_LEN = 256\n",
    "MAX_NODES = 419 # Or whatever max_nodes_found was in your 1_PubChem_SMILES_Extraction.ipynb output\n",
    "NUM_ATOM_FEATURES = 5 # As defined in your atom_to_feature_vector\n",
    "PROJECTION_DIM = 128\n",
    "HIDDEN_DIM_GIN = 256 \n",
    "EMBED_DIM_TRANSFORMER = 256 \n",
    "\n",
    "\n",
    "# --- 2. Load Pre-trained Encoders ---\n",
    "# Assuming your 2_GRASP_Pretraining.ipynb saved these to /kaggle/working/\n",
    "# When you commit notebook 2, these will become available as a Kaggle dataset.\n",
    "# You will need to \"Add Data\" to this notebook (3_GRASP_Finetune_BBBP.ipynb)\n",
    "# and select the output of your 2_GRASP_Pretraining.ipynb.\n",
    "# The path will typically be something like /kaggle/input/name-of-your-notebook2-commit/\n",
    "# e.g., /kaggle/input/grasp-pretraining-model-v1/gin_encoder_pretrained\n",
    "# Make sure these paths are correct after you add the data!\n",
    "\n",
    "PRETRAINED_GIN_ENCODER_PATH = '/kaggle/input/your-2-pretraining-notebook-output/gin_encoder_pretrained' # <--- UPDATE THIS PATH\n",
    "PRETRAINED_TRANSFORMER_ENCODER_PATH = '/kaggle/input/your-2-pretraining-notebook-output/transformer_encoder_pretrained' # <--- UPDATE THIS PATH\n",
    "\n",
    "with strategy.scope(): # Load within strategy scope for TPU compatibility\n",
    "    # Custom objects are required if your model layers are custom Keras Layers\n",
    "    # like GINLayer. You must pass them when loading.\n",
    "    custom_objects = {\n",
    "        'GINLayer': GINLayer, # Define GINLayer class here as well if not already\n",
    "        'GINEncoder': GINEncoder, # Define GINEncoder class here as well if not already\n",
    "        'TransformerEncoder': TransformerEncoder, # Define TransformerEncoder class here as well if not already\n",
    "        # Add other custom layers/losses if any from your pre-training model\n",
    "    }\n",
    "    \n",
    "    # Load the saved encoders\n",
    "    # Ensure all original classes (GINLayer, GINEncoder, TransformerEncoder) are defined\n",
    "    # in this notebook before loading, even if they aren't directly instantiated,\n",
    "    # as tf.saved_model.load needs their definitions to reconstruct the graph.\n",
    "    pre_trained_gin_encoder = tf.saved_model.load(PRETRAINED_GIN_ENCODER_PATH, options=tf.saved_model.LoadOptions(experimental_io_device='/job:localhost'))\n",
    "    pre_trained_transformer_encoder = tf.saved_model.load(PRETRAINED_TRANSFORMER_ENCODER_PATH, options=tf.saved_model.LoadOptions(experimental_io_device='/job:localhost'))\n",
    "\n",
    "print(\"Pre-trained GIN Encoder and Transformer Encoder loaded successfully.\")\n",
    "\n",
    "# --- Define Helper Functions (Copied from pre-training notebook) ---\n",
    "# These functions are needed to preprocess the MoleculeNet data in the same way\n",
    "# your pre-trained encoders expect.\n",
    "\n",
    "# --- SMILES Tokenization ---\n",
    "# A simple character-level tokenizer for demonstration.\n",
    "def build_smiles_vocab(smiles_list, max_vocab_size=None):\n",
    "    all_chars = set()\n",
    "    for smiles in smiles_list:\n",
    "        for char in smiles:\n",
    "            all_chars.add(char)\n",
    "    vocab = sorted(list(all_chars))\n",
    "    vocab = ['<pad>', '<unk>', '<cls>', '<eos>'] + vocab\n",
    "    if max_vocab_size:\n",
    "        vocab = vocab[:max_vocab_size]\n",
    "    char_to_idx = {char: i for i, char in enumerate(vocab)}\n",
    "    idx_to_char = {i: char for i, char in enumerate(vocab)}\n",
    "    return vocab, char_to_idx, idx_to_char\n",
    "\n",
    "# Temporarily build vocab from a small sample for featurization logic check\n",
    "# In a real scenario, you'd load the full vocab saved from pre-training\n",
    "# or ensure a consistent vocab build process.\n",
    "# For simplicity, we assume your pre-training vocab contains all chars needed.\n",
    "# For full reproducibility, save/load the vocab from pre-training.\n",
    "dummy_smiles_for_vocab = [\"C\", \"CC\", \"CCC\"] # Small dummy for local function def\n",
    "vocab, char_to_idx, idx_to_char = build_smiles_vocab(dummy_smiles_for_vocab)\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "\n",
    "def tokenize_smiles(smiles, char_to_idx, max_len):\n",
    "    tokens = list(smiles)\n",
    "    indexed_tokens = [char_to_idx.get(char, char_to_idx['<unk>']) for char in tokens]\n",
    "    if len(indexed_tokens) < max_len:\n",
    "        padded_tokens = indexed_tokens + [char_to_idx['<pad>']] * (max_len - len(indexed_tokens))\n",
    "    else:\n",
    "        padded_tokens = indexed_tokens[:max_len]\n",
    "    return np.array(padded_tokens, dtype=np.int32)\n",
    "\n",
    "def create_smiles_mask(token_ids, pad_token_id):\n",
    "    return tf.cast(token_ids == pad_token_id, tf.bool)\n",
    "\n",
    "\n",
    "# --- SMILES to TensorFlow Graph Conversion Helpers ---\n",
    "NUM_ATOM_FEATURES = 5 # Must match your pre-training notebook\n",
    "\n",
    "def atom_to_feature_vector(atom):\n",
    "    features = []\n",
    "    features.append(atom.GetAtomicNum())\n",
    "    features.append(atom.GetDegree())\n",
    "    features.append(int(atom.GetHybridization()))\n",
    "    features.append(int(atom.GetIsAromatic()))\n",
    "    features.append(atom.GetFormalCharge())\n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "def bond_to_feature_vector(bond):\n",
    "    features = []\n",
    "    features.append(int(bond.GetBondType()))\n",
    "    features.append(int(bond.GetIsConjugated()))\n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "def smiles_to_tf_graph(smiles_string):\n",
    "    mol = Chem.MolFromSmiles(smiles_string)\n",
    "    if mol is None:\n",
    "        return None, None, None, None \n",
    "\n",
    "    node_features = [atom_to_feature_vector(atom) for atom in mol.GetAtoms()]\n",
    "    if not node_features:\n",
    "        return None, None, None, None \n",
    "    node_features = np.array(node_features, dtype=np.float32)\n",
    "    num_nodes = len(node_features)\n",
    "\n",
    "    edge_indices = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices.append([i, j])\n",
    "        edge_indices.append([j, i])\n",
    "\n",
    "    num_edges = len(edge_indices) \n",
    "    if num_edges == 0: \n",
    "        if num_nodes == 1:\n",
    "            edge_indices_final = np.array([[0, 0]], dtype=np.int32)\n",
    "            num_edges_final = 1\n",
    "        else:\n",
    "            edge_indices_final = tf.zeros((0, 2), dtype=tf.int32)\n",
    "            num_edges_final = 0\n",
    "        return node_features, edge_indices_final, num_nodes, num_edges_final\n",
    "    \n",
    "    edge_indices_final = np.array(edge_indices, dtype=np.int32)\n",
    "    num_edges_final = len(edge_indices_final)\n",
    "\n",
    "    return node_features, edge_indices_final, num_nodes, num_edges_final\n",
    "\n",
    "\n",
    "# --- 3. Data Preparation for BBBP Downstream Task ---\n",
    "\n",
    "# Define the featurization function for the dataset map operation\n",
    "# This is a slightly modified version for DownstreamModel input (adds label)\n",
    "def featurize_smiles_and_graph_with_label(smiles_string, label):\n",
    "    # This function will run on CPU\n",
    "    token_ids = tokenize_smiles(smiles_string.numpy().decode('utf-8'), char_to_idx, MAX_SMILES_LEN)\n",
    "    mask = create_smiles_mask(token_ids, char_to_idx['<pad>'])\n",
    "\n",
    "    node_features, edge_indices, num_nodes, num_edges = smiles_to_tf_graph(smiles_string.numpy().decode('utf-8'))\n",
    "\n",
    "    # Handle cases where graph conversion fails\n",
    "    if node_features is None:\n",
    "        # Return dummy values that will be filtered out later\n",
    "        dummy_node_features = tf.zeros((1, NUM_ATOM_FEATURES), dtype=tf.float32)\n",
    "        dummy_edge_indices = tf.zeros((0, 2), dtype=tf.int32)\n",
    "        dummy_num_nodes = tf.constant(0, dtype=tf.int32)\n",
    "        dummy_num_edges = tf.constant(0, dtype=tf.int32)\n",
    "        return (dummy_node_features, dummy_edge_indices, dummy_num_nodes, dummy_num_edges, token_ids, mask), label\n",
    "    \n",
    "    padded_node_features = tf.pad(node_features, [[0, MAX_NODES - num_nodes], [0, 0]])\n",
    "    \n",
    "    return ((tf.constant(padded_node_features, dtype=tf.float32),\n",
    "             tf.constant(edge_indices, dtype=tf.int32),\n",
    "             tf.constant(num_nodes, dtype=tf.int32),\n",
    "             tf.constant(num_edges, dtype=tf.int32),\n",
    "             tf.constant(token_ids, dtype=tf.int32),\n",
    "             tf.constant(mask, dtype=tf.bool)),\n",
    "            label) # Return inputs as a tuple and label separately\n",
    "\n",
    "\n",
    "# Need to define padded_shapes and padding_values for tf.data.Dataset.padded_batch\n",
    "# These should mirror the definitions in the pre-training notebook for the input features\n",
    "padded_shapes_inputs = (\n",
    "    tf.TensorShape([MAX_NODES, NUM_ATOM_FEATURES]),\n",
    "    tf.TensorShape([None, 2]),\n",
    "    tf.TensorShape([]),\n",
    "    tf.TensorShape([]),\n",
    "    tf.TensorShape([MAX_SMILES_LEN]),\n",
    "    tf.TensorShape([MAX_SMILES_LEN])\n",
    ")\n",
    "padding_values_inputs = (\n",
    "    tf.constant(0.0, dtype=tf.float32),\n",
    "    tf.constant(0, dtype=tf.int32),\n",
    "    tf.constant(0, dtype=tf.int32),\n",
    "    tf.constant(0, dtype=tf.int32),\n",
    "    tf.constant(char_to_idx['<pad>'], dtype=tf.int32),\n",
    "    tf.constant(True, dtype=tf.bool)\n",
    ")\n",
    "\n",
    "# Function to convert DeepChem dataset to TensorFlow dataset\n",
    "def dc_dataset_to_tf_dataset_for_downstream(dc_dataset, batch_size, shuffle_buffer_size=1000):\n",
    "    smiles_list = dc_dataset.X.tolist() # Get SMILES strings\n",
    "    labels = dc_dataset.y # Get labels. Shape is (num_samples, num_tasks)\n",
    "\n",
    "    # Use tf.data.Dataset.from_generator to handle flexible data loading\n",
    "    # Generator yields (inputs_tuple, label)\n",
    "    def generator():\n",
    "        for i in range(len(smiles_list)):\n",
    "            yield smiles_list[i], labels[i] # Yield raw SMILES and label first\n",
    "\n",
    "    # Create dataset from generator, then map featurization\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(tf.TensorSpec(shape=(), dtype=tf.string), # Raw SMILES\n",
    "                          tf.TensorSpec(shape=labels.shape[1:] if labels.ndim > 1 else (), dtype=labels.dtype)) # Label\n",
    "    )\n",
    "    \n",
    "    # Map the featurization function\n",
    "    dataset = dataset.map(lambda smiles, label: tf.py_function(\n",
    "        featurize_smiles_and_graph_with_label,\n",
    "        inp=[smiles, label],\n",
    "        Tout=(padded_shapes_inputs.as_dict(), tf.TensorSpec(shape=labels.shape[1:] if labels.ndim > 1 else (), dtype=labels.dtype).as_dict()) # Output signature for tuple and label\n",
    "    ), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Filter out failed graph conversions (where num_nodes was 0)\n",
    "    dataset = dataset.filter(lambda inputs_tuple, label: inputs_tuple[2] > 0) # inputs_tuple[2] is num_nodes\n",
    "\n",
    "    dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Define full padded shapes and padding values including the label\n",
    "    padded_shapes_full = (padded_shapes_inputs, tf.TensorShape(labels.shape[1:] if labels.ndim > 1 else ()))\n",
    "    padding_values_full = (padding_values_inputs, tf.constant(0, dtype=labels.dtype))\n",
    "    \n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes=padded_shapes_full, padding_values=padding_values_full, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Load BBBP dataset\n",
    "# 'Raw' featurizer is used to get raw SMILES strings for your custom featurizer\n",
    "bbbp_tasks, bbbp_datasets, bbbp_transformers = dc.molnet.load_bbbp(featurizer='Raw', splitter='scaffold')\n",
    "train_bbbp, valid_bbbp, test_bbbp = bbbp_datasets\n",
    "\n",
    "print(f\"BBBP Train Samples: {len(train_bbbp)}\")\n",
    "print(f\"BBBP Validation Samples: {len(valid_bbbp)}\")\n",
    "print(f\"BBBP Test Samples: {len(test_bbbp)}\")\n",
    "\n",
    "# Convert DeepChem datasets to TensorFlow datasets\n",
    "GLOBAL_BATCH_SIZE = 64 * strategy.num_replicas_in_sync # Use the same GLOBAL_BATCH_SIZE as pre-training\n",
    "train_bbbp_tf = dc_dataset_to_tf_dataset_for_downstream(train_bbbp, GLOBAL_BATCH_SIZE)\n",
    "valid_bbbp_tf = dc_dataset_to_tf_dataset_for_downstream(valid_bbbp, GLOBAL_BATCH_SIZE)\n",
    "test_bbbp_tf = dc_dataset_to_tf_dataset_for_downstream(test_bbbp, GLOBAL_BATCH_SIZE)\n",
    "\n",
    "\n",
    "# --- 4. Define Downstream Model ---\n",
    "# You need to redefine GINLayer, GINEncoder, TransformerEncoder classes here\n",
    "# (or import them if saved in a separate utility script)\n",
    "# for `tf.saved_model.load` and Keras to correctly build the graph.\n",
    "\n",
    "# GIN Layer (Custom Keras Layer) - RE-DEFINE\n",
    "class GINLayer(layers.Layer):\n",
    "    def __init__(self, output_dim, activation=None, **kwargs):\n",
    "        super(GINLayer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.mlp = keras.Sequential([\n",
    "            layers.Dense(output_dim, activation='relu'),\n",
    "            layers.Dense(output_dim)\n",
    "        ])\n",
    "        self.epsilon = self.add_weight(name='epsilon', shape=(),\n",
    "                                       initializer=keras.initializers.Constant(0.0),\n",
    "                                       trainable=True)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_features, edge_indices_batch, num_nodes_batch = inputs # num_nodes_batch is unused here, but passed\n",
    "        edge_values = tf.ones(tf.shape(edge_indices_batch)[0], dtype=tf.float32)\n",
    "        total_nodes_in_batch = tf.shape(node_features)[0]\n",
    "        adj_shape = tf.cast([total_nodes_in_batch, total_nodes_in_batch], dtype=tf.int64)\n",
    "        adj_sparse = tf.sparse.SparseTensor(indices=tf.cast(edge_indices_batch, tf.int64),\n",
    "                                            values=edge_values,\n",
    "                                            dense_shape=adj_shape)\n",
    "        neighbor_sum = tf.sparse.sparse_dense_matmul(adj_sparse, node_features)\n",
    "        combined_features = (1 + self.epsilon) * node_features + neighbor_sum\n",
    "        output = self.mlp(combined_features)\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0][0], self.output_dim\n",
    "\n",
    "# GIN Encoder - RE-DEFINE\n",
    "class GINEncoder(keras.Model):\n",
    "    def __init__(self, num_layers, hidden_dim, num_node_features, **kwargs):\n",
    "        super(GINEncoder, self).__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.initial_mlp = keras.Sequential([\n",
    "            layers.Dense(hidden_dim, activation='relu'),\n",
    "            layers.Dense(hidden_dim) \n",
    "        ])\n",
    "        self.gin_layers = []\n",
    "        self.bns = []\n",
    "        for i in range(num_layers): \n",
    "            self.gin_layers.append(GINLayer(hidden_dim, activation='relu' if i < num_layers - 1 else None))\n",
    "            self.bns.append(layers.BatchNormalization())\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        node_features, edge_indices, num_nodes = inputs\n",
    "        x = self.initial_mlp(node_features)\n",
    "        for i in range(len(self.gin_layers)): \n",
    "            x = self.gin_layers[i]((x, edge_indices, num_nodes))\n",
    "            x = self.bns[i](x)\n",
    "        batch_size = tf.shape(node_features)[0] // MAX_NODES\n",
    "        x_reshaped = tf.reshape(x, (batch_size, MAX_NODES, self.hidden_dim))\n",
    "        sequence_mask = tf.sequence_mask(num_nodes, maxlen=MAX_NODES, dtype=tf.float32) \n",
    "        sequence_mask = tf.expand_dims(sequence_mask, axis=-1) \n",
    "        masked_x = x_reshaped * sequence_mask\n",
    "        graph_embedding = tf.reduce_sum(masked_x, axis=1)\n",
    "        return graph_embedding\n",
    "\n",
    "# Transformer Encoder - RE-DEFINE\n",
    "class TransformerEncoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, max_seq_len, dropout_rate=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_embedding = self.add_weight(\n",
    "            name=\"pos_embed\",\n",
    "            shape=(1, max_seq_len, embed_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.encoder_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.encoder_layers.append([\n",
    "                layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=dropout_rate),\n",
    "                layers.LayerNormalization(epsilon=1e-6),\n",
    "                layers.Dense(embed_dim * 4, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "                layers.LayerNormalization(epsilon=1e-6),\n",
    "            ])\n",
    "        self.final_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        token_ids, padding_mask_bool = inputs \n",
    "        x = self.token_embedding(token_ids)\n",
    "        x = x + self.positional_embedding[:, :tf.shape(x)[1], :]\n",
    "        attention_mask_additive = tf.cast(padding_mask_bool, dtype=tf.float32) * -1e9\n",
    "        attention_mask_additive = tf.expand_dims(attention_mask_additive, axis=1) \n",
    "        \n",
    "        for i, (attention, norm1, ff_dense1, ff_dense2, norm2) in enumerate(self.encoder_layers):\n",
    "            attn_output = attention(x, x, attention_mask=attention_mask_additive, training=training)\n",
    "            x = norm1(x + attn_output)\n",
    "            ff_output = ff_dense2(ff_dense1(x))\n",
    "            x = norm2(x + ff_output)\n",
    "        \n",
    "        expanded_padding_mask = tf.cast(tf.expand_dims(padding_mask_bool, axis=-1), dtype=x.dtype)\n",
    "        non_padded_mask = 1.0 - expanded_padding_mask\n",
    "        x_masked = x * non_padded_mask\n",
    "        sum_embeddings = tf.reduce_sum(x_masked, axis=1)\n",
    "        non_padded_len = tf.reduce_sum(non_padded_mask, axis=1)\n",
    "        smiles_embedding = sum_embeddings / (non_padded_len + 1e-9)\n",
    "        \n",
    "        return self.final_norm(smiles_embedding)\n",
    "\n",
    "\n",
    "# Downstream Model\n",
    "class DownstreamModel(keras.Model):\n",
    "    def __init__(self, pre_trained_gin_encoder, pre_trained_transformer_encoder, output_dim, task_type='classification', **kwargs):\n",
    "        super(DownstreamModel, self).__init__(**kwargs)\n",
    "        self.gin_encoder = pre_trained_gin_encoder\n",
    "        self.transformer_encoder = pre_trained_transformer_encoder\n",
    "        \n",
    "        # Freeze the pre-trained encoders initially\n",
    "        # You might unfreeze them after a few epochs for full fine-tuning\n",
    "        self.gin_encoder.trainable = False\n",
    "        self.transformer_encoder.trainable = False\n",
    "\n",
    "        # Head for classification/regression\n",
    "        input_to_head_dim = PROJECTION_DIM + EMBED_DIM_TRANSFORMER # Combined dimensions of the *outputs* from encoders\n",
    "                                                                 # before their original projection heads.\n",
    "                                                                 # Adjust based on whether you take raw encoder outputs\n",
    "                                                                 # or their *projected* outputs for fine-tuning.\n",
    "                                                                 # Assuming PROJECTION_DIM for both after pre-training projection\n",
    "\n",
    "        self.classifier_head = keras.Sequential([\n",
    "            layers.Dense(input_to_head_dim // 2, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(output_dim, activation='sigmoid' if task_type == 'classification' else 'linear')\n",
    "        ])\n",
    "        self.task_type = task_type\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        node_features_padded, edge_indices_padded, num_nodes, num_edges, token_ids, smiles_mask = inputs\n",
    "\n",
    "        # Flatten node_features_padded (must match GRASPModel.call preprocessing)\n",
    "        node_features_flat = tf.reshape(node_features_padded, (-1, tf.shape(node_features_padded)[2]))\n",
    "        \n",
    "        batch_size = tf.shape(node_features_padded)[0] \n",
    "        \n",
    "        # Edge preprocessing (must match GRASPModel.call preprocessing)\n",
    "        edge_mask = tf.sequence_mask(num_edges, maxlen=tf.shape(edge_indices_padded)[1], dtype=tf.bool)\n",
    "        valid_edge_indices = tf.boolean_mask(edge_indices_padded, edge_mask)\n",
    "        batch_ids_for_edges = tf.cast(tf.where(edge_mask)[:, 0], dtype=tf.int32)\n",
    "        node_offsets_for_edges = tf.range(batch_size) * MAX_NODES\n",
    "        offsets = tf.gather(node_offsets_for_edges, batch_ids_for_edges)\n",
    "        offsets = tf.expand_dims(offsets, axis=-1)\n",
    "        global_edge_indices_filtered = valid_edge_indices + offsets\n",
    "        \n",
    "        # Get embeddings from pre-trained encoders\n",
    "        # Note: training=training is important to pass through for BatchNorm/Dropout\n",
    "        graph_embeddings = self.gin_encoder((node_features_flat, global_edge_indices_filtered, num_nodes), training=training)\n",
    "        smiles_embeddings = self.transformer_encoder((token_ids, smiles_mask), training=training)\n",
    "\n",
    "        # Concatenate embeddings\n",
    "        combined_embeddings = tf.concat([graph_embeddings, smiles_embeddings], axis=-1)\n",
    "\n",
    "        # Pass through classification/regression head\n",
    "        output = self.classifier_head(combined_embeddings, training=training)\n",
    "        return output\n",
    "\n",
    "# --- 5. Fine-tuning and Evaluation Loop (BBBP) ---\n",
    "\n",
    "# BBBP task specific output dimension (binary classification)\n",
    "bbbp_output_dim = len(bbbp_tasks) # Should be 1 for BBBP\n",
    "\n",
    "with strategy.scope():\n",
    "    # Instantiate Downstream Model\n",
    "    bbbp_model = DownstreamModel(pre_trained_gin_encoder, pre_trained_transformer_encoder, \n",
    "                                 output_dim=bbbp_output_dim, task_type='classification')\n",
    "\n",
    "    # Compile the model\n",
    "    bbbp_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                       loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                       metrics=[keras.metrics.AUC(name='auc')])\n",
    "\n",
    "    # Build the model with a dummy input shape to ensure it's built before fit, especially for custom layers\n",
    "    dummy_input_shape = (GLOBAL_BATCH_SIZE, MAX_NODES, NUM_ATOM_FEATURES) # for node_features_padded\n",
    "    dummy_input = (tf.zeros(dummy_input_shape), \n",
    "                   tf.zeros((GLOBAL_BATCH_SIZE, 10, 2), dtype=tf.int32), # dummy edges\n",
    "                   tf.constant([50]*GLOBAL_BATCH_SIZE, dtype=tf.int32), # dummy num_nodes\n",
    "                   tf.constant([10]*GLOBAL_BATCH_SIZE, dtype=tf.int32), # dummy num_edges\n",
    "                   tf.zeros((GLOBAL_BATCH_SIZE, MAX_SMILES_LEN), dtype=tf.int32),\n",
    "                   tf.zeros((GLOBAL_BATCH_SIZE, MAX_SMILES_LEN), dtype=tf.bool))\n",
    "    \n",
    "    # Try to call the model with dummy inputs to build it\n",
    "    try:\n",
    "        _ = bbbp_model(dummy_input)\n",
    "        print(\"BBBP DownstreamModel built successfully with dummy input.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error building BBBP DownstreamModel with dummy input: {e}\")\n",
    "        # If building fails, training will likely fail. You might need to debug\n",
    "        # the model's call method or input shapes.\n",
    "\n",
    "\n",
    "print(\"\\nFine-tuning BBBP model...\")\n",
    "FINE_TUNE_EPOCHS = 10 # Adjust as needed\n",
    "bbbp_model.fit(train_bbbp_tf, epochs=FINE_TUNE_EPOCHS, validation_data=valid_bbbp_tf)\n",
    "\n",
    "print(\"\\nEvaluating BBBP model on test set...\")\n",
    "bbbp_results = bbbp_model.evaluate(test_bbbp_tf)\n",
    "print(f\"BBBP Test Loss: {bbbp_results[0]:.4f}, Test AUC: {bbbp_results[1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Optional: Save Fine-tuned Model (if desired) ---\n",
    "# bbbp_model.save('bbbp_finetuned_model')\n",
    "# print(\"BBBP fine-tuned model saved.\")\n",
    "\n",
    "print(\"\\nBBBP fine-tuning complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
