{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d958b96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit in /opt/anaconda3/lib/python3.12/site-packages (2025.3.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from rdkit) (10.4.0)\n",
      "Requirement already satisfied: tensorflow-macos in /opt/anaconda3/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: tensorflow==2.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow-macos) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.73.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.16.2->tensorflow-macos) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.1.0)\n",
      "Requirement already satisfied: tensorflow-metal in /opt/anaconda3/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow-metal) (0.44.0)\n",
      "Requirement already satisfied: six>=1.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow-metal) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit\n",
    "!pip install tensorflow-macos\n",
    "!pip install tensorflow-metal\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5dfa840-2c2a-4244-9755-e3edb3ed0215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alihussain/miniforge3/envs/tf-metal/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60df5b5",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2244650b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d707b",
   "metadata": {},
   "source": [
    "# Initialize Apple Silicon (M1/M2/M3) GPU (MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5414e1b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "GPU is available.\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "if tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    print(\"GPU is available.\")\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Setting whatever the default device\n",
    "tf.config.set_visible_devices(tf.config.list_physical_devices('GPU'), 'GPU')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207f4b0",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6083378",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:07:22] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum nodes found in the first 100000 molecules: 419\n",
      "Loaded 100000 SMILES strings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:07:27] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "SMILES_FILE_PATH = 'pubchem_smiles_for_pretraining.txt'\n",
    "\n",
    "if not os.path.exists(SMILES_FILE_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset file not found at '{SMILES_FILE_PATH}'.\")\n",
    "\n",
    "max_nodes_found = 0\n",
    "num_lines_to_check = 100000 \n",
    "\n",
    "with open(SMILES_FILE_PATH, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= num_lines_to_check:\n",
    "            break\n",
    "        smiles = line.strip()\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            num_nodes = mol.GetNumAtoms()\n",
    "            if num_nodes > max_nodes_found:\n",
    "                max_nodes_found = num_nodes\n",
    "print(f\"Maximum nodes found in the first {num_lines_to_check} molecules: {max_nodes_found}\")\n",
    "\n",
    "def load_smiles_data(file_path, num_samples=None):\n",
    "    smiles_list = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if num_samples and i >= num_samples:\n",
    "                break\n",
    "            smiles_list.append(line.strip())\n",
    "    print(f\"Loaded {len(smiles_list)} SMILES strings.\")\n",
    "    return smiles_list\n",
    "\n",
    "\n",
    "# For testing we will use 10k, 50k, 100k for quick tests basically.\n",
    "all_smiles = load_smiles_data(SMILES_FILE_PATH, num_samples=100000) \n",
    "\n",
    "# We are Defining the featurization function for the dataset map operation\n",
    "def featurize_smiles_and_graph(smiles_string):\n",
    "    token_ids = tokenize_smiles(smiles_string.numpy().decode('utf-8'), char_to_idx, MAX_SMILES_LEN)\n",
    "    mask = create_smiles_mask(token_ids, char_to_idx['<pad>'])\n",
    "\n",
    "    node_features, edge_indices, num_nodes, num_edges = smiles_to_tf_graph(smiles_string.numpy().decode('utf-8'))\n",
    "\n",
    "    # Handling the cases where graph conversion fails suhc as invalid smiles\n",
    "    if node_features is None:\n",
    "        # we are creating dummy tensors with shapes that can be padded later\n",
    "        dummy_node_features = tf.zeros((0, NUM_ATOM_FEATURES), dtype=tf.float32)\n",
    "        dummy_edge_indices = tf.zeros((0, 2), dtype=tf.int32)\n",
    "        dummy_num_nodes = tf.constant(0, dtype=tf.int32)\n",
    "        dummy_num_edges = tf.constant(0, dtype=tf.int32) \n",
    "        return dummy_node_features, dummy_edge_indices, dummy_num_nodes, dummy_num_edges, token_ids, mask\n",
    "    \n",
    "    # We will ensure node_features has consistent shape by padding if necessary for batching\n",
    "    padded_node_features = tf.pad(node_features, [[0, MAX_NODES - num_nodes], [0, 0]])\n",
    "    \n",
    "    return (tf.constant(padded_node_features, dtype=tf.float32),\n",
    "            tf.constant(edge_indices, dtype=tf.int32),\n",
    "            tf.constant(num_nodes, dtype=tf.int32),\n",
    "            tf.constant(num_edges, dtype=tf.int32),\n",
    "            tf.constant(token_ids, dtype=tf.int32),\n",
    "            tf.constant(mask, dtype=tf.bool))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9c69f",
   "metadata": {},
   "source": [
    "# FurtherData Processing & Creating tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd387dee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built vocabulary of size: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 23:07:27.507969: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-06-22 23:07:27.507992: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-06-22 23:07:27.507997: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-06-22 23:07:27.508015: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-22 23:07:27.508025: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# SMILES Tokenization \n",
    "def build_smiles_vocab(smiles_list, max_vocab_size=None):\n",
    "    all_chars = set()\n",
    "    for smiles in smiles_list:\n",
    "        for char in smiles:\n",
    "            all_chars.add(char)\n",
    "    vocab = sorted(list(all_chars))\n",
    "    # Adding special tokens\n",
    "    vocab = ['<pad>', '<unk>', '<cls>', '<eos>'] + vocab\n",
    "    if max_vocab_size:\n",
    "        vocab = vocab[:max_vocab_size]\n",
    "    char_to_idx = {char: i for i, char in enumerate(vocab)}\n",
    "    idx_to_char = {i: char for i, char in enumerate(vocab)}\n",
    "    print(f\"Built vocabulary of size: {len(vocab)}\")\n",
    "    return vocab, char_to_idx, idx_to_char\n",
    "\n",
    "vocab, char_to_idx, idx_to_char = build_smiles_vocab(all_smiles)\n",
    "VOCAB_SIZE = len(vocab)\n",
    "MAX_SMILES_LEN = 256 # Max sequence length for Transformer.\n",
    "MAX_NODES = max_nodes_found # Maximum number of nodes in any graph in a batch.\n",
    "\n",
    "def tokenize_smiles(smiles, char_to_idx, max_len):\n",
    "    \"\"\"Here we convert a SMILES string to a sequence of token IDs.\"\"\"\n",
    "    tokens = list(smiles)\n",
    "    indexed_tokens = [char_to_idx.get(char, char_to_idx['<unk>']) for char in tokens]\n",
    "    \n",
    "    if len(indexed_tokens) < max_len:\n",
    "        padded_tokens = indexed_tokens + [char_to_idx['<pad>']] * (max_len - len(indexed_tokens))\n",
    "    else:\n",
    "        padded_tokens = indexed_tokens[:max_len]\n",
    "    return np.array(padded_tokens, dtype=np.int32)\n",
    "\n",
    "def create_smiles_mask(token_ids, pad_token_id):\n",
    "    \"\"\"This will create a boolean mask for padded tokens.\"\"\"\n",
    "    return tf.cast(token_ids == pad_token_id, tf.bool)\n",
    "\n",
    "#  SMILES to TensorFlow Graph Conversion \n",
    "def atom_to_feature_vector(atom):\n",
    "    \"\"\"This will convert an RDKit atom to a feature vector.\"\"\"\n",
    "    features = []\n",
    "    features.append(atom.GetAtomicNum())\n",
    "    features.append(atom.GetDegree())\n",
    "    features.append(int(atom.GetHybridization())) # Converting enum to int\n",
    "    features.append(int(atom.GetIsAromatic()))\n",
    "    features.append(atom.GetFormalCharge())\n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "NUM_ATOM_FEATURES = len(atom_to_feature_vector(Chem.Atom(6))) \n",
    "\n",
    "def smiles_to_tf_graph(smiles_string):\n",
    "    \"\"\"\n",
    "    Converts a SMILES string to TensorFlow graph components:\n",
    "    node_features, edge_indices, num_nodes, and num_edges.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles_string)\n",
    "    if mol is None:\n",
    "        return None, None, None, None \n",
    "\n",
    "    node_features = [atom_to_feature_vector(atom) for atom in mol.GetAtoms()]\n",
    "    if not node_features:\n",
    "        return None, None, None, None \n",
    "    node_features = np.array(node_features, dtype=np.float32)\n",
    "    num_nodes = len(node_features)\n",
    "\n",
    "    edge_indices = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices.append([i, j])\n",
    "        edge_indices.append([j, i])\n",
    "\n",
    "    if len(edge_indices) == 0:\n",
    "        if num_nodes > 0: # Handling single atom molecules\n",
    "            edge_indices_final = np.empty((0, 2), dtype=np.int32)\n",
    "            num_edges_final = 0\n",
    "        else: \n",
    "            return None, None, None, None\n",
    "    else:\n",
    "        edge_indices_final = np.array(edge_indices, dtype=np.int32)\n",
    "        num_edges_final = len(edge_indices_final)\n",
    "\n",
    "    return node_features, edge_indices_final, num_nodes, num_edges_final\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(all_smiles)\n",
    "\n",
    "dataset = dataset.map(lambda x: tf.py_function(\n",
    "    featurize_smiles_and_graph,\n",
    "    inp=[x],\n",
    "    Tout=(tf.float32, tf.int32, tf.int32, tf.int32, tf.int32, tf.bool)\n",
    "), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.filter(lambda node_feat, edge_idx, num_nodes, num_edges, token_ids, mask: num_nodes > 0)\n",
    "\n",
    "BATCH_SIZE = 32 \n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE\n",
    "padded_shapes = (\n",
    "    tf.TensorShape([MAX_NODES, NUM_ATOM_FEATURES]), # node_features\n",
    "    tf.TensorShape([None, 2]),                     # edge_indices\n",
    "    tf.TensorShape([]),                           # num_nodes\n",
    "    tf.TensorShape([]),                           # num_edges\n",
    "    tf.TensorShape([MAX_SMILES_LEN]),              # token_ids\n",
    "    tf.TensorShape([MAX_SMILES_LEN])               # mask\n",
    ")\n",
    "padding_values = (\n",
    "    tf.constant(0.0, dtype=tf.float32),\n",
    "    tf.constant(0, dtype=tf.int32),\n",
    "    tf.constant(0, dtype=tf.int32),\n",
    "    tf.constant(0, dtype=tf.int32),\n",
    "    tf.constant(char_to_idx['<pad>'], dtype=tf.int32),\n",
    "    tf.constant(True, dtype=tf.bool)\n",
    ")\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=padded_shapes, padding_values=padding_values, drop_remainder=True)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56811d",
   "metadata": {},
   "source": [
    "#  Model Architecture (TensorFlow/Keras) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac10263",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GINLayer(layers.Layer):\n",
    "    def __init__(self, output_dim, activation=None, **kwargs):\n",
    "        super(GINLayer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.mlp = keras.Sequential([\n",
    "            layers.Dense(output_dim, activation='relu'),\n",
    "            layers.Dense(output_dim)\n",
    "        ])\n",
    "        self.epsilon = self.add_weight(name='epsilon', shape=(),\n",
    "                                       initializer=keras.initializers.Constant(0.0),\n",
    "                                       trainable=True)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_features, edge_indices_batch, num_nodes_batch = inputs\n",
    "        \n",
    "        edge_values = tf.ones(tf.shape(edge_indices_batch)[0], dtype=tf.float32)\n",
    "        \n",
    "        total_nodes_in_batch = tf.shape(node_features)[0]\n",
    "        adj_shape = tf.cast([total_nodes_in_batch, total_nodes_in_batch], dtype=tf.int64)\n",
    "\n",
    "        adj_sparse = tf.sparse.SparseTensor(indices=tf.cast(edge_indices_batch, tf.int64),\n",
    "                                            values=edge_values,\n",
    "                                            dense_shape=adj_shape)\n",
    "        \n",
    "        neighbor_sum = tf.sparse.sparse_dense_matmul(adj_sparse, node_features)\n",
    "\n",
    "        combined_features = (1 + self.epsilon) * node_features + neighbor_sum\n",
    "        output = self.mlp(combined_features)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "class GINEncoder(keras.Model):\n",
    "    def __init__(self, num_layers, hidden_dim, num_node_features, **kwargs): \n",
    "        super(GINEncoder, self).__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.initial_mlp = keras.Sequential([\n",
    "            layers.Dense(hidden_dim, activation='relu'),\n",
    "            layers.Dense(hidden_dim)\n",
    "        ])\n",
    "        self.gin_layers = [GINLayer(hidden_dim, activation='relu') for _ in range(num_layers - 1)]\n",
    "        self.gin_layers.append(GINLayer(hidden_dim, activation=None))\n",
    "        self.bns = [layers.BatchNormalization() for _ in range(num_layers)]\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        node_features, edge_indices, num_nodes = inputs\n",
    "        x = self.initial_mlp(node_features)\n",
    "        for i in range(len(self.gin_layers)):\n",
    "            x = self.gin_layers[i]((x, edge_indices, num_nodes))\n",
    "            x = self.bns[i](x)\n",
    "        \n",
    "        batch_size = tf.shape(node_features)[0] // MAX_NODES\n",
    "        x_reshaped = tf.reshape(x, (batch_size, MAX_NODES, self.hidden_dim))\n",
    "        sequence_mask = tf.sequence_mask(num_nodes, maxlen=MAX_NODES, dtype=tf.float32)\n",
    "        sequence_mask = tf.expand_dims(sequence_mask, axis=-1)\n",
    "        masked_x = x_reshaped * sequence_mask\n",
    "        graph_embedding = tf.reduce_sum(masked_x, axis=1)\n",
    "        return graph_embedding\n",
    "\n",
    "class TransformerEncoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, max_seq_len, dropout_rate=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_embedding = self.add_weight(\n",
    "            name=\"pos_embed\",\n",
    "            shape=(1, max_seq_len, embed_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.encoder_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.encoder_layers.append([\n",
    "                layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=dropout_rate),\n",
    "                layers.LayerNormalization(epsilon=1e-6),\n",
    "                layers.Dense(embed_dim * 4, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "                layers.LayerNormalization(epsilon=1e-6),\n",
    "            ])\n",
    "        self.final_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        token_ids, padding_mask_bool = inputs\n",
    "        x = self.token_embedding(token_ids)\n",
    "        x = x + self.positional_embedding[:, :tf.shape(x)[1], :]\n",
    "        attention_mask_additive = tf.cast(padding_mask_bool, dtype=tf.float32) * -1e9\n",
    "        attention_mask_additive = tf.expand_dims(attention_mask_additive, axis=1)\n",
    "        \n",
    "        for attention, norm1, ff_dense1, ff_dense2, norm2 in self.encoder_layers:\n",
    "            attn_output = attention(x, x, attention_mask=attention_mask_additive, training=training)\n",
    "            x = norm1(x + attn_output)\n",
    "            ff_output = ff_dense2(ff_dense1(x))\n",
    "            x = norm2(x + ff_output)\n",
    "        \n",
    "        expanded_padding_mask = tf.cast(tf.expand_dims(padding_mask_bool, axis=-1), dtype=x.dtype)\n",
    "        non_padded_mask = 1.0 - expanded_padding_mask\n",
    "        x_masked = x * non_padded_mask\n",
    "        sum_embeddings = tf.reduce_sum(x_masked, axis=1)\n",
    "        non_padded_len = tf.reduce_sum(non_padded_mask, axis=1)\n",
    "        smiles_embedding = sum_embeddings / (non_padded_len + 1e-9)\n",
    "        return self.final_norm(smiles_embedding)\n",
    "\n",
    "class ProjectionHead(keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256, **kwargs):\n",
    "        super(ProjectionHead, self).__init__(**kwargs)\n",
    "        self.net = keras.Sequential([\n",
    "            layers.Dense(hidden_dim, activation='relu'),\n",
    "            layers.Dense(output_dim)\n",
    "        ])\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class GRASPModel(keras.Model):\n",
    "    def __init__(self, gin_config, transformer_config, projection_dim, **kwargs):\n",
    "        super(GRASPModel, self).__init__(**kwargs)\n",
    "        self.gin_encoder = GINEncoder(**gin_config)\n",
    "        self.transformer_encoder = TransformerEncoder(**transformer_config)\n",
    "        self.graph_projection_head = ProjectionHead(gin_config['hidden_dim'], projection_dim)\n",
    "        self.smiles_projection_head = ProjectionHead(transformer_config['embed_dim'], projection_dim)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        node_features_padded, edge_indices_padded, num_nodes, num_edges, token_ids, smiles_mask = inputs\n",
    "        node_features_flat = tf.reshape(node_features_padded, (-1, tf.shape(node_features_padded)[2]))\n",
    "        batch_size = tf.shape(node_features_padded)[0]\n",
    "        \n",
    "        edge_mask = tf.sequence_mask(num_edges, maxlen=tf.shape(edge_indices_padded)[1], dtype=tf.bool)\n",
    "        valid_edge_indices = tf.cast(tf.boolean_mask(edge_indices_padded, edge_mask), dtype=tf.int32)\n",
    "        \n",
    "        node_offsets_for_edges = tf.range(batch_size) * MAX_NODES\n",
    "        node_offsets_for_edges = tf.expand_dims(node_offsets_for_edges, axis=1)\n",
    "        node_offsets_for_edges_expanded = tf.boolean_mask(tf.tile(node_offsets_for_edges, [1, tf.shape(edge_indices_padded)[1]]), edge_mask)\n",
    "        node_offsets_for_edges_expanded = tf.expand_dims(node_offsets_for_edges_expanded, axis=-1)\n",
    "        \n",
    "        global_edge_indices_filtered = valid_edge_indices + tf.cast(node_offsets_for_edges_expanded, dtype=tf.int32)\n",
    "        \n",
    "        graph_embeddings_raw = self.gin_encoder((node_features_flat, global_edge_indices_filtered, num_nodes), training=training)\n",
    "        graph_embeddings_projected = self.graph_projection_head(graph_embeddings_raw, training=training)\n",
    "        \n",
    "        smiles_embeddings_raw = self.transformer_encoder((token_ids, smiles_mask), training=training)\n",
    "        smiles_embeddings_projected = self.smiles_projection_head(smiles_embeddings_raw, training=training)\n",
    "        \n",
    "        graph_embeddings_projected = tf.linalg.normalize(graph_embeddings_projected, axis=1)[0]\n",
    "        smiles_embeddings_projected = tf.linalg.normalize(smiles_embeddings_projected, axis=1)[0]\n",
    "        \n",
    "        return graph_embeddings_projected, smiles_embeddings_projected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3da95f",
   "metadata": {},
   "source": [
    "#  Contrastive Loss (InfoNCE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395bcb8e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class InfoNCELoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.07, name='info_nce_loss', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, graph_embeddings, smiles_embeddings):\n",
    "        logits = tf.matmul(graph_embeddings, smiles_embeddings, transpose_b=True) / self.temperature\n",
    "        batch_size = tf.shape(logits)[0]\n",
    "        labels = tf.eye(batch_size)\n",
    "        loss_g_s = tf.keras.losses.categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        loss_s_g = tf.keras.losses.categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)\n",
    "        total_loss = (loss_g_s + loss_s_g) / 2\n",
    "        return tf.reduce_mean(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8b822",
   "metadata": {},
   "source": [
    "#  Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f4a776a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting pre-training for 5 epochs...\n",
      "Dataset has 3125 batches per epoch.\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/3125 [00:00<?, ?it/s][23:07:29] WARNING: not removing hydrogen atom without neighbors\n",
      "2025-06-22 23:07:37.765524: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 5453 of 10000\n",
      "2025-06-22 23:07:44.957851: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "2025-06-22 23:07:47.327036: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "Epoch 1 Training:  13%|█▎        | 408/3125 [03:38<21:22,  2.12it/s, loss=1.5479] [23:11:07] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  16%|█▌        | 503/3125 [04:22<19:49,  2.20it/s, loss=1.1868][23:11:50] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  16%|█▌        | 504/3125 [04:22<19:52,  2.20it/s, loss=1.1775][23:11:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:11:51] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  16%|█▌        | 504/3125 [04:23<19:52,  2.20it/s, loss=1.6663][23:11:51] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  16%|█▋        | 510/3125 [04:25<20:04,  2.17it/s, loss=1.5401][23:11:53] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  18%|█▊        | 557/3125 [04:47<19:58,  2.14it/s, loss=1.3296][23:12:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:12:15] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  46%|████▋     | 1447/3125 [11:32<12:31,  2.23it/s, loss=1.3663][23:19:00] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:19:00] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  47%|████▋     | 1470/3125 [11:43<12:32,  2.20it/s, loss=1.6639][23:19:11] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  57%|█████▋    | 1775/3125 [14:01<10:56,  2.06it/s, loss=1.1541][23:21:30] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:21:30] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  65%|██████▌   | 2032/3125 [15:58<08:14,  2.21it/s, loss=1.3875][23:23:26] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  66%|██████▌   | 2051/3125 [16:07<08:08,  2.20it/s, loss=1.2854][23:23:35] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  66%|██████▌   | 2055/3125 [16:09<08:12,  2.17it/s, loss=1.3608][23:23:37] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  66%|██████▌   | 2056/3125 [16:09<08:13,  2.17it/s, loss=1.0365][23:23:37] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  66%|██████▋   | 2071/3125 [16:16<08:01,  2.19it/s, loss=1.1400][23:23:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:23:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:23:44] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  66%|██████▋   | 2072/3125 [16:16<07:58,  2.20it/s, loss=1.1400][23:23:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:23:44] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  66%|██████▋   | 2075/3125 [16:18<07:54,  2.21it/s, loss=1.3520][23:23:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:23:46] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  68%|██████▊   | 2126/3125 [16:41<07:51,  2.12it/s, loss=1.2815][23:24:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:09] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  69%|██████▊   | 2141/3125 [16:48<07:22,  2.22it/s, loss=0.9305][23:24:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:16] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  69%|██████▉   | 2170/3125 [17:02<07:19,  2.17it/s, loss=1.0787][23:24:29] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:29] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:29] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:29] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:29] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:29] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  69%|██████▉   | 2171/3125 [17:02<07:20,  2.17it/s, loss=1.2110][23:24:30] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  69%|██████▉   | 2171/3125 [17:03<07:20,  2.17it/s, loss=0.8819][23:24:30] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:30] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  70%|██████▉   | 2172/3125 [17:03<07:24,  2.14it/s, loss=0.8819][23:24:30] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  70%|██████▉   | 2173/3125 [17:03<07:21,  2.16it/s, loss=0.8835][23:24:31] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  70%|██████▉   | 2182/3125 [17:07<07:12,  2.18it/s, loss=1.1745][23:24:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:35] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  70%|███████   | 2198/3125 [17:15<07:13,  2.14it/s, loss=1.2537][23:24:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:43] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  71%|███████   | 2217/3125 [17:23<07:00,  2.16it/s, loss=1.0436][23:24:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:52] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  71%|███████   | 2224/3125 [17:27<06:50,  2.20it/s, loss=1.2607][23:24:54] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  71%|███████▏  | 2228/3125 [17:28<06:51,  2.18it/s, loss=0.9857][23:24:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:24:56] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  75%|███████▍  | 2338/3125 [18:19<06:01,  2.18it/s, loss=1.3534][23:25:47] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  76%|███████▌  | 2382/3125 [18:40<05:44,  2.15it/s, loss=0.9929][23:26:08] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  77%|███████▋  | 2391/3125 [18:44<05:36,  2.18it/s, loss=1.0062][23:26:12] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  85%|████████▍ | 2649/3125 [20:44<03:40,  2.16it/s, loss=1.1265][23:28:12] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:12] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:12] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:12] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  86%|████████▋ | 2697/3125 [21:06<03:16,  2.18it/s, loss=1.3666][23:28:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:34] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  86%|████████▋ | 2699/3125 [21:07<03:16,  2.16it/s, loss=1.4742][23:28:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:35] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  86%|████████▋ | 2701/3125 [21:08<03:17,  2.15it/s, loss=1.3031][23:28:36] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:36] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  87%|████████▋ | 2706/3125 [21:10<03:14,  2.15it/s, loss=1.2415][23:28:38] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:38] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  87%|████████▋ | 2711/3125 [21:13<03:16,  2.10it/s, loss=1.1518][23:28:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:40] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  87%|████████▋ | 2728/3125 [21:21<03:02,  2.17it/s, loss=1.1019][23:28:48] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  87%|████████▋ | 2729/3125 [21:21<03:04,  2.14it/s, loss=1.0236][23:28:49] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  88%|████████▊ | 2735/3125 [21:24<02:59,  2.18it/s, loss=1.0724][23:28:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:28:52] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  88%|████████▊ | 2737/3125 [21:25<03:00,  2.14it/s, loss=0.8731][23:28:53] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  88%|████████▊ | 2740/3125 [21:26<02:58,  2.15it/s, loss=1.2515][23:28:54] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  89%|████████▊ | 2770/3125 [21:40<02:42,  2.19it/s, loss=1.3684][23:29:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:29:08] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  89%|████████▊ | 2772/3125 [21:41<02:41,  2.18it/s, loss=1.2077][23:29:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:29:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:29:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:29:09] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  89%|████████▉ | 2780/3125 [21:45<02:44,  2.10it/s, loss=1.1533][23:29:12] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:29:12] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  90%|████████▉ | 2800/3125 [21:54<02:30,  2.15it/s, loss=0.9561][23:29:22] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:29:22] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training:  90%|████████▉ | 2807/3125 [21:57<02:26,  2.18it/s, loss=1.1642][23:29:25] WARNING: not removing hydrogen atom without neighbors\n",
      "Epoch 1 Training: 100%|██████████| 3125/3125 [24:13<00:00,  2.35it/s, loss=1.1741]2025-06-22 23:31:40.806109: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Epoch 1 Training: 100%|██████████| 3125/3125 [24:13<00:00,  2.15it/s, loss=1.1741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 finished. Average Loss: 1.2832\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 3125/3125 [21:54<00:00,  2.38it/s, loss=1.1583]2025-06-22 23:53:35.054577: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Epoch 2 Training: 100%|██████████| 3125/3125 [21:54<00:00,  2.38it/s, loss=1.1583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 finished. Average Loss: 1.1770\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 3125/3125 [22:25<00:00,  2.32it/s, loss=1.1018]2025-06-23 00:16:00.389725: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Epoch 3 Training: 100%|██████████| 3125/3125 [22:25<00:00,  2.32it/s, loss=1.1018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 finished. Average Loss: 1.2637\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 3125/3125 [22:48<00:00,  2.29it/s, loss=1.2610]2025-06-23 00:38:49.222983: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Epoch 4 Training: 100%|██████████| 3125/3125 [22:48<00:00,  2.28it/s, loss=1.2610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 finished. Average Loss: 1.4348\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 3125/3125 [23:01<00:00,  2.22it/s, loss=1.3705]2025-06-23 01:01:51.081134: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Epoch 5 Training: 100%|██████████| 3125/3125 [23:01<00:00,  2.26it/s, loss=1.3705]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 finished. Average Loss: 1.5458\n",
      "\n",
      "Pre-training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PROJECTION_DIM = 128\n",
    "HIDDEN_DIM_GIN = 256\n",
    "NUM_GIN_LAYERS = 3\n",
    "EMBED_DIM_TRANSFORMER = 256\n",
    "NUM_TRANSFORMER_HEADS = 8\n",
    "NUM_TRANSFORMER_LAYERS = 3\n",
    "\n",
    "gin_config = {\n",
    "    'num_layers': NUM_GIN_LAYERS,\n",
    "    'hidden_dim': HIDDEN_DIM_GIN,\n",
    "    'num_node_features': NUM_ATOM_FEATURES\n",
    "}\n",
    "\n",
    "transformer_config = {\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'embed_dim': EMBED_DIM_TRANSFORMER,\n",
    "    'num_heads': NUM_TRANSFORMER_HEADS,\n",
    "    'num_layers': NUM_TRANSFORMER_LAYERS,\n",
    "    'max_seq_len': MAX_SMILES_LEN\n",
    "}\n",
    "\n",
    "# Instantiating model, loss, and optimizer\n",
    "model = GRASPModel(gin_config, transformer_config, PROJECTION_DIM)\n",
    "info_nce_loss = InfoNCELoss(temperature=0.07)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# The train_step function performs a single gradient update.\n",
    "@tf.function(reduce_retracing=True)\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        graph_embeddings, smiles_embeddings = model(inputs, training=True)\n",
    "        loss = info_nce_loss(graph_embeddings, smiles_embeddings)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "EPOCHS = 5\n",
    "# here we  the number of steps per epoch for tqdm\n",
    "steps_per_epoch_tqdm = len(all_smiles) // GLOBAL_BATCH_SIZE\n",
    "steps_per_epoch_tqdm = max(1, steps_per_epoch_tqdm)\n",
    "\n",
    "print(f\"\\nStarting pre-training for {EPOCHS} epochs...\")\n",
    "print(f\"Dataset has {steps_per_epoch_tqdm} batches per epoch.\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    epoch_iterator = tqdm(dataset, desc=f\"Epoch {epoch + 1} Training\", total=steps_per_epoch_tqdm, leave=True)\n",
    "    \n",
    "    num_batches = 0 \n",
    "\n",
    "    for i, batch_inputs in enumerate(epoch_iterator):\n",
    "        loss = train_step(batch_inputs)\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "        \n",
    "        epoch_iterator.set_postfix(loss=f\"{loss.numpy():.4f}\")\n",
    "    \n",
    "    if num_batches > 0: # Ensure num_batches is not zero to avoid division by zero\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"\\nEpoch {epoch + 1} finished. Average Loss: {avg_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nEpoch {epoch + 1} finished. No batches processed (check dataset size/filtering).\")\n",
    "\n",
    "print(\"\\nPre-training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d91127e",
   "metadata": {},
   "source": [
    "#  Saving the encoders and the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddbdee85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gin_encoder_pretrained/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gin_encoder_pretrained/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_pretrained/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_pretrained/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders saved.\n",
      "INFO:tensorflow:Assets written to: grasp_pretrained_model_tf_savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: grasp_pretrained_model_tf_savedmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'grasp_pretrained_model_tf_savedmodel'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): Tuple[TensorSpec(shape=(None, 419, 5), dtype=tf.float32, name=None), TensorSpec(shape=(None, 114, 2), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256), dtype=tf.float32, name=None)]\n",
      "Output Type:\n",
      "  Tuple[TensorSpec(shape=(None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128), dtype=tf.float32, name=None)]\n",
      "Captures:\n",
      "  6362182080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6362182608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5980019296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6366714992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6000846176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6362180320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6366717280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6366722208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6366712176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6362182256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409480400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409471600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409484800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6001051072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409478816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409485504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409483040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409482512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409477760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409480576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409480048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409483216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6001058112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409481632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6366711472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6409482160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411309776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411307664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411306256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411306960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411305904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411310304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411312944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411311712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411314000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411310832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6001065328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411309600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411315584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411313472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411316640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411311184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411317696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411314528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411319280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411312416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411314880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411318576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411314352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411316992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411320160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411313296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411318752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411315408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411312592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411320688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411319456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411311008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411359632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411356992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411361216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411359456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411356816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411360512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411363328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411362096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411364384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411363152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411357520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411363680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411366496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411365264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411367552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411363856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411368608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411365968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411370192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411364912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411364208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411365440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411369840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411366320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411369488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411403856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411405264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411367904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411403680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411406848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411409488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411408256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6411410544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Full Model saved to 'grasp_pretrained_model_tf_savedmodel' directory.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model.gin_encoder, 'gin_encoder_pretrained')\n",
    "tf.saved_model.save(model.transformer_encoder, 'transformer_encoder_pretrained')\n",
    "print(\"Encoders saved.\")\n",
    "\n",
    "model.export('grasp_pretrained_model_tf_savedmodel')\n",
    "print(\"Full Model saved to 'grasp_pretrained_model_tf_savedmodel' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
