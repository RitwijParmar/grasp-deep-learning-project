{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12258251,"sourceType":"datasetVersion","datasetId":7724386},{"sourceId":459199,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":372205,"modelId":393093}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GRASP: Self-Supervised Molecular Representation Learning (Kaggle Edition)\n\nThis notebook contains the complete PyTorch implementation of the GRASP model, optimized for the Kaggle environment. It performs self-supervised pre-training by aligning molecular graph representations with their corresponding SMILES string representations using a free Kaggle GPU.\n\n**Instructions:**\n1.  Ensure the **GPU is enabled** as the accelerator in the notebook settings.\n2.  Use the `+ Add data` button to attach your uploaded PubChem SMILES dataset.\n3.  **Update the `dataset_folder_name` variable** in Cell #3 to match your dataset's folder name.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Install All Necessary Libraries\nThis cell will install all required packages.","metadata":{}},{"cell_type":"code","source":"\n!pip uninstall torch torchvision torchaudio torch-scatter torch-sparse torch-geometric torch-cluster torch-spline-conv sentence-transformers transformers accelerate peft -y --quiet\n\n#  torchvision for cuda\n!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121 --quiet\n\n# older versions \n!pip install \"transformers==4.36.2\" \"accelerate==0.25.0\" \"timm>=0.9.2\" --quiet\n\n!pip install torch_geometric rdkit-pypi pandas tqdm --quiet\n\n!pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html --quiet\n\nprint(\"\\nInstallation of a fully version-locked, compatible library set is complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:35:55.388497Z","iopub.execute_input":"2025-06-23T17:35:55.388826Z","iopub.status.idle":"2025-06-23T17:37:22.318845Z","shell.execute_reply.started":"2025-06-23T17:35:55.388782Z","shell.execute_reply":"2025-06-23T17:37:22.317644Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-spline-conv as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping sentence-transformers as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping peft as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\nInstallation of a fully version-locked, compatible library set is complete.\nThis setup should resolve all dependency conflicts. Please proceed with the rest of the notebook.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 2. Imports and Configuration\nImporting all libraries and define the key parameters for our training run.","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.nn import GINConv, global_add_pool\n\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nfrom rdkit import Chem, rdBase\nfrom tqdm import tqdm\nimport pandas as pd\n\nrdBase.DisableLog('rdApp.warning')\nrdBase.DisableLog('rdApp.error')\n\ndataset_folder_name = 'pubchem-smiles-for-pretraining-txt'\nSMILES_FILE_PATH = f'/kaggle/input/{dataset_folder_name}/pubchem_smiles_for_pretraining.txt'\n\n\nTOKENIZER_NAME = 'seyonec/ChemBERTa-zinc-base-v1'\n\n\nNUM_SAMPLES = 500000\n\nBATCH_SIZE = 64\nEPOCHS = 5\nLEARNING_RATE = 1e-4\nTEMPERATURE = 0.07\n\nPROJECTION_DIM = 128\nGRAPH_EMB_DIM = 128\nGRAPH_LAYERS = 4\n\n\nNUM_WORKERS = 2 \n\n# Verify\nif not os.path.exists(SMILES_FILE_PATH):\n    raise FileNotFoundError(\n        f\"Dataset file not found at '{SMILES_FILE_PATH}'. \"\n        \"Please check the 'dataset_folder_name' variable and your uploaded file's name.\"\n    )\nprint(f\"Dataset found: {SMILES_FILE_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Utility Functions (SMILES to Graph Conversion)\nThese functions handle the conversion of a SMILES string into a graph data structure with rich, one-hot encoded atom features.","metadata":{}},{"cell_type":"code","source":"# one hot encoding\nATOM_FEATURE_MAP = {\n    'atomic_num': list(range(1, 119)),\n    'degree': list(range(6)),\n    'formal_charge': list(range(-2, 3)),\n    'hybridization': [\n        Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n        Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,\n        Chem.rdchem.HybridizationType.SP3D2, Chem.rdchem.HybridizationType.UNSPECIFIED\n    ],\n    'is_aromatic': [0, 1],\n    'is_in_ring': [0, 1]\n}\n\ndef one_hot_encode(value, choices):\n    encoding = [0] * (len(choices) + 1) \n    try:\n        index = choices.index(value)\n    except ValueError:\n        index = -1\n    encoding[index] = 1\n    return encoding\n\ndef get_atom_features(atom):\n    features = []\n    features += one_hot_encode(atom.GetAtomicNum(), ATOM_FEATURE_MAP['atomic_num'])\n    features += one_hot_encode(atom.GetDegree(), ATOM_FEATURE_MAP['degree'])\n    features += one_hot_encode(atom.GetFormalCharge(), ATOM_FEATURE_MAP['formal_charge'])\n    features += one_hot_encode(atom.GetHybridization(), ATOM_FEATURE_MAP['hybridization'])\n    features += one_hot_encode(int(atom.GetIsAromatic()), ATOM_FEATURE_MAP['is_aromatic'])\n    features += one_hot_encode(int(atom.IsInRing()), ATOM_FEATURE_MAP['is_in_ring'])\n    return torch.tensor(features, dtype=torch.float)\n\ndef get_num_node_features():\n    return sum(len(choices) + 1 for choices in ATOM_FEATURE_MAP.values())\n\ndef smiles_to_graph_data(smiles_string: str):\n    try:\n        mol = Chem.MolFromSmiles(smiles_string)\n        if mol is None: return None\n\n        atom_features = [get_atom_features(atom) for atom in mol.GetAtoms()]\n        x = torch.stack(atom_features)\n\n        edge_indices = []\n        for bond in mol.GetBonds():\n            i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n            edge_indices.extend([(i, j), (j, i)])\n        \n        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n\n        return Data(x=x, edge_index=edge_index, smiles=smiles_string)\n\n    except Exception:\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:37:22.335464Z","iopub.execute_input":"2025-06-23T17:37:22.335797Z","iopub.status.idle":"2025-06-23T17:37:22.351703Z","shell.execute_reply.started":"2025-06-23T17:37:22.335778Z","shell.execute_reply":"2025-06-23T17:37:22.351132Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## 4. Data Pipeline (Dataset and Collator)\nThis section defines the robust data pipeline using PyTorch's `Dataset` and a custom `collator` function for efficient, dynamic batching.","metadata":{}},{"cell_type":"code","source":"\n\nclass MoleculeDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset.\n    --- we filter out molecules with too many atoms. ---\n    \"\"\"\n    def __init__(self, file_path, tokenizer_name, num_samples=None, max_atoms=512):\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        self.max_atoms = max_atoms\n        \n        print(\"Loading and filtering SMILES strings from file\")\n        self.smiles_list = []\n        with open(file_path, 'r') as f:\n            # Using tqdm to show progress \n            for i, line in enumerate(tqdm(f, desc=\"Reading file\")):\n                if num_samples is not None and len(self.smiles_list) >= num_samples:\n                    break\n                \n                smiles = line.strip()\n            \n                if len(smiles) > self.max_atoms * 2: # skiping very long strings quickly\n                    continue\n                \n                # Checking actual atom count with rdkit\n                mol = Chem.MolFromSmiles(smiles)\n                if mol is not None and mol.GetNumAtoms() <= self.max_atoms:\n                    self.smiles_list.append(smiles)\n\n        print(f\"Loaded {len(self.smiles_list)} molecules after filtering (max atoms = {self.max_atoms}).\")\n\n    def __len__(self):\n        return len(self.smiles_list)\n\n    def __getitem__(self, idx):\n        smiles = self.smiles_list[idx]\n        \n        graph_data = smiles_to_graph_data(smiles)\n        if graph_data is None:\n            return None\n\n        smiles_tokens = self.tokenizer(\n            smiles,\n            padding=False, \n            truncation=True,\n            max_length=256,\n            return_tensors='pt'\n        )\n        smiles_tokens = {key: val.squeeze(0) for key, val in smiles_tokens.items()}\n        \n        return graph_data, smiles_tokens\n\nclass CustomCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        batch = [item for item in batch if item is not None]\n        if not batch:\n            return None, None\n\n        graphs, smiles_tokens_list = zip(*batch)\n        graph_batch = Batch.from_data_list(graphs)\n\n        smiles_padded = self.tokenizer.pad(\n            {'input_ids': [s['input_ids'] for s in smiles_tokens_list],\n             'attention_mask': [s['attention_mask'] for s in smiles_tokens_list]},\n            return_tensors='pt',\n            padding='longest'\n        )\n        \n        return graph_batch, smiles_padded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:37:22.353441Z","iopub.execute_input":"2025-06-23T17:37:22.353698Z","iopub.status.idle":"2025-06-23T17:37:22.372932Z","shell.execute_reply.started":"2025-06-23T17:37:22.353679Z","shell.execute_reply":"2025-06-23T17:37:22.372167Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## 5. Model Architecture\nThis section defines the three core PyTorch `nn.Module` classes: `GraphEncoder`, `SmilesEncoder`, and the main `GRASPModel`.","metadata":{}},{"cell_type":"code","source":"class GraphEncoder(nn.Module):\n    def __init__(self, num_node_features, embedding_dim, num_layers, dropout):\n        super(GraphEncoder, self).__init__()\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n        self.dropout = dropout\n        self.num_layers = num_layers\n\n        for i in range(num_layers):\n            in_dim = int(num_node_features) if i == 0 else int(embedding_dim)\n            hidden_dim = int(embedding_dim)\n\n            mlp = nn.Sequential(\n                nn.Linear(in_dim, 2 * hidden_dim),\n                nn.ReLU(),\n                nn.Linear(2 * hidden_dim, hidden_dim)\n            )\n            conv = GINConv(mlp, train_eps=True)\n            self.convs.append(conv)\n            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n\n    def forward(self, x, edge_index, batch):\n        h = x\n        for i in range(self.num_layers):\n            h = self.convs[i](h, edge_index)\n            h = self.batch_norms[i](h)\n            h = F.relu(h)\n            h = F.dropout(h, p=self.dropout, training=self.training)\n        \n        h_graph = global_add_pool(h, batch)\n        return h_graph\n\nclass SmilesEncoder(nn.Module):\n    def __init__(self, model_name='seyonec/ChemBERTa-zinc-base-v1', dropout=0.1):\n        super(SmilesEncoder, self).__init__()\n        config = AutoConfig.from_pretrained(model_name)\n        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n        self.smiles_embedding_dim = self.transformer.config.hidden_size\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n        return outputs.last_hidden_state[:, 0, :] \n\nclass GRASPModel(nn.Module):\n    def __init__(self, graph_emb_dim, graph_layers, projection_dim, dropout=0.1):\n        super(GRASPModel, self).__init__()\n        \n        self.graph_encoder = GraphEncoder(\n            num_node_features=get_num_node_features(),\n            embedding_dim=graph_emb_dim,\n            num_layers=graph_layers,\n            dropout=dropout\n        )\n        \n        self.smiles_encoder = SmilesEncoder(dropout=dropout)\n        \n        self.graph_projection = nn.Sequential(\n            nn.Linear(graph_emb_dim, graph_emb_dim),\n            nn.ReLU(),\n            nn.Linear(graph_emb_dim, projection_dim)\n        )\n        \n        self.smiles_projection = nn.Sequential(\n            nn.Linear(self.smiles_encoder.smiles_embedding_dim, self.smiles_encoder.smiles_embedding_dim),\n            nn.ReLU(),\n            nn.Linear(self.smiles_encoder.smiles_embedding_dim, projection_dim)\n        )\n\n    def forward(self, graph_batch, smiles_batch):\n        graph_embeddings = self.graph_encoder(\n            x=graph_batch.x, edge_index=graph_batch.edge_index, batch=graph_batch.batch\n        )\n        smiles_embeddings = self.smiles_encoder(\n            input_ids=smiles_batch['input_ids'], attention_mask=smiles_batch['attention_mask']\n        )\n        \n        graph_proj = self.graph_projection(graph_embeddings)\n        smiles_proj = self.smiles_projection(smiles_embeddings)\n        \n        return F.normalize(graph_proj, p=2, dim=1, eps=1e-8), F.normalize(smiles_proj, p=2, dim=1, eps=1e-8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:37:22.373614Z","iopub.execute_input":"2025-06-23T17:37:22.373815Z","iopub.status.idle":"2025-06-23T17:37:22.392397Z","shell.execute_reply.started":"2025-06-23T17:37:22.373798Z","shell.execute_reply":"2025-06-23T17:37:22.391831Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## 6. Pre-training Script\nThis section defines the InfoNCE loss function and the main training function that ties everything together.","metadata":{}},{"cell_type":"code","source":"class InfoNCELoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super(InfoNCELoss, self).__init__()\n        self.temperature = temperature\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, z_i, z_j):\n        batch_size = z_i.size(0)\n        labels = torch.arange(batch_size, device=z_i.device)\n        sim_matrix = torch.matmul(z_i, z_j.T) / self.temperature\n        loss_i_j = self.loss_fn(sim_matrix, labels)\n        loss_j_i = self.loss_fn(sim_matrix.T, labels)\n        return (loss_i_j + loss_j_i) / 2\n\ndef train_grasp():\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n    else:\n        device = torch.device(\"cpu\")\n        print(\"No CUDA GPU found, using CPU.\")\n\n    # Pipeline\n    dataset = MoleculeDataset(SMILES_FILE_PATH, TOKENIZER_NAME, num_samples=NUM_SAMPLES)\n    collator = CustomCollator(tokenizer=dataset.tokenizer)\n    data_loader = DataLoader(\n        dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator,\n        num_workers=NUM_WORKERS, pin_memory=True if device.type == 'cuda' else False,\n        persistent_workers=True if NUM_WORKERS > 0 else False\n    )\n\n    # Model, Loss, and Optimizer\n    model = GRASPModel(\n        projection_dim=PROJECTION_DIM, \n        graph_emb_dim=GRAPH_EMB_DIM, \n        graph_layers=GRAPH_LAYERS\n    ).to(device)\n    criterion = InfoNCELoss(temperature=TEMPERATURE)\n    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(data_loader)*EPOCHS)\n\n    # pre-training\n    print(\"\\nStarting pre-training\")\n    for epoch in range(EPOCHS):\n        model.train()\n        total_loss = 0\n        \n        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n        \n        for batch_data in progress_bar:\n            graph_batch, smiles_batch = batch_data\n            if graph_batch is None: continue\n\n            graph_batch = graph_batch.to(device)\n            smiles_batch = {key: val.to(device) for key, val in smiles_batch.items()}\n            \n            optimizer.zero_grad()\n            graph_proj, smiles_proj = model(graph_batch, smiles_batch)\n            loss = criterion(graph_proj, smiles_proj)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            \n            total_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item())\n\n        avg_loss = total_loss / len(data_loader) if data_loader else 0\n        print(f\"\\nEpoch {epoch+1}/{EPOCHS} - Average Loss: {avg_loss:.4f}\")\n        \n        checkpoint_path = f\"/kaggle/working/grasp_model_epoch_{epoch+1}.pt\"\n        torch.save(model.state_dict(), checkpoint_path)\n        print(f\"Model checkpoint saved to {checkpoint_path}\")\n\n    print(\"\\nPre-training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:37:22.393099Z","iopub.execute_input":"2025-06-23T17:37:22.393345Z","iopub.status.idle":"2025-06-23T17:37:22.415489Z","shell.execute_reply.started":"2025-06-23T17:37:22.393323Z","shell.execute_reply":"2025-06-23T17:37:22.414840Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## 7. Run Training\nThis cell executes the main training function.","metadata":{}},{"cell_type":"code","source":"train_grasp()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:39:03.648599Z","iopub.execute_input":"2025-06-23T18:39:03.648999Z","iopub.status.idle":"2025-06-24T01:30:59.280334Z","shell.execute_reply.started":"2025-06-23T18:39:03.648967Z","shell.execute_reply":"2025-06-24T01:30:59.279445Z"}},"outputs":[{"name":"stdout","text":"Using CUDA GPU: Tesla T4\nLoading and filtering SMILES strings from file...\n","output_type":"stream"},{"name":"stderr","text":"Reading file: 500537it [01:30, 5527.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded 500000 molecules after filtering (max atoms = 512).\n\nStarting pre-training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5:   0%|          | 0/7813 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n2025-06-23 18:40:35.590377: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-23 18:40:35.590760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750704035.625684     492 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750704035.625676     493 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750704035.635765     493 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1750704035.635797     492 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bcd55a7e700>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x7bcd55a7e700>\n\n Traceback (most recent call last):\n    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n       if w.is_alive(): \n^ ^^  ^^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'\n^ ^ ^ ^ ^ ^ \n AssertionError:  can only test a child process \n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nEpoch 1/5:   2%|▏         | 139/7813 [01:38<1:52:32,  1.14it/s, loss=0.519]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3056], which does not match the required output shape [2, 3056]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:   3%|▎         | 242/7813 [02:46<1:28:22,  1.43it/s, loss=0.311]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3292], which does not match the required output shape [2, 3292]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:   3%|▎         | 263/7813 [02:58<1:10:44,  1.78it/s, loss=0.338]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2864], which does not match the required output shape [2, 2864]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:   5%|▍         | 371/7813 [04:08<1:24:00,  1.48it/s, loss=0.248]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2826], which does not match the required output shape [2, 2826]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:   6%|▌         | 484/7813 [05:19<1:17:37,  1.57it/s, loss=0.227]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3492], which does not match the required output shape [2, 3492]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  10%|█         | 799/7813 [08:38<1:29:05,  1.31it/s, loss=0.15]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3066], which does not match the required output shape [2, 3066]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  21%|██        | 1656/7813 [17:41<1:15:36,  1.36it/s, loss=0.0889]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3088], which does not match the required output shape [2, 3088]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  22%|██▏       | 1718/7813 [18:19<57:21,  1.77it/s, loss=0.0489]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3234], which does not match the required output shape [2, 3234]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  23%|██▎       | 1767/7813 [18:49<1:02:56,  1.60it/s, loss=0.057] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2510], which does not match the required output shape [2, 2510]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  23%|██▎       | 1804/7813 [19:13<1:06:44,  1.50it/s, loss=0.0356]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2714], which does not match the required output shape [2, 2714]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  26%|██▌       | 2027/7813 [21:31<1:10:30,  1.37it/s, loss=0.0639]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2592], which does not match the required output shape [2, 2592]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  29%|██▉       | 2270/7813 [24:08<1:04:50,  1.42it/s, loss=0.0625]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3406], which does not match the required output shape [2, 3406]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  33%|███▎      | 2611/7813 [27:38<47:54,  1.81it/s, loss=0.0418]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2886], which does not match the required output shape [2, 2886]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  35%|███▍      | 2708/7813 [28:37<56:14,  1.51it/s, loss=0.0332]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2630], which does not match the required output shape [2, 2630]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  40%|████      | 3162/7813 [33:21<56:06,  1.38it/s, loss=0.0917]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3398], which does not match the required output shape [2, 3398]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  43%|████▎     | 3385/7813 [35:41<44:40,  1.65it/s, loss=0.0441]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2920], which does not match the required output shape [2, 2920]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  47%|████▋     | 3669/7813 [38:51<43:23,  1.59it/s, loss=0.061]   /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2496], which does not match the required output shape [2, 2496]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  48%|████▊     | 3778/7813 [40:00<42:40,  1.58it/s, loss=0.0375]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3332], which does not match the required output shape [2, 3332]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  50%|████▉     | 3894/7813 [41:11<37:15,  1.75it/s, loss=0.0438]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2938], which does not match the required output shape [2, 2938]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  62%|██████▏   | 4812/7813 [50:45<29:22,  1.70it/s, loss=0.0668]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2730], which does not match the required output shape [2, 2730]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  63%|██████▎   | 4901/7813 [51:39<32:28,  1.49it/s, loss=0.0389]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2646], which does not match the required output shape [2, 2646]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  63%|██████▎   | 4916/7813 [51:47<26:56,  1.79it/s, loss=0.0153]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3118], which does not match the required output shape [2, 3118]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  64%|██████▍   | 4985/7813 [52:28<33:35,  1.40it/s, loss=0.0422]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3110], which does not match the required output shape [2, 3110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  65%|██████▌   | 5115/7813 [53:51<34:16,  1.31it/s, loss=0.0229]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2810], which does not match the required output shape [2, 2810]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  67%|██████▋   | 5208/7813 [54:47<27:33,  1.58it/s, loss=0.0222]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3194], which does not match the required output shape [2, 3194]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  68%|██████▊   | 5303/7813 [55:49<26:23,  1.59it/s, loss=0.0197]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2804], which does not match the required output shape [2, 2804]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  73%|███████▎  | 5666/7813 [59:43<18:33,  1.93it/s, loss=0.0213] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3126], which does not match the required output shape [2, 3126]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  75%|███████▍  | 5851/7813 [1:01:41<20:52,  1.57it/s, loss=0.021] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3064], which does not match the required output shape [2, 3064]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  76%|███████▋  | 5966/7813 [1:02:53<18:52,  1.63it/s, loss=0.01]   /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2940], which does not match the required output shape [2, 2940]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  79%|███████▉  | 6204/7813 [1:05:16<15:38,  1.71it/s, loss=0.0267] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2882], which does not match the required output shape [2, 2882]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  80%|███████▉  | 6235/7813 [1:05:36<14:02,  1.87it/s, loss=0.0207]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3116], which does not match the required output shape [2, 3116]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  94%|█████████▍| 7377/7813 [1:17:37<04:20,  1.67it/s, loss=0.011]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2858], which does not match the required output shape [2, 2858]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  96%|█████████▌| 7493/7813 [1:18:50<03:13,  1.66it/s, loss=0.0279] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2926], which does not match the required output shape [2, 2926]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5:  99%|█████████▉| 7729/7813 [1:21:21<00:55,  1.51it/s, loss=0.032]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2880], which does not match the required output shape [2, 2880]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5: 100%|██████████| 7813/7813 [1:22:14<00:00,  1.58it/s, loss=0.00427]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/5 - Average Loss: 0.0786\nModel checkpoint saved to /kaggle/working/grasp_model_epoch_1.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5:   3%|▎         | 227/7813 [02:25<1:19:01,  1.60it/s, loss=0.0159] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3124], which does not match the required output shape [2, 3124]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:   6%|▌         | 467/7813 [05:00<2:07:04,  1.04s/it, loss=0.0143] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3202], which does not match the required output shape [2, 3202]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:   7%|▋         | 517/7813 [05:33<1:20:04,  1.52it/s, loss=0.017]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3010], which does not match the required output shape [2, 3010]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:   8%|▊         | 651/7813 [06:57<1:10:35,  1.69it/s, loss=0.0471] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2878], which does not match the required output shape [2, 2878]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  10%|█         | 782/7813 [08:23<1:22:45,  1.42it/s, loss=0.00966]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3084], which does not match the required output shape [2, 3084]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  13%|█▎        | 1020/7813 [10:50<1:00:12,  1.88it/s, loss=0.0143] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3128], which does not match the required output shape [2, 3128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  17%|█▋        | 1300/7813 [13:43<1:15:29,  1.44it/s, loss=0.0135] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3026], which does not match the required output shape [2, 3026]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  19%|█▉        | 1513/7813 [16:00<54:29,  1.93it/s, loss=0.0177]   /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3044], which does not match the required output shape [2, 3044]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  22%|██▏       | 1754/7813 [18:30<1:02:22,  1.62it/s, loss=0.0214] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3336], which does not match the required output shape [2, 3336]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  24%|██▍       | 1892/7813 [19:58<59:42,  1.65it/s, loss=0.00993]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2978], which does not match the required output shape [2, 2978]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  26%|██▌       | 2004/7813 [21:10<56:19,  1.72it/s, loss=0.0101]   /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3210], which does not match the required output shape [2, 3210]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  28%|██▊       | 2210/7813 [23:22<1:05:36,  1.42it/s, loss=0.0264] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2988], which does not match the required output shape [2, 2988]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  30%|███       | 2364/7813 [24:55<53:08,  1.71it/s, loss=0.0075]   /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2650], which does not match the required output shape [2, 2650]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  31%|███▏      | 2450/7813 [25:48<1:03:19,  1.41it/s, loss=0.0118] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3090], which does not match the required output shape [2, 3090]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  38%|███▊      | 2943/7813 [30:59<1:00:36,  1.34it/s, loss=0.00646]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3144], which does not match the required output shape [2, 3144]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  49%|████▊     | 3808/7813 [40:00<38:37,  1.73it/s, loss=0.0112]   /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2840], which does not match the required output shape [2, 2840]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  51%|█████     | 3987/7813 [41:54<52:49,  1.21it/s, loss=0.00772]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3372], which does not match the required output shape [2, 3372]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  53%|█████▎    | 4120/7813 [43:20<45:09,  1.36it/s, loss=0.0192]   /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2980], which does not match the required output shape [2, 2980]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  72%|███████▏  | 5644/7813 [59:17<20:58,  1.72it/s, loss=0.0104] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2966], which does not match the required output shape [2, 2966]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  74%|███████▍  | 5774/7813 [1:00:41<21:59,  1.55it/s, loss=0.00441]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3142], which does not match the required output shape [2, 3142]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  81%|████████  | 6311/7813 [1:06:14<13:12,  1.90it/s, loss=0.00488]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2992], which does not match the required output shape [2, 2992]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  90%|█████████ | 7040/7813 [1:13:56<08:38,  1.49it/s, loss=0.0102] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3076], which does not match the required output shape [2, 3076]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5:  92%|█████████▏| 7164/7813 [1:15:12<08:33,  1.26it/s, loss=0.0107] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3068], which does not match the required output shape [2, 3068]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5: 100%|██████████| 7813/7813 [1:21:59<00:00,  1.59it/s, loss=0.00507]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/5 - Average Loss: 0.0139\nModel checkpoint saved to /kaggle/working/grasp_model_epoch_2.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5:   0%|          | 20/7813 [00:12<1:18:43,  1.65it/s, loss=0.00583]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2662], which does not match the required output shape [2, 2662]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:   7%|▋         | 513/7813 [05:15<1:35:18,  1.28it/s, loss=0.00501]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2764], which does not match the required output shape [2, 2764]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  14%|█▍        | 1103/7813 [11:23<1:23:12,  1.34it/s, loss=0.00672]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2558], which does not match the required output shape [2, 2558]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  19%|█▉        | 1496/7813 [15:33<1:05:06,  1.62it/s, loss=0.014]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3100], which does not match the required output shape [2, 3100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  20%|██        | 1596/7813 [16:34<1:17:30,  1.34it/s, loss=0.00624]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2836], which does not match the required output shape [2, 2836]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  23%|██▎       | 1810/7813 [18:50<57:53,  1.73it/s, loss=0.00854]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3012], which does not match the required output shape [2, 3012]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  27%|██▋       | 2110/7813 [22:04<1:03:38,  1.49it/s, loss=0.016]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3412], which does not match the required output shape [2, 3412]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  30%|██▉       | 2340/7813 [24:25<47:56,  1.90it/s, loss=0.00493]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2628], which does not match the required output shape [2, 2628]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  36%|███▌      | 2810/7813 [29:19<48:36,  1.72it/s, loss=0.004]    /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2698], which does not match the required output shape [2, 2698]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  40%|████      | 3153/7813 [32:52<51:44,  1.50it/s, loss=0.027]    /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2932], which does not match the required output shape [2, 2932]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  45%|████▍     | 3489/7813 [36:16<42:12,  1.71it/s, loss=0.00673]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3288], which does not match the required output shape [2, 3288]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  46%|████▋     | 3629/7813 [37:44<37:57,  1.84it/s, loss=0.00341]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2744], which does not match the required output shape [2, 2744]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  50%|████▉     | 3876/7813 [40:20<42:10,  1.56it/s, loss=0.0287]   /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2568], which does not match the required output shape [2, 2568]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  55%|█████▌    | 4315/7813 [44:56<34:32,  1.69it/s, loss=0.00907]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2874], which does not match the required output shape [2, 2874]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  58%|█████▊    | 4533/7813 [47:13<41:58,  1.30it/s, loss=0.0288] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3096], which does not match the required output shape [2, 3096]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  58%|█████▊    | 4537/7813 [47:15<30:14,  1.81it/s, loss=0.00293]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2794], which does not match the required output shape [2, 2794]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  60%|██████    | 4715/7813 [49:11<43:32,  1.19it/s, loss=0.00556]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3132], which does not match the required output shape [2, 3132]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  64%|██████▍   | 4984/7813 [52:00<28:34,  1.65it/s, loss=0.0201] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3096], which does not match the required output shape [2, 3096]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  70%|███████   | 5498/7813 [57:23<22:39,  1.70it/s, loss=0.00268]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3166], which does not match the required output shape [2, 3166]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  79%|███████▉  | 6194/7813 [1:04:47<17:03,  1.58it/s, loss=0.00304]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2664], which does not match the required output shape [2, 2664]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  89%|████████▉ | 6938/7813 [1:12:33<08:14,  1.77it/s, loss=0.00467]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3296], which does not match the required output shape [2, 3296]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  90%|████████▉ | 7001/7813 [1:13:13<08:20,  1.62it/s, loss=0.0061] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2928], which does not match the required output shape [2, 2928]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5:  95%|█████████▍| 7390/7813 [1:17:24<04:15,  1.65it/s, loss=0.00527]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2636], which does not match the required output shape [2, 2636]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 3/5: 100%|██████████| 7813/7813 [1:21:54<00:00,  1.59it/s, loss=0.000863]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/5 - Average Loss: 0.0076\nModel checkpoint saved to /kaggle/working/grasp_model_epoch_3.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5:   2%|▏         | 136/7813 [01:22<1:29:45,  1.43it/s, loss=0.00637]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2668], which does not match the required output shape [2, 2668]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:   3%|▎         | 210/7813 [02:09<1:11:59,  1.76it/s, loss=0.00765]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2942], which does not match the required output shape [2, 2942]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  12%|█▏        | 953/7813 [09:50<1:07:02,  1.71it/s, loss=0.00276]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2840], which does not match the required output shape [2, 2840]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  12%|█▏        | 973/7813 [10:02<1:36:50,  1.18it/s, loss=0.0148] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3186], which does not match the required output shape [2, 3186]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  15%|█▍        | 1139/7813 [11:50<1:04:18,  1.73it/s, loss=0.00255]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3384], which does not match the required output shape [2, 3384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  16%|█▌        | 1229/7813 [12:47<1:03:50,  1.72it/s, loss=0.00562]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3106], which does not match the required output shape [2, 3106]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  27%|██▋       | 2116/7813 [22:13<1:15:47,  1.25it/s, loss=0.00212]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3322], which does not match the required output shape [2, 3322]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  30%|██▉       | 2340/7813 [24:29<1:12:59,  1.25it/s, loss=0.0026] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2776], which does not match the required output shape [2, 2776]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  34%|███▍      | 2675/7813 [28:09<48:35,  1.76it/s, loss=0.00389]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2956], which does not match the required output shape [2, 2956]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  36%|███▌      | 2807/7813 [29:30<45:22,  1.84it/s, loss=0.00526]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2902], which does not match the required output shape [2, 2902]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  37%|███▋      | 2898/7813 [30:26<46:48,  1.75it/s, loss=0.00551]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3020], which does not match the required output shape [2, 3020]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  37%|███▋      | 2920/7813 [30:41<50:39,  1.61it/s, loss=0.00414]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2884], which does not match the required output shape [2, 2884]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  39%|███▉      | 3056/7813 [32:07<1:01:04,  1.30it/s, loss=0.00359]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2898], which does not match the required output shape [2, 2898]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  45%|████▍     | 3511/7813 [36:55<46:36,  1.54it/s, loss=0.011]    /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2898], which does not match the required output shape [2, 2898]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  54%|█████▎    | 4190/7813 [44:06<38:11,  1.58it/s, loss=0.00315]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2856], which does not match the required output shape [2, 2856]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  65%|██████▌   | 5098/7813 [53:38<25:59,  1.74it/s, loss=0.00819]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3454], which does not match the required output shape [2, 3454]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  66%|██████▋   | 5190/7813 [54:36<30:46,  1.42it/s, loss=0.00204]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3268], which does not match the required output shape [2, 3268]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  71%|███████   | 5526/7813 [58:04<23:06,  1.65it/s, loss=0.00346]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3280], which does not match the required output shape [2, 3280]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  73%|███████▎  | 5669/7813 [59:38<21:42,  1.65it/s, loss=0.00255]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2802], which does not match the required output shape [2, 2802]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  74%|███████▎  | 5754/7813 [1:00:31<25:25,  1.35it/s, loss=0.00396]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3206], which does not match the required output shape [2, 3206]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  77%|███████▋  | 6002/7813 [1:03:10<19:47,  1.53it/s, loss=0.00518]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3202], which does not match the required output shape [2, 3202]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  78%|███████▊  | 6067/7813 [1:03:52<15:53,  1.83it/s, loss=0.00274]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3386], which does not match the required output shape [2, 3386]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  78%|███████▊  | 6112/7813 [1:04:21<21:58,  1.29it/s, loss=0.00149]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3174], which does not match the required output shape [2, 3174]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  79%|███████▉  | 6172/7813 [1:04:57<15:08,  1.81it/s, loss=0.00267]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3050], which does not match the required output shape [2, 3050]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  80%|████████  | 6252/7813 [1:05:49<16:08,  1.61it/s, loss=0.0026] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2910], which does not match the required output shape [2, 2910]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  87%|████████▋ | 6814/7813 [1:11:39<11:39,  1.43it/s, loss=0.00279]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2720], which does not match the required output shape [2, 2720]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  89%|████████▉ | 6949/7813 [1:13:05<08:29,  1.70it/s, loss=0.00258]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3482], which does not match the required output shape [2, 3482]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  90%|█████████ | 7064/7813 [1:14:17<09:50,  1.27it/s, loss=0.0074] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3346], which does not match the required output shape [2, 3346]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  94%|█████████▎| 7306/7813 [1:16:52<06:09,  1.37it/s, loss=0.00515]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3518], which does not match the required output shape [2, 3518]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5:  96%|█████████▌| 7478/7813 [1:18:37<03:28,  1.60it/s, loss=0.00404]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3200], which does not match the required output shape [2, 3200]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 4/5: 100%|██████████| 7813/7813 [1:22:06<00:00,  1.59it/s, loss=0.000633]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/5 - Average Loss: 0.0050\nModel checkpoint saved to /kaggle/working/grasp_model_epoch_4.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5:   1%|          | 63/7813 [00:38<1:27:13,  1.48it/s, loss=0.00298]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3050], which does not match the required output shape [2, 3050]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:   2%|▏         | 119/7813 [01:12<1:10:45,  1.81it/s, loss=0.018]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2968], which does not match the required output shape [2, 2968]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:   6%|▌         | 449/7813 [04:45<1:12:33,  1.69it/s, loss=0.00336]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3034], which does not match the required output shape [2, 3034]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:   6%|▋         | 501/7813 [05:16<1:00:53,  2.00it/s, loss=0.00171]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3276], which does not match the required output shape [2, 3276]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:   7%|▋         | 515/7813 [05:24<1:15:12,  1.62it/s, loss=0.00274]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3174], which does not match the required output shape [2, 3174]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:   7%|▋         | 528/7813 [05:31<1:06:36,  1.82it/s, loss=0.00482]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2732], which does not match the required output shape [2, 2732]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:   7%|▋         | 573/7813 [06:01<1:12:22,  1.67it/s, loss=0.00325]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2642], which does not match the required output shape [2, 2642]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:   8%|▊         | 616/7813 [06:27<1:11:10,  1.69it/s, loss=0.0068] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2930], which does not match the required output shape [2, 2930]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  14%|█▍        | 1128/7813 [11:58<1:08:56,  1.62it/s, loss=0.00205]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3054], which does not match the required output shape [2, 3054]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  22%|██▏       | 1708/7813 [18:07<1:00:10,  1.69it/s, loss=0.00479]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2926], which does not match the required output shape [2, 2926]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  28%|██▊       | 2169/7813 [22:49<1:06:32,  1.41it/s, loss=0.00378]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2896], which does not match the required output shape [2, 2896]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  29%|██▉       | 2256/7813 [23:45<59:55,  1.55it/s, loss=0.00375]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3330], which does not match the required output shape [2, 3330]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  33%|███▎      | 2611/7813 [27:23<46:26,  1.87it/s, loss=0.00199]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3142], which does not match the required output shape [2, 3142]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  40%|████      | 3143/7813 [32:52<44:07,  1.76it/s, loss=0.00463]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2906], which does not match the required output shape [2, 2906]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  47%|████▋     | 3661/7813 [38:25<47:35,  1.45it/s, loss=0.00179]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2978], which does not match the required output shape [2, 2978]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  52%|█████▏    | 4046/7813 [42:21<34:46,  1.81it/s, loss=0.00183]  /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3654], which does not match the required output shape [2, 3654]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  53%|█████▎    | 4117/7813 [43:04<44:22,  1.39it/s, loss=0.00555]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3586], which does not match the required output shape [2, 3586]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  64%|██████▍   | 5007/7813 [52:32<24:59,  1.87it/s, loss=0.00542]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2836], which does not match the required output shape [2, 2836]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  65%|██████▍   | 5067/7813 [53:10<22:12,  2.06it/s, loss=0.00743]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3246], which does not match the required output shape [2, 3246]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  66%|██████▌   | 5176/7813 [54:20<33:29,  1.31it/s, loss=0.00549]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2816], which does not match the required output shape [2, 2816]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  67%|██████▋   | 5214/7813 [54:46<29:15,  1.48it/s, loss=0.00521]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2834], which does not match the required output shape [2, 2834]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  74%|███████▍  | 5816/7813 [1:01:09<19:54,  1.67it/s, loss=0.00189]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3080], which does not match the required output shape [2, 3080]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  75%|███████▍  | 5822/7813 [1:01:13<22:15,  1.49it/s, loss=0.00125]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2784], which does not match the required output shape [2, 2784]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  76%|███████▌  | 5950/7813 [1:02:31<20:18,  1.53it/s, loss=0.0224] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3300], which does not match the required output shape [2, 3300]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  77%|███████▋  | 6009/7813 [1:03:09<23:37,  1.27it/s, loss=0.00316]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3036], which does not match the required output shape [2, 3036]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  90%|████████▉ | 7019/7813 [1:13:57<07:12,  1.84it/s, loss=0.00375] /usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3146], which does not match the required output shape [2, 3146]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  91%|█████████ | 7123/7813 [1:15:01<06:46,  1.70it/s, loss=0.00167]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2970], which does not match the required output shape [2, 2970]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  92%|█████████▏| 7177/7813 [1:15:37<09:42,  1.09it/s, loss=0.00227]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3374], which does not match the required output shape [2, 3374]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  95%|█████████▌| 7437/7813 [1:18:11<03:49,  1.64it/s, loss=0.00247]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3000], which does not match the required output shape [2, 3000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  96%|█████████▌| 7515/7813 [1:18:59<02:47,  1.78it/s, loss=0.00418]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [3090], which does not match the required output shape [2, 3090]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5:  97%|█████████▋| 7576/7813 [1:19:38<01:55,  2.05it/s, loss=0.00556]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [2928], which does not match the required output shape [2, 2928]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/5: 100%|██████████| 7813/7813 [1:22:08<00:00,  1.59it/s, loss=0.000955]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/5 - Average Loss: 0.0039\nModel checkpoint saved to /kaggle/working/grasp_model_epoch_5.pt\n\nPre-training complete.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## 8. Qualitative Evaluation\nAfter training, this section validates that the model has learned effectively. It loads the final checkpoint, generates embeddings for a few test molecules, and computes their similarity. A high score on the diagonal of the similarity matrix indicates success.","metadata":{}},{"cell_type":"code","source":"def evaluate_model():\n    print(\"\\n Starting Post-Pretraining Qualitative Evaluation \")\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Evaluation using device: {device}\")\n    \n    final_checkpoint_path = f\"/kaggle/working/grasp_model_epoch_{EPOCHS}.pt\"\n    if not os.path.exists(final_checkpoint_path):\n        print(f\"Error: Checkpoint file not found at {final_checkpoint_path}. Cannot evaluate.\")\n        return\n        \n    model = GRASPModel(\n        projection_dim=PROJECTION_DIM, \n        graph_emb_dim=GRAPH_EMB_DIM, \n        graph_layers=GRAPH_LAYERS\n    ).to(device)\n    model.load_state_dict(torch.load(final_checkpoint_path, map_location=device))\n    model.eval()\n    print(f\"Successfully loaded model from {final_checkpoint_path}\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n\n    # Test Cases\n    test_smiles = [\n        \"CCO\",                      # Ethanol\n        \"c1ccccc1\",                 # Benzene\n        \"CC(=O)Oc1ccccc1C(=O)O\", # Aspirin\n        \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\", # Caffeine\n        \"C\"                         # Methane\n    ]\n\n    all_graph_embeddings = []\n    all_smiles_embeddings = []\n    valid_smiles_for_display = []\n\n    print(\"\\nGenerating embeddings for test molecules...\")\n    with torch.no_grad():\n        for smiles in test_smiles:\n            graph_data = smiles_to_graph_data(smiles)\n            if graph_data is None:\n                print(f\"Warning: Could not process SMILES: {smiles}\")\n                continue\n                   \n            # edge case \n            if graph_data.edge_index.dim() == 1:\n               \n                graph_data.edge_index = torch.empty((2, 0), dtype=torch.long)\n                    \n            smiles_tokens = tokenizer(smiles, return_tensors='pt', padding=True)\n            \n            graph_batch = Batch.from_data_list([graph_data]).to(device)\n            smiles_batch = {k: v.to(device) for k, v in smiles_tokens.items()}\n            \n            graph_proj, smiles_proj = model(graph_batch, smiles_batch)\n            \n            all_graph_embeddings.append(graph_proj)\n            all_smiles_embeddings.append(smiles_proj)\n            valid_smiles_for_display.append(smiles) # Add to list only if successful\n    \n    if not all_graph_embeddings:\n        print(\"No embeddings were generated. Cannot create similarity matrix.\")\n        return\n        \n    graph_embeddings_tensor = torch.cat(all_graph_embeddings, dim=0)\n    smiles_embeddings_tensor = torch.cat(all_smiles_embeddings, dim=0)\n\n    # Similarity Matrix \n    print(\"\\nCosine Similarity Matrix (Graph vs. SMILES) \")\n    similarity_matrix = torch.matmul(graph_embeddings_tensor, smiles_embeddings_tensor.T).cpu().numpy()\n\n    df = pd.DataFrame(similarity_matrix, index=valid_smiles_for_display, columns=valid_smiles_for_display)\n    print(df.round(4))\n    \n    print(\"\\nKey Observations \")\n    print(\"High values on the diagonal (positive pairs), low values off-diagonal.\")\n    print(\"This indicates the model successfully learned to align representations.\")\n\n\nevaluate_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T01:34:36.317140Z","iopub.execute_input":"2025-06-24T01:34:36.317676Z","iopub.status.idle":"2025-06-24T01:34:36.963988Z","shell.execute_reply.started":"2025-06-24T01:34:36.317645Z","shell.execute_reply":"2025-06-24T01:34:36.963338Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Post-Pretraining Qualitative Evaluation ---\nEvaluation using device: cuda\nSuccessfully loaded model from /kaggle/working/grasp_model_epoch_5.pt\n\nGenerating embeddings for test molecules...\n\n--- Cosine Similarity Matrix (Graph vs. SMILES) ---\n                                 CCO  c1ccccc1  CC(=O)Oc1ccccc1C(=O)O  \\\nCCO                           0.9609    0.3435                -0.0449   \nc1ccccc1                      0.3822    0.8747                -0.0821   \nCC(=O)Oc1ccccc1C(=O)O        -0.0338   -0.0404                 0.9520   \nCN1C=NC2=C1C(=O)N(C(=O)N2C)C  0.0961    0.2039                 0.0124   \nC                             0.4803    0.3894                 0.2627   \n\n                              CN1C=NC2=C1C(=O)N(C(=O)N2C)C       C  \nCCO                                                 0.2041  0.5417  \nc1ccccc1                                            0.3024  0.2932  \nCC(=O)Oc1ccccc1C(=O)O                              -0.2467  0.2543  \nCN1C=NC2=C1C(=O)N(C(=O)N2C)C                        0.3382  0.1732  \nC                                                   0.0990  0.9389  \n\n--- Key Observations ---\nExpected: High values on the diagonal (positive pairs), low values off-diagonal.\nThis indicates the model successfully learned to align representations.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# RUN THIS CELL FIRST!\n!pip uninstall torch torchvision torchaudio torch-scatter torch-sparse torch-geometric torch-cluster torch-spline-conv -y --quiet\n\n# Install a compatible CUDA version of PyTorch\n!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121 --quiet\n\n# Install core libraries\n!pip install \"transformers==4.36.2\" \"accelerate==0.25.0\" \"deepchem\" --quiet\n\n# Install PyTorch Geometric and its dependencies\n!pip install torch_geometric rdkit-pypi pandas tqdm --quiet\n!pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html --quiet\n\nprint(\"\\n✅ All required libraries have been installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T07:47:55.637256Z","iopub.execute_input":"2025-07-05T07:47:55.637879Z","iopub.status.idle":"2025-07-05T07:50:19.430743Z","shell.execute_reply.started":"2025-07-05T07:47:55.637835Z","shell.execute_reply":"2025-07-05T07:50:19.429729Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-spline-conv as it is not installed.\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m548.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\n✅ All required libraries have been installed successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ===================================================================\n# CELL 1: COMPLETE & ROBUST INSTALLATION\n# ===================================================================\n# This command installs all necessary libraries at once.\n# It lets pip resolve the dependencies for the latest Kaggle environment.\n!pip install deepchem rdkit-pypi torch_geometric --quiet\n\nprint(\"✅ Installation complete. All required libraries (DeepChem, RDKit, PyTorch Geometric) are installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:08:21.439203Z","iopub.execute_input":"2025-07-05T08:08:21.439546Z","iopub.status.idle":"2025-07-05T08:08:31.276998Z","shell.execute_reply.started":"2025-07-05T08:08:21.439523Z","shell.execute_reply":"2025-07-05T08:08:31.276064Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h✅ Installation complete. All required libraries (DeepChem, RDKit, PyTorch Geometric) are installed.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ===================================================================\n# CELL 2: ALL IMPORTS AND CONFIGURATION\n# ===================================================================\n\n# --- All Imports ---\nimport os, gc, numpy as np, pandas as pd\nfrom tqdm import tqdm\nimport torch, torch.nn as nn, torch.optim as optim, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.nn import GINConv, global_add_pool\nfrom transformers import AutoModel, AutoConfig\nfrom rdkit import Chem, rdBase\nimport deepchem as dc\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\n\n# --- Environment Setup ---\nrdBase.DisableLog('rdApp.warning'); rdBase.DisableLog('rdApp.error')\nprint(f\"✅ Setup Complete. Using DeepChem version: {dc.__version__}\")\n\n# --- Configuration ---\nPROJECTION_DIM, GRAPH_EMB_DIM, GRAPH_LAYERS, NUM_WORKERS = 128, 128, 4, 2\nPRETRAINED_CHECKPOINT_PATH = \"/kaggle/input/baseline/pytorch/default/1/Saved-Model-and-Encoders(Kaggle)/grasp_model_epoch_5.pt\"\nFT_WARMUP_EPOCHS, FT_MAIN_EPOCHS, FT_BATCH_SIZE = 5, 25, 32\nENCODER_LEARNING_RATE, HEAD_LEARNING_RATE = 1e-5, 1e-4\n\nif not os.path.exists(PRETRAINED_CHECKPOINT_PATH): raise FileNotFoundError(f\"Checkpoint not found at {PRETRAINED_CHECKPOINT_PATH}\")\nprint(f\"Found checkpoint: {PRETRAINED_CHECKPOINT_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:08:51.348107Z","iopub.execute_input":"2025-07-05T08:08:51.348478Z","iopub.status.idle":"2025-07-05T08:09:19.212963Z","shell.execute_reply.started":"2025-07-05T08:08:51.348447Z","shell.execute_reply":"2025-07-05T08:09:19.212251Z"}},"outputs":[{"name":"stderr","text":"2025-07-05 08:09:04.071839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751702944.251847      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751702944.303117      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"name":"stdout","text":"✅ Setup Complete. Using DeepChem version: 2.8.0\nFound checkpoint: /kaggle/input/baseline/pytorch/default/1/Saved-Model-and-Encoders(Kaggle)/grasp_model_epoch_5.pt\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ===================================================================\n# CELL 3: RE-DEFINE PRE-TRAINING BUILDING BLOCKS (FINAL CORRECTION)\n# ===================================================================\n\n# --- SMILES to Graph Utility Functions ---\n# (This part is correct and remains the same)\nATOM_FEATURE_MAP = {'atomic_num': list(range(1, 119)), 'degree': list(range(6)),'formal_charge': list(range(-2, 3)), 'hybridization': [Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,Chem.rdchem.HybridizationType.SP3D2, Chem.rdchem.HybridizationType.UNSPECIFIED],'is_aromatic': [0, 1], 'is_in_ring': [0, 1]}\ndef one_hot_encode(v, c): e = [0] * (len(c) + 1); e[c.index(v) if v in c else -1] = 1; return e\ndef get_atom_features(a): f = []; f += one_hot_encode(a.GetAtomicNum(), ATOM_FEATURE_MAP['atomic_num']); f += one_hot_encode(a.GetDegree(), ATOM_FEATURE_MAP['degree']); f += one_hot_encode(a.GetFormalCharge(), ATOM_FEATURE_MAP['formal_charge']); f += one_hot_encode(a.GetHybridization(), ATOM_FEATURE_MAP['hybridization']); f += one_hot_encode(int(a.GetIsAromatic()), ATOM_FEATURE_MAP['is_aromatic']); f += one_hot_encode(int(a.IsInRing()), ATOM_FEATURE_MAP['is_in_ring']); return torch.tensor(f, dtype=torch.float)\ndef get_num_node_features(): return sum(len(c) + 1 for c in ATOM_FEATURE_MAP.values())\ndef smiles_to_graph_data(s):\n    try:\n        m = Chem.MolFromSmiles(s);\n        if m is None: return None\n        af = [get_atom_features(a) for a in m.GetAtoms()];\n        if not af: return None\n        x = torch.stack(af);\n        ei = []; [ei.extend([(b.GetBeginAtomIdx(), b.GetEndAtomIdx()), (b.GetEndAtomIdx(), b.GetBeginAtomIdx())]) for b in m.GetBonds()]\n        return Data(x=x, edge_index=torch.tensor(ei, dtype=torch.long).t().contiguous())\n    except Exception: return None\n\n# --- Core GRASP Model Architecture (THE ORIGINAL, CORRECT VERSION) ---\nclass GraphEncoder(nn.Module):\n    def __init__(self, num_node_features, embedding_dim, num_layers, dropout):\n        super(GraphEncoder, self).__init__()\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n        self.dropout = dropout\n        self.num_layers = num_layers  # The variable is named num_layers here\n        for i in range(num_layers):\n            in_dim = num_node_features if i == 0 else embedding_dim\n            mlp = nn.Sequential(\n                nn.Linear(in_dim, 2 * embedding_dim),\n                nn.ReLU(),\n                nn.Linear(2 * embedding_dim, embedding_dim)\n            )\n            self.convs.append(GINConv(mlp, train_eps=True))\n            self.batch_norms.append(nn.BatchNorm1d(embedding_dim))\n            \n    def forward(self, x, edge_index, batch):\n        h = x\n        # *** THE FIX IS HERE ***\n        # Changed self.n_layers to self.num_layers to match the __init__ method\n        for i in range(self.num_layers): \n            h = self.convs[i](h, edge_index)\n            h = self.batch_norms[i](h)\n            h = F.relu(h)\n            h = F.dropout(h, p=self.dropout, training=self.training)\n        return global_add_pool(h, batch)\n\nclass GRASPModel(nn.Module):\n    def __init__(self, g, l, p, d):\n        super(GRASPModel, self).__init__()\n        self.graph_encoder = GraphEncoder(get_num_node_features(), g, l, d)\n    def forward(self, gb, sb):\n        pass\n\nprint(\"✅ Pre-training building blocks defined with FINAL corrected GraphEncoder.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:15:59.651827Z","iopub.execute_input":"2025-07-05T08:15:59.652367Z","iopub.status.idle":"2025-07-05T08:15:59.665944Z","shell.execute_reply.started":"2025-07-05T08:15:59.652336Z","shell.execute_reply":"2025-07-05T08:15:59.665016Z"}},"outputs":[{"name":"stdout","text":"✅ Pre-training building blocks defined with FINAL corrected GraphEncoder.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ===================================================================\n# CELL 4: (FINAL CORRECTED) MANUAL DATA LOADING & DOWNSTREAM PIPELINE\n# ===================================================================\nimport pandas as pd\nimport requests\nfrom io import StringIO, BytesIO # BytesIO is needed for compressed data\n\n# --- CORRECTED: Function to manually download data (handles compression) ---\ndef download_moleculenet_csv(url):\n    \"\"\"\n    Downloads a CSV file from a URL and returns a pandas DataFrame.\n    It now correctly handles .gz compressed files like Tox21.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        \n        # Check if the URL suggests compression\n        if url.endswith('.gz'):\n            # For compressed data, we use BytesIO and tell pandas about the compression\n            return pd.read_csv(BytesIO(response.content), compression='gzip')\n        else:\n            # For plain text CSVs, we use StringIO\n            return pd.read_csv(StringIO(response.text))\n            \n    except requests.exceptions.RequestException as e:\n        print(f\"Error downloading data from {url}: {e}\")\n        return None\n\n# --- NEW: Function to manually create DeepChem Dataset objects ---\ndef create_dc_dataset(dataframe, smiles_col, label_cols):\n    \"\"\"Converts a pandas DataFrame into a deepchem.data.Dataset object.\"\"\"\n    smiles = dataframe[smiles_col].tolist()\n    labels = dataframe[label_cols].values\n    return dc.data.NumpyDataset(X=np.zeros(len(smiles)), y=labels, ids=smiles)\n\n\n# --- REVISED: load_moleculenet_dataset that uses manual download ---\ndef load_moleculenet_dataset(name):\n    print(f\"Bypassing dc.molnet. Manually loading '{name}' dataset...\")\n    \n    if name == 'bbbp':\n        url = \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\"\n        df = download_moleculenet_csv(url)\n        train_df, valid_df, test_df = np.split(df.sample(frac=1, random_state=42), [int(.8*len(df)), int(.9*len(df))])\n        tasks, smiles_col, label_cols = ['p_np'], 'smiles', ['p_np']\n        \n    elif name == 'esol':\n        url = \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv\"\n        df = download_moleculenet_csv(url)\n        train_df, valid_df, test_df = np.split(df.sample(frac=1, random_state=42), [int(.8*len(df)), int(.9*len(df))])\n        tasks, smiles_col, label_cols = ['measured log solubility in mols per litre'], 'smiles', ['measured log solubility in mols per litre']\n\n    elif name == 'tox21':\n        url = \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/tox21.csv.gz\"\n        df = download_moleculenet_csv(url)\n        train_df, valid_df, test_df = np.split(df.sample(frac=1, random_state=42), [int(.8*len(df)), int(.9*len(df))])\n        tasks = [col for col in df.columns if col not in ['mol_id', 'smiles', 'Set']]\n        smiles_col, label_cols = 'smiles', tasks\n        \n    else:\n        raise ValueError(f\"Dataset {name} not supported for manual loading.\")\n\n    train_dc = create_dc_dataset(train_df, smiles_col, label_cols)\n    valid_dc = create_dc_dataset(valid_df, smiles_col, label_cols)\n    test_dc = create_dc_dataset(test_df, smiles_col, label_cols)\n\n    print(f\"Loaded {name}: Train {len(train_dc)}, Valid {len(valid_dc)}, Test {len(test_dc)}\")\n    return tasks, train_dc, valid_dc, test_dc\n\n# The rest of the definitions are correct and remain the same.\nclass MoleculeNetDataset(Dataset):\n    def __init__(self,d): self.s, self.l = d.ids, d.y\n    def __len__(self): return len(self.s)\n    def __getitem__(self,i): g=smiles_to_graph_data(self.s[i]); return (g, torch.tensor(self.l[i],dtype=torch.float)) if g else None\n\nclass DownstreamCollator:\n    def __call__(self,b): b=[i for i in b if i is not None]; return (Batch.from_data_list([i[0] for i in b]), torch.stack([i[1] for i in b])) if b else (None,None)\n\nclass DownstreamModel(nn.Module):\n    def __init__(self, p, nt):\n        super().__init__();\n        sd = torch.load(p, map_location=torch.device('cpu'));\n        self.graph_encoder = GraphEncoder(get_num_node_features(), GRAPH_EMB_DIM, GRAPH_LAYERS, 0.1)\n        esd = {k.replace('graph_encoder.',''): v for k,v in sd.items() if k.startswith('graph_encoder.')}\n        self.graph_encoder.load_state_dict(esd);\n        self.prediction_head = nn.Sequential(nn.Linear(GRAPH_EMB_DIM, GRAPH_EMB_DIM//2), nn.ReLU(), nn.Dropout(0.2), nn.Linear(GRAPH_EMB_DIM//2, nt))\n    def freeze_encoder(self): [p.requires_grad_(False) for p in self.graph_encoder.parameters()]; print(\"Encoder frozen.\")\n    def unfreeze_encoder(self): [p.requires_grad_(True) for p in self.graph_encoder.parameters()]; print(\"Encoder unfrozen.\")\n    def forward(self, gb): return self.prediction_head(self.graph_encoder(gb.x, gb.edge_index, gb.batch))\n\nprint(\"✅ Downstream data utilities (with GZIP support) and model defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:34:46.590815Z","iopub.execute_input":"2025-07-05T08:34:46.591436Z","iopub.status.idle":"2025-07-05T08:34:46.607639Z","shell.execute_reply.started":"2025-07-05T08:34:46.591406Z","shell.execute_reply":"2025-07-05T08:34:46.606672Z"}},"outputs":[{"name":"stdout","text":"✅ Downstream data utilities (with GZIP support) and model defined.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ===================================================================\n# CELL 5: MASTER FINE-TUNING SCRIPT\n# ===================================================================\ndef run_finetuning_experiment_v2(dataset_name):\n    # (The function definition is identical to the one in the previous correct responses)\n    # ... It is quite long, so I will omit re-pasting it for brevity, but you should\n    # ensure this cell contains the full `run_finetuning_experiment_v2` function.\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"\\n{'='*60}\\n  STARTING EXPERIMENT FOR: {dataset_name.upper()} on device: {device}\\n{'='*60}\")\n\n    tasks, train_dc, valid_dc, test_dc = load_moleculenet_dataset(dataset_name)\n    num_tasks, task_type = len(tasks), 'classification' if dataset_name != 'esol' else 'regression'\n\n    collator = DownstreamCollator()\n    train_loader = DataLoader(MoleculeNetDataset(train_dc), FT_BATCH_SIZE, shuffle=True, collate_fn=collator, num_workers=NUM_WORKERS)\n    valid_loader = DataLoader(MoleculeNetDataset(valid_dc), FT_BATCH_SIZE, shuffle=False, collate_fn=collator, num_workers=NUM_WORKERS)\n    test_loader = DataLoader(MoleculeNetDataset(test_dc), FT_BATCH_SIZE, shuffle=False, collate_fn=collator, num_workers=NUM_WORKERS)\n\n    model = DownstreamModel(PRETRAINED_CHECKPOINT_PATH, num_tasks).to(device)\n    criterion = nn.BCEWithLogitsLoss(reduction='none') if task_type == 'classification' else nn.MSELoss()\n\n    print(f\"\\n--- [STAGE 1] Head Warm-up ({FT_WARMUP_EPOCHS} epochs) ---\")\n    model.freeze_encoder()\n    optimizer = optim.AdamW(model.prediction_head.parameters(), lr=HEAD_LEARNING_RATE)\n    for epoch in range(FT_WARMUP_EPOCHS):\n        model.train()\n        for graph_batch, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{FT_WARMUP_EPOCHS} [Warm-up]\"):\n            if graph_batch is None: continue\n            graph_batch, labels = graph_batch.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(graph_batch)\n            mask = ~torch.isnan(labels)\n            if task_type == 'classification': loss = criterion(outputs[mask], labels[mask]).mean() if mask.any() else torch.tensor(0.0)\n            else: loss = criterion(outputs, labels)\n            if loss.requires_grad: loss.backward(); optimizer.step()\n\n    print(f\"\\n--- [STAGE 2] Full Fine-tuning ({FT_MAIN_EPOCHS} epochs) ---\")\n    model.unfreeze_encoder()\n    optimizer = optim.AdamW([\n        {'params': model.graph_encoder.parameters(), 'lr': ENCODER_LEARNING_RATE},\n        {'params': model.prediction_head.parameters(), 'lr': HEAD_LEARNING_RATE}\n    ])\n    best_valid_metric, best_model_path = (-1 if task_type == 'classification' else float('inf')), f\"/kaggle/working/best_{dataset_name}.pt\"\n\n    for epoch in range(FT_MAIN_EPOCHS):\n        model.train(); total_loss = 0\n        for graph_batch, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{FT_MAIN_EPOCHS} [Fine-tune]\"):\n            if graph_batch is None: continue\n            graph_batch, labels = graph_batch.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(graph_batch)\n            mask = ~torch.isnan(labels)\n            if task_type == 'classification': loss = criterion(outputs[mask], labels[mask]).mean() if mask.any() else torch.tensor(0.0)\n            else: loss = criterion(outputs, labels)\n            if loss.requires_grad: loss.backward(); optimizer.step()\n            total_loss += loss.item()\n        \n        model.eval(); all_preds, all_trues = [], []\n        with torch.no_grad():\n            for graph_batch, labels in valid_loader:\n                if graph_batch is None: continue\n                outputs = model(graph_batch.to(device))\n                all_preds.append((torch.sigmoid(outputs) if task_type == 'classification' else outputs).cpu().numpy())\n                all_trues.append(labels.cpu().numpy())\n        all_preds, all_trues = np.concatenate(all_preds), np.concatenate(all_trues)\n\n        if task_type == 'classification':\n            aucs = [roc_auc_score(all_trues[:,i][~np.isnan(all_trues[:,i])], all_preds[:,i][~np.isnan(all_trues[:,i])]) for i in range(num_tasks) if len(np.unique(all_trues[:,i][~np.isnan(all_trues[:,i])])) > 1]\n            valid_metric = np.mean(aucs) if aucs else 0.0\n            print(f\"Epoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f} | Valid AUC: {valid_metric:.4f}\")\n            if valid_metric > best_valid_metric: best_valid_metric, _ = valid_metric, torch.save(model.state_dict(), best_model_path); print(\"  -> New best model saved\")\n        else:\n            valid_metric = np.sqrt(mean_squared_error(all_trues, all_preds))\n            print(f\"Epoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f} | Valid RMSE: {valid_metric:.4f}\")\n            if valid_metric < best_valid_metric: best_valid_metric, _ = valid_metric, torch.save(model.state_dict(), best_model_path); print(\"  -> New best model saved\")\n\n    print(f\"\\n--- Evaluating best model on {dataset_name} test set ---\")\n    model.load_state_dict(torch.load(best_model_path)); model.eval()\n    all_preds_test, all_trues_test = [], []\n    with torch.no_grad():\n        for graph_batch, labels in tqdm(test_loader, desc=\"[Final Test]\"):\n            if graph_batch is None: continue\n            outputs = model(graph_batch.to(device))\n            all_preds_test.append((torch.sigmoid(outputs) if task_type == 'classification' else outputs).cpu().numpy())\n            all_trues_test.append(labels.cpu().numpy())\n    all_preds_test, all_trues_test = np.concatenate(all_preds_test), np.concatenate(all_trues_test)\n    \n    if task_type == 'classification':\n        test_aucs = [roc_auc_score(all_trues_test[:,i][~np.isnan(all_trues_test[:,i])], all_preds_test[:,i][~np.isnan(all_trues_test[:,i])]) for i in range(num_tasks) if len(np.unique(all_trues_test[:,i][~np.isnan(all_trues_test[:,i])])) > 1]\n        test_metric = np.mean(test_aucs) if test_aucs else 0.0\n        print(f\"\\nFINAL TEST METRIC for {dataset_name}: ROC-AUC = {test_metric:.4f}\")\n    else:\n        test_metric = np.sqrt(mean_squared_error(all_trues_test, all_preds_test))\n        print(f\"\\nFINAL TEST METRIC for {dataset_name}: RMSE = {test_metric:.4f}\")\n    \n    del model, train_loader, valid_loader, test_loader; gc.collect(); torch.cuda.empty_cache()\n    return test_metric\n\nprint(\"✅ Master fine-tuning function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:34:50.333034Z","iopub.execute_input":"2025-07-05T08:34:50.333559Z","iopub.status.idle":"2025-07-05T08:34:50.351289Z","shell.execute_reply.started":"2025-07-05T08:34:50.333515Z","shell.execute_reply":"2025-07-05T08:34:50.350501Z"}},"outputs":[{"name":"stdout","text":"✅ Master fine-tuning function defined.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import deepchem as dc\n\n# List all functions in dc.molnet that seem to be loaders\nloader_functions = [func for func in dir(dc.molnet) if func.startswith('load_')]\n\nprint(\"Available loader functions in your deepchem.molnet version:\")\nprint(loader_functions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:19:58.469972Z","iopub.execute_input":"2025-07-05T08:19:58.470691Z","iopub.status.idle":"2025-07-05T08:19:58.476272Z","shell.execute_reply.started":"2025-07-05T08:19:58.470658Z","shell.execute_reply":"2025-07-05T08:19:58.475585Z"}},"outputs":[{"name":"stdout","text":"Available loader functions in your deepchem.molnet version:\n['load_Platinum_Adsorption', 'load_bace_classification', 'load_bace_regression', 'load_bandgap', 'load_bbbc001', 'load_bbbc002', 'load_bbbc003', 'load_bbbc004', 'load_bbbc005', 'load_bbbp', 'load_cell_counting', 'load_chembl', 'load_chembl25', 'load_clearance', 'load_clintox', 'load_delaney', 'load_factors', 'load_freesolv', 'load_function', 'load_hiv', 'load_hopv', 'load_hppb', 'load_kaggle', 'load_kinase', 'load_lipo', 'load_mp_formation_energy', 'load_mp_metallicity', 'load_muv', 'load_nci', 'load_pcba', 'load_pdbbind', 'load_perovskite', 'load_ppb', 'load_qm7', 'load_qm8', 'load_qm9', 'load_sampl', 'load_sider', 'load_sweet', 'load_thermosol', 'load_tox21', 'load_toxcast', 'load_uspto', 'load_uv', 'load_zinc15']\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ===================================================================\n# CELL 6: EXECUTE ALL EXPERIMENTS AND SUMMARIZE\n# ===================================================================\nresults = {\n    'bbbp': {'metric': 'ROC-AUC', 'score': run_finetuning_experiment_v2('bbbp')},\n    'esol': {'metric': 'RMSE', 'score': run_finetuning_experiment_v2('esol')},\n    'tox21': {'metric': 'ROC-AUC', 'score': run_finetuning_experiment_v2('tox21')}\n}\n\n# --- Summarize Results ---\nprint(\"\\n\" + \"=\"*50)\nprint(\"           GRASP FINAL BENCHMARK RESULTS\")\nprint(\"=\"*50)\nprint(f\"| {'Dataset':<8} | {'Metric':<10}| {'Score':<11}|\")\nprint(f\"|{'-'*10}|{'-'*12}|{'-'*13}|\")\nprint(f\"| {'BBBP':<8} | {results['bbbp']['metric']:<10}| {results['bbbp']['score']:<11.4f}|\")\nprint(f\"| {'Tox21':<8} | {results['tox21']['metric']:<10}| {results['tox21']['score']:<11.4f}|\")\nprint(f\"| {'ESOL':<8} | {results['esol']['metric']:<10}| {results['esol']['score']:<11.4f}|\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:34:58.801216Z","iopub.execute_input":"2025-07-05T08:34:58.801905Z","iopub.status.idle":"2025-07-05T08:39:32.435799Z","shell.execute_reply.started":"2025-07-05T08:34:58.801882Z","shell.execute_reply":"2025-07-05T08:39:32.435118Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n  STARTING EXPERIMENT FOR: BBBP on device: cuda\n============================================================\nBypassing dc.molnet. Manually loading 'bbbp' dataset...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"},{"name":"stdout","text":"Loaded bbbp: Train 1640, Valid 205, Test 205\n\n--- [STAGE 1] Head Warm-up (5 epochs) ---\nEncoder frozen.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5 [Warm-up]: 100%|██████████| 52/52 [00:01<00:00, 29.86it/s]\nEpoch 2/5 [Warm-up]: 100%|██████████| 52/52 [00:01<00:00, 29.11it/s]\nEpoch 3/5 [Warm-up]: 100%|██████████| 52/52 [00:01<00:00, 29.34it/s]\nEpoch 4/5 [Warm-up]: 100%|██████████| 52/52 [00:01<00:00, 29.65it/s]\nEpoch 5/5 [Warm-up]: 100%|██████████| 52/52 [00:01<00:00, 29.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- [STAGE 2] Full Fine-tuning (25 epochs) ---\nEncoder unfrozen.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.4964 | Valid AUC: 0.8298\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 0.4806 | Valid AUC: 0.8372\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 27.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.4526 | Valid AUC: 0.8405\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 0.4241 | Valid AUC: 0.8473\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 0.4011 | Valid AUC: 0.8400\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 0.4034 | Valid AUC: 0.8528\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 28.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 0.3908 | Valid AUC: 0.8586\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 30.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 0.3730 | Valid AUC: 0.8664\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 0.3515 | Valid AUC: 0.8694\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 30.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.3548 | Valid AUC: 0.8674\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 30.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 0.3527 | Valid AUC: 0.8842\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 0.3279 | Valid AUC: 0.8817\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 30.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 0.3395 | Valid AUC: 0.8851\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 0.3237 | Valid AUC: 0.8896\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 30.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 0.3117 | Valid AUC: 0.8944\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 30.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Train Loss: 0.3134 | Valid AUC: 0.8901\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 28.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Train Loss: 0.3025 | Valid AUC: 0.8912\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Train Loss: 0.3040 | Valid AUC: 0.9045\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 28.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Train Loss: 0.2940 | Valid AUC: 0.9055\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 30.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Train Loss: 0.2902 | Valid AUC: 0.8970\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | Train Loss: 0.2882 | Valid AUC: 0.9077\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | Train Loss: 0.2802 | Valid AUC: 0.8979\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 30.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | Train Loss: 0.2772 | Valid AUC: 0.9045\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 | Train Loss: 0.2879 | Valid AUC: 0.9110\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/25 [Fine-tune]: 100%|██████████| 52/52 [00:01<00:00, 29.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 | Train Loss: 0.2784 | Valid AUC: 0.9128\n  -> New best model saved\n\n--- Evaluating best model on bbbp test set ---\n","output_type":"stream"},{"name":"stderr","text":"[Final Test]: 100%|██████████| 7/7 [00:00<00:00, 22.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFINAL TEST METRIC for bbbp: ROC-AUC = 0.9400\n\n============================================================\n  STARTING EXPERIMENT FOR: ESOL on device: cuda\n============================================================\nBypassing dc.molnet. Manually loading 'esol' dataset...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"},{"name":"stdout","text":"Loaded esol: Train 902, Valid 113, Test 113\n\n--- [STAGE 1] Head Warm-up (5 epochs) ---\nEncoder frozen.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5 [Warm-up]: 100%|██████████| 29/29 [00:00<00:00, 38.60it/s]\nEpoch 2/5 [Warm-up]: 100%|██████████| 29/29 [00:00<00:00, 37.24it/s]\nEpoch 3/5 [Warm-up]: 100%|██████████| 29/29 [00:00<00:00, 38.19it/s]\nEpoch 4/5 [Warm-up]: 100%|██████████| 29/29 [00:00<00:00, 39.34it/s]\nEpoch 5/5 [Warm-up]: 100%|██████████| 29/29 [00:00<00:00, 37.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- [STAGE 2] Full Fine-tuning (25 epochs) ---\nEncoder unfrozen.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 39.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 2.6189 | Valid RMSE: 1.1844\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 37.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 2.2085 | Valid RMSE: 1.1189\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 39.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 1.9776 | Valid RMSE: 1.0360\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 40.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 1.6972 | Valid RMSE: 1.0461\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 39.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 1.7412 | Valid RMSE: 0.9693\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 39.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 1.7779 | Valid RMSE: 0.9398\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 36.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 1.5539 | Valid RMSE: 0.9446\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 37.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 1.6547 | Valid RMSE: 0.9318\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 39.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 1.3516 | Valid RMSE: 0.9130\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 38.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 1.3035 | Valid RMSE: 0.8868\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 37.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 1.2321 | Valid RMSE: 0.8688\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 38.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 1.2562 | Valid RMSE: 0.8602\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 32.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 1.3935 | Valid RMSE: 0.8557\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 35.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 1.3107 | Valid RMSE: 0.8553\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 39.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 1.1215 | Valid RMSE: 0.8410\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 37.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Train Loss: 1.0148 | Valid RMSE: 0.8436\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 36.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Train Loss: 1.1825 | Valid RMSE: 0.8096\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 38.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Train Loss: 1.1373 | Valid RMSE: 0.8190\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 39.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Train Loss: 1.0838 | Valid RMSE: 0.7987\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 39.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Train Loss: 1.0747 | Valid RMSE: 0.7985\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 38.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | Train Loss: 1.0390 | Valid RMSE: 0.7973\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 38.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | Train Loss: 0.9608 | Valid RMSE: 0.7931\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 39.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | Train Loss: 1.0152 | Valid RMSE: 0.7690\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 38.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 | Train Loss: 0.9602 | Valid RMSE: 0.7635\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/25 [Fine-tune]: 100%|██████████| 29/29 [00:00<00:00, 37.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 | Train Loss: 0.9365 | Valid RMSE: 0.7843\n\n--- Evaluating best model on esol test set ---\n","output_type":"stream"},{"name":"stderr","text":"[Final Test]: 100%|██████████| 4/4 [00:00<00:00, 20.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFINAL TEST METRIC for esol: RMSE = 0.8986\n\n============================================================\n  STARTING EXPERIMENT FOR: TOX21 on device: cuda\n============================================================\nBypassing dc.molnet. Manually loading 'tox21' dataset...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"},{"name":"stdout","text":"Loaded tox21: Train 6264, Valid 783, Test 784\n\n--- [STAGE 1] Head Warm-up (5 epochs) ---\nEncoder frozen.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5 [Warm-up]:  78%|███████▊  | 153/196 [00:04<00:01, 37.73it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [1296], which does not match the required output shape [2, 1296]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5 [Warm-up]:  97%|█████████▋| 191/196 [00:05<00:00, 37.57it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [1358], which does not match the required output shape [2, 1358]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 1/5 [Warm-up]: 100%|██████████| 196/196 [00:05<00:00, 36.02it/s]\nEpoch 2/5 [Warm-up]:  69%|██████▉   | 135/196 [00:03<00:01, 37.81it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [1384], which does not match the required output shape [2, 1384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 2/5 [Warm-up]: 100%|██████████| 196/196 [00:05<00:00, 36.65it/s]\nEpoch 3/5 [Warm-up]: 100%|██████████| 196/196 [00:05<00:00, 37.13it/s]\nEpoch 4/5 [Warm-up]: 100%|██████████| 196/196 [00:05<00:00, 35.15it/s]\nEpoch 5/5 [Warm-up]: 100%|██████████| 196/196 [00:05<00:00, 37.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- [STAGE 2] Full Fine-tuning (25 epochs) ---\nEncoder unfrozen.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 36.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.2618 | Valid AUC: 0.7105\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 38.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 0.2466 | Valid AUC: 0.7319\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.2390 | Valid AUC: 0.7437\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 36.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 0.2330 | Valid AUC: 0.7505\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/25 [Fine-tune]:  49%|████▉     | 97/196 [00:02<00:02, 41.99it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [1454], which does not match the required output shape [2, 1454]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 5/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 0.2286 | Valid AUC: 0.7569\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 36.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 0.2239 | Valid AUC: 0.7607\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 0.2205 | Valid AUC: 0.7697\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 36.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 0.2175 | Valid AUC: 0.7701\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 36.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 0.2156 | Valid AUC: 0.7737\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/25 [Fine-tune]:  19%|█▉        | 37/196 [00:01<00:04, 34.32it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [966], which does not match the required output shape [2, 966]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 10/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 36.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.2113 | Valid AUC: 0.7849\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 0.2115 | Valid AUC: 0.7835\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/25 [Fine-tune]:  51%|█████     | 99/196 [00:02<00:02, 39.20it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [966], which does not match the required output shape [2, 966]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 12/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 0.2073 | Valid AUC: 0.7891\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/25 [Fine-tune]:  51%|█████     | 100/196 [00:02<00:02, 38.85it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [1470], which does not match the required output shape [2, 1470]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 13/25 [Fine-tune]:  95%|█████████▍| 186/196 [00:05<00:00, 40.63it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [1124], which does not match the required output shape [2, 1124]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 13/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 0.2062 | Valid AUC: 0.7923\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/25 [Fine-tune]:  29%|██▉       | 57/196 [00:01<00:03, 38.35it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [1172], which does not match the required output shape [2, 1172]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 14/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 0.2046 | Valid AUC: 0.7914\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 35.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 0.2010 | Valid AUC: 0.7929\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Train Loss: 0.2011 | Valid AUC: 0.8007\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/25 [Fine-tune]:   0%|          | 0/196 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [1120], which does not match the required output shape [2, 1120]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 17/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Train Loss: 0.1983 | Valid AUC: 0.8021\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Train Loss: 0.1949 | Valid AUC: 0.8062\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Train Loss: 0.1952 | Valid AUC: 0.8089\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 35.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Train Loss: 0.1958 | Valid AUC: 0.8086\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | Train Loss: 0.1933 | Valid AUC: 0.8110\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | Train Loss: 0.1934 | Valid AUC: 0.8114\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/25 [Fine-tune]:   3%|▎         | 5/196 [00:00<00:09, 20.78it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [1372], which does not match the required output shape [2, 1372]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 23/25 [Fine-tune]:  49%|████▉     | 96/196 [00:02<00:02, 38.03it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [1246], which does not match the required output shape [2, 1246]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 23/25 [Fine-tune]:  81%|████████  | 159/196 [00:04<00:00, 37.55it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [954], which does not match the required output shape [2, 954]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 23/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 36.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | Train Loss: 0.1916 | Valid AUC: 0.8141\n  -> New best model saved\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 36.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 | Train Loss: 0.1894 | Valid AUC: 0.8126\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/25 [Fine-tune]:   3%|▎         | 5/196 [00:00<00:09, 20.75it/s]/usr/local/lib/python3.11/dist-packages/torch_geometric/data/collate.py:205: UserWarning: An output with one or more elements was resized since it had shape [1172], which does not match the required output shape [2, 1172]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n  value = torch.cat(values, dim=cat_dim or 0, out=out)\nEpoch 25/25 [Fine-tune]: 100%|██████████| 196/196 [00:05<00:00, 37.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 | Train Loss: 0.1902 | Valid AUC: 0.8128\n\n--- Evaluating best model on tox21 test set ---\n","output_type":"stream"},{"name":"stderr","text":"[Final Test]: 100%|██████████| 25/25 [00:00<00:00, 26.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFINAL TEST METRIC for tox21: ROC-AUC = 0.8186\n\n==================================================\n           GRASP FINAL BENCHMARK RESULTS\n==================================================\n| Dataset  | Metric    | Score      |\n|----------|------------|-------------|\n| BBBP     | ROC-AUC   | 0.9400     |\n| Tox21    | ROC-AUC   | 0.8186     |\n| ESOL     | RMSE      | 0.8986     |\n==================================================\n","output_type":"stream"}],"execution_count":29}]}